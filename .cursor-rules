# Auto-reload trigger: 1756624930
# Context-Aware Rules (Working System)
# Context: DEBUGGING
# Total Rules: 8
# Generated from: @debug Fix the system issues...
# Timestamp: 31.08.2025 09:22


# === safety_first_principle ===
---
description: "Critical safety principle that must always be applied to prevent disasters"
category: "core-foundation"
priority: "critical"
alwaysApply: true
globs: ["**/*"]
tags: ['core_foundation', 'safety', 'critical']
tier: "1"
---

# Safety First Principle

**CRITICAL**: Always prioritize safety over speed, convenience, or automation. If we shoot ourselves in the foot, we are not efficient.

## Core Principle

**"Safety First, Efficiency Second"**

Every development decision, automation, script, or process must be evaluated for safety before implementation. The cost of fixing disasters far exceeds the time saved by unsafe shortcuts.

## Safety Requirements

### 1. **No Automatic File Moving**
**MANDATORY**: Never automatically move, rename, or delete files without explicit human approval
```python
# FORBIDDEN: Automatic file operations
def organize_files():
    for file in files:
        move_file(file, new_location)  # DANGEROUS

# REQUIRED: Safe validation only
def validate_file_organization():
    issues = find_organization_issues()
    return generate_fix_suggestions(issues)  # SAFE
```

### 2. **No Destructive Operations**
**MANDATORY**: All destructive operations must require explicit confirmation
```python
# FORBIDDEN: Silent destructive operations
def cleanup():
    delete_all_temp_files()  # DANGEROUS

# REQUIRED: Safe with confirmation
def safe_cleanup():
    temp_files = find_temp_files()
    if confirm_deletion(temp_files):
        delete_files(temp_files)
    else:
        print("Cleanup cancelled - safety first")
```

### 3. **Validation Before Action**
**MANDATORY**: Always validate before taking any action
```python
# REQUIRED: Validate first
def safe_operation():
    # 1. Validate current state
    if not validate_current_state():
        raise SafetyException("Invalid state detected")
    
    # 2. Check prerequisites
    if not check_prerequisites():
        raise SafetyException("Prerequisites not met")
    
    # 3. Perform operation
    perform_operation()
    
    # 4. Validate result
    if not validate_result():
        raise SafetyException("Operation failed validation")
```

### 4. **Rollback Capability**
**MANDATORY**: Every operation must have a clear rollback path
```python
# REQUIRED: Always provide rollback
def safe_database_update():
    # 1. Create backup
    backup = create_backup()
    
    # 2. Perform update
    try:
        perform_update()
        validate_update()
    except Exception as e:
        # 3. Rollback on failure
        rollback_from_backup(backup)
        raise SafetyException(f"Update failed, rolled back: {e}")
```

## Safety Patterns

### 1. **Read-Only by Default**
```python
# REQUIRED: Start with read-only operations
def safe_analysis():
    # Read and analyze only
    data = read_data()
    analysis = analyze_data(data)
    return generate_report(analysis)  # No modifications
```

### 2. **Dry-Run Mode**
```python
# REQUIRED: Always support dry-run
def safe_operation(dry_run=True):
    if dry_run:
        return simulate_operation()
    else:
        return perform_actual_operation()
```

### 3. **Explicit Confirmation**
```python
# REQUIRED: Always require explicit confirmation for dangerous operations
def dangerous_operation():
    print("‚ö†Ô∏è  WARNING: This operation will delete files")
    print("Files to be deleted:", list_files_to_delete())
    
    confirmation = input("Type 'YES' to confirm: ")
    if confirmation != "YES":
        print("Operation cancelled - safety first")
        return False
    
    return perform_deletion()
```

## Safety Checklist

Before any operation, verify:

- [ ] **No automatic file moving/deletion**
- [ ] **Destructive operations require explicit confirmation**
- [ ] **Validation performed before action**
- [ ] **Rollback path exists**
- [ ] **Dry-run mode available**
- [ ] **Read-only analysis first**
- [ ] **Explicit confirmation for dangerous operations**

## Implementation Examples

### Safe File Operations
```python
def safe_file_operation():
    # 1. Analyze current state
    current_files = scan_directory()
    
    # 2. Generate recommendations (read-only)
    recommendations = analyze_file_organization(current_files)
    
    # 3. Present options to user
    print("File organization recommendations:")
    for rec in recommendations:
        print(f"- {rec}")
    
    # 4. Wait for explicit approval
    if user_approves():
        perform_approved_operations()
    else:
        print("Operation cancelled - safety first")
```

### Safe Database Operations
```python
def safe_database_operation():
    # 1. Create backup
    backup = create_database_backup()
    
    # 2. Validate operation
    if not validate_operation_parameters():
        raise SafetyException("Invalid parameters")
    
    # 3. Perform operation with rollback
    try:
        result = perform_database_operation()
        validate_result(result)
        return result
    except Exception as e:
        rollback_from_backup(backup)
        raise SafetyException(f"Operation failed: {e}")
```

## Safety Enforcement

This rule is **ALWAYS ACTIVE** and applies to:

- All file operations
- All database operations
- All system modifications
- All automation scripts
- All deployment operations
- All configuration changes

**Remember**: It's better to be slow and safe than fast and sorry. Safety first, always.



# === intelligent_context_aware_rule_system ===
---
description: "Intelligent context-aware rule selection system that automatically detects development context and applies only relevant rules"
category: "core-foundation"
priority: "critical"
alwaysApply: true
globs: ["**/*"]
tags: ['core_foundation', 'context_awareness', 'rule_selection']
tier: "1"
---


# Intelligent Context-Aware Rule System

**CRITICAL**: Automatically select and apply only the most relevant rules based on current development context, reducing cognitive overhead while maintaining excellence standards. This system serves as the foundation for future agent swarm coordination.

## Description

This rule implements intelligent, context-aware rule selection that automatically detects development context and applies only the most relevant rules. It reduces the active rule set from 39+ rules to 6-10 focused rules per session, achieving 75-85% efficiency improvement while maintaining quality standards.

**Agent Swarm Foundation**: This system serves as the prototype for future agent swarm coordination, where context detection becomes agent selection and rule sets become agent behavioral DNA.

## Core Requirements

### 1. **Dual Context Detection System**
**MANDATORY**: Support both automatic detection and explicit user control

```yaml
context_detection_modes:
  automatic_detection:
    method: "Pattern matching on user queries, files, and directory context"
    confidence_threshold: 0.7
    fallback: "DEFAULT mode if confidence < threshold"
    
  explicit_control:
    method: "Keyword-based user intention signaling"
    priority: "Always overrides automatic detection"
    format: "@keyword at start of user message"
    
  hybrid_approach:
    - "Check for explicit keywords first"
    - "Fall back to automatic detection if no keywords"
    - "Provide transparency on detection method used"
```

### 2. **Context Categories and Rule Sets**
**MANDATORY**: Predefined contexts with optimized rule sets

```yaml
context_categories:
  DEFAULT:
    keywords: ["@default", "@all", "(no keyword)"]
    auto_detect: ["general", "unclear", "mixed"]
    rules: ["safety_first", "no_premature_victory", "boyscout", "context_awareness", "philosophy_separation"]
    agent_future: "GeneralCoordinatorAgent"
    
  CODING:
    keywords: ["@code", "@implement", "@build", "@develop"]
    auto_detect: ["implement", "code", "function", "class", "*.py", "*.js", "*.ts", "src/"]
    rules: ["safety_first", "tdd", "clean_code", "error_handling", "boyscout", "live_documentation"]
    agent_future: "DeveloperAgent"
    
  ARCHITECTURE:
    keywords: ["@design", "@architecture", "@system", "@structure"]
    auto_detect: ["architecture", "design", "system", "structure", "pattern", "docs/architecture/"]
    rules: ["safety_first", "foundational_development", "systematic_construction", "documentation_excellence", "technical_precision"]
    agent_future: "ArchitectAgent"
    
  DEBUGGING:
    keywords: ["@debug", "@troubleshoot", "@fix", "@solve"]
    auto_detect: ["debug", "error", "bug", "issue", "problem", "failing", "logs/"]
    rules: ["safety_first", "systematic_problem_solving", "no_silent_errors", "error_exposure", "test_monitoring"]
    agent_future: "DebuggingAgent"
    
  TESTING:
    keywords: ["@test", "@testing", "@qa", "@validate"]
    auto_detect: ["test", "testing", "*test*.py", "pytest", "unittest", "tests/"]
    rules: ["safety_first", "tdd", "test_monitoring", "no_failing_tests", "test_coverage", "quality_validation"]
    agent_future: "QAAgent"
    
  AGILE:
    keywords: ["@agile", "@sprint", "@story", "@backlog"]
    auto_detect: ["sprint", "backlog", "story", "agile", "scrum", "docs/agile/"]
    rules: ["safety_first", "agile_artifacts_maintenance", "live_documentation_updates", "sprint_management", "user_story_management"]
    agent_future: "ScrumMasterAgent"
    
  GIT_OPERATIONS:
    keywords: ["@git", "@commit", "@push", "@merge"]
    auto_detect: ["git", "commit", "push", "merge", "pull request", "PR", ".git/"]
    rules: ["safety_first", "automated_git_protection", "clean_commit_messages", "merge_validation", "deployment_safety"]
    agent_future: "DevOpsAgent"
    
  DOCUMENTATION:
    keywords: ["@docs", "@document", "@readme", "@guide"]
    auto_detect: ["document", "docs", "readme", "guide", "manual", "*.md", "docs/"]
    rules: ["safety_first", "documentation_excellence", "live_documentation_updates", "clear_communication", "user_experience"]
    agent_future: "TechnicalWriterAgent"
    
  PERFORMANCE:
    keywords: ["@optimize", "@performance", "@benchmark", "@speed"]
    auto_detect: ["optimize", "performance", "speed", "efficiency", "benchmark", "profiling"]
    rules: ["safety_first", "performance_monitoring", "benchmark_validation", "optimization_validation", "scalability_testing"]
    agent_future: "PerformanceAgent"
    
  SECURITY:
    keywords: ["@security", "@secure", "@vulnerability", "@audit"]
    auto_detect: ["security", "secure", "vulnerability", "auth", "encryption", "audit"]
    rules: ["safety_first", "security_vulnerability_assessment", "secure_coding", "penetration_testing", "compliance_validation"]
    agent_future: "SecurityAgent"
```

### 3. **Context Detection Algorithm**
**MANDATORY**: Simple, reliable pattern matching

```python
def detect_context(user_message, open_files, current_directory):
    """
    Detect development context using dual detection system.
    
    Args:
        user_message: User's input message
        open_files: List of currently open files
        current_directory: Current working directory
        
    Returns:
        ContextResult with detected context and confidence
    """
    
    # Step 1: Check for explicit keywords (highest priority)
    explicit_context = check_explicit_keywords(user_message)
    if explicit_context:
        return ContextResult(
            context=explicit_context,
            method="explicit_keyword",
            confidence=1.0,
            reasoning=f"User specified {explicit_context} with keyword"
        )
    
    # Step 2: Automatic detection using pattern matching
    context_scores = {}
    
    # Analyze user message
    message_lower = user_message.lower()
    for context, config in CONTEXT_CATEGORIES.items():
        score = 0
        for pattern in config["auto_detect"]:
            if pattern in message_lower:
                score += 2
        context_scores[context] = score
    
    # Analyze open files
    for file_path in open_files:
        for context, config in CONTEXT_CATEGORIES.items():
            for pattern in config["auto_detect"]:
                if pattern in file_path.lower():
                    context_scores[context] += 1
    
    # Analyze directory context
    dir_lower = current_directory.lower()
    for context, config in CONTEXT_CATEGORIES.items():
        for pattern in config["auto_detect"]:
            if pattern in dir_lower:
                context_scores[context] += 1
    
    # Select best context
    best_context = max(context_scores.items(), key=lambda x: x[1])
    confidence = min(best_context[1] / 5.0, 1.0)  # Normalize to 0-1
    
    if confidence >= 0.7:
        return ContextResult(
            context=best_context[0],
            method="auto_detected",
            confidence=confidence,
            reasoning=f"Auto-detected based on patterns (confidence: {confidence:.1f})"
        )
    else:
        return ContextResult(
            context="DEFAULT",
            method="fallback",
            confidence=0.5,
            reasoning="Low confidence in detection, using DEFAULT mode"
        )

def check_explicit_keywords(message):
    """Check for explicit @keywords in user message."""
    message_lower = message.lower()
    
    keyword_map = {
        "@code": "CODING", "@implement": "CODING", "@build": "CODING", "@develop": "CODING",
        "@design": "ARCHITECTURE", "@architecture": "ARCHITECTURE", "@system": "ARCHITECTURE",
        "@debug": "DEBUGGING", "@troubleshoot": "DEBUGGING", "@fix": "DEBUGGING", "@solve": "DEBUGGING",
        "@test": "TESTING", "@testing": "TESTING", "@qa": "TESTING", "@validate": "TESTING",
        "@agile": "AGILE", "@sprint": "AGILE", "@story": "AGILE", "@backlog": "AGILE",
        "@git": "GIT_OPERATIONS", "@commit": "GIT_OPERATIONS", "@push": "GIT_OPERATIONS", "@merge": "GIT_OPERATIONS",
        "@docs": "DOCUMENTATION", "@document": "DOCUMENTATION", "@readme": "DOCUMENTATION",
        "@optimize": "PERFORMANCE", "@performance": "PERFORMANCE", "@benchmark": "PERFORMANCE",
        "@security": "SECURITY", "@secure": "SECURITY", "@vulnerability": "SECURITY",
        "@default": "DEFAULT", "@all": "DEFAULT"
    }
    
    for keyword, context in keyword_map.items():
        if keyword in message_lower:
            return context
    
    return None
```

### 4. **Rule Application System**
**MANDATORY**: Apply selected rules with transparency

```python
def apply_context_aware_rules(context_result):
    """
    Apply rules based on detected context.
    
    Args:
        context_result: Result from context detection
        
    Returns:
        RuleApplicationResult with active rules and metrics
    """
    
    context = context_result.context
    rule_config = CONTEXT_CATEGORIES[context]
    active_rules = rule_config["rules"]
    
    # Calculate efficiency metrics
    total_available_rules = 39  # Current total rule count
    active_rule_count = len(active_rules)
    efficiency_improvement = ((total_available_rules - active_rule_count) / total_available_rules) * 100
    
    # Log context detection and rule selection
    print(f"üéØ **Context Detected**: {context}")
    print(f"üìã **Detection Method**: {context_result.method}")
    print(f"üîç **Reasoning**: {context_result.reasoning}")
    print(f"üìä **Rules Active**: {active_rule_count} rules loaded")
    print(f"‚ö° **Efficiency**: {efficiency_improvement:.0f}% reduction from full rule set")
    print(f"ü§ñ **Future Agent**: {rule_config['agent_future']}")
    print(f"üìù **Active Rules**: {', '.join(active_rules)}")
    
    return RuleApplicationResult(
        context=context,
        active_rules=active_rules,
        total_rules_available=total_available_rules,
        efficiency_improvement=efficiency_improvement,
        agent_future=rule_config["agent_future"],
        detection_confidence=context_result.confidence
    )
```

### 5. **Agent Swarm Foundation**
**MANDATORY**: Design for future agent swarm coordination

```yaml
agent_swarm_preparation:
  rule_to_agent_mapping:
    - "Each context category maps to future specialized agent"
    - "Rule sets become agent behavioral DNA"
    - "Context detection becomes agent selection logic"
    - "Optimization patterns become swarm coordination intelligence"
    
  scalability_design:
    - "Context detection scales from single AI to multi-agent orchestration"
    - "Rule application scales from individual behavior to swarm coordination"
    - "Efficiency improvements compound across agent swarm"
    - "Learning and optimization scale to collective intelligence"
    
  coordination_protocols:
    - "Shared context understanding across agents"
    - "Inter-agent communication using same context categories"
    - "Coordinated rule application for complex multi-agent tasks"
    - "Swarm-wide optimization using same efficiency metrics"
```

### 6. **Performance and Efficiency**
**MANDATORY**: Achieve significant efficiency improvements

```yaml
efficiency_targets:
  rule_reduction: "75-85% reduction in active rules per session"
  startup_performance: "50% faster session initialization"
  cognitive_load: "80% reduction in rule complexity per task"
  context_accuracy: "90%+ correct context detection"
  
efficiency_monitoring:
  - "Track rule usage patterns and effectiveness"
  - "Monitor context detection accuracy over time"
  - "Measure performance improvements in real sessions"
  - "Collect user satisfaction with focused rule sets"
```

### 7. **Learning and Optimization**
**MANDATORY**: Continuous improvement of context detection

```yaml
learning_system:
  pattern_recognition:
    - "Learn from successful context detections"
    - "Identify patterns that improve detection accuracy"
    - "Adapt detection algorithms based on usage data"
    - "Optimize rule sets based on effectiveness metrics"
    
  feedback_integration:
    - "Collect feedback on context detection accuracy"
    - "Learn from manual context corrections"
    - "Improve auto-detection patterns over time"
    - "Optimize rule combinations for better outcomes"
```

## Implementation Guidelines

### 1. **Rule Metadata Modification**
**CRITICAL**: To enable context-aware rule loading, modify rule metadata as follows:

```yaml
# OLD STRUCTURE (always loads)
# NEW STRUCTURE (context-aware)
```

### 2. **Context-Specific Rule Categories**
**MANDATORY**: Organize rules by context for proper loading:

#### **Always Apply Rules (Tier 1)**
- `safety_first_principle` - Always loaded for safety
- `intelligent_context_aware_rule_system` - The system itself
- `core_rule_application_framework` - Framework for rule application

#### **Context-Dependent Rules (Tier 2)**
- **CODING**: `xp_test_first_development_rule`, `development_core_principles_rule`
- **DEBUGGING**: `development_systematic_problem_solving_rule`, `development_error_exposure_rule`
- **AGILE**: `agile_artifacts_maintenance_rule`, `agile_sprint_management_rule`
- **DOCUMENTATION**: `documentation_live_updates_rule`, `rule_document_excellence_rule`
- **RESEARCH**: `development_context_awareness_excellence_rule`, `development_clear_communication_rule`

### 3. **Session Startup Process**
```yaml
startup_sequence:
  1_initialize: "Load intelligent context-aware rule system"
  2_detect: "Analyze initial context from user query and environment"
  3_select: "Choose appropriate rule set based on context"
  4_apply: "Activate selected rules and provide transparency"
  5_monitor: "Track context changes during session"
  6_adapt: "Adjust rule set if context changes significantly"
```

### 2. **Context Change Handling**
```yaml
context_adaptation:
  trigger_conditions:
    - "User explicitly changes context with new @keyword"
    - "File context changes significantly (different file types)"
    - "Activity type changes (coding ‚Üí debugging ‚Üí documentation)"
    
  adaptation_process:
    - "Detect context change"
    - "Evaluate need for rule set adjustment"
    - "Smoothly transition to new rule set"
    - "Maintain continuity of ongoing work"
```

### 3. **Quality Assurance**
```yaml
quality_maintenance:
  critical_rules_always_active:
    - "safety_first_principle always included"
    - "Core quality standards never compromised"
    - "Essential safety checks always performed"
    
  context_validation:
    - "Validate context detection accuracy"
    - "Ensure appropriate rules for detected context"
    - "Monitor for context detection failures"
    - "Provide fallback to DEFAULT mode when uncertain"
```

## Benefits

### **Immediate Benefits**
- **75-85% Rule Reduction**: From 39 rules to 6-10 focused rules per session
- **50% Faster Startup**: Reduced rule processing overhead
- **Improved Focus**: Only relevant rules for current work
- **Better Performance**: Less cognitive load, faster responses

### **Strategic Benefits**
- **Agent Swarm Foundation**: Architecture ready for multi-agent coordination
- **Scalable Design**: Patterns that work for single AI and agent swarms
- **Learning Capability**: System improves over time
- **Future-Proof**: Designed for evolution to autonomous agent teams

### **User Experience Benefits**
- **Explicit Control**: @keywords for precise rule selection
- **Intelligent Assistance**: Automatic context detection when needed
- **Transparency**: Clear visibility into rule selection reasoning
- **Flexibility**: Easy context switching during development

## Enforcement

This rule is **CONDITIONALLY APPLIED** based on context.

**The intelligent context-aware rule system is the foundation for both current efficiency and future agent swarm coordination.**

## Remember

**"Context awareness enables precision."**

**"Focused rules deliver better results than scattered rules."**

**"Today's rule system is tomorrow's agent swarm DNA."**

**"Efficiency improvements compound across the entire system."**

This system transforms rule management from overwhelming complexity to intelligent precision, while laying the foundation for the future of autonomous software development.
**"Context awareness enables precision."**

**"Focused rules deliver better results than scattered rules."**

**"Today's rule system is tomorrow's agent swarm DNA."**

**"Efficiency improvements compound across the entire system."**

This system transforms rule management from overwhelming complexity to intelligent precision, while laying the foundation for the future of autonomous software development.


# === core_rule_application_framework ===
---
description: "Auto-generated description for core_rule_application_framework.mdc"
category: "core-foundation"
priority: "critical"
alwaysApply: true
globs: ["**/*"]
tags: ['core_foundation']
tier: "2"
---

# Core Rule Application Framework

**CRITICAL**: This framework ensures that critical rules are automatically applied to every situation, task, and development session. The Courage Rule and No Premature Victory Declaration Rule are ALWAYS active.

## Framework Overview

This framework provides automatic rule application that ensures critical rules are never missed, forgotten, or bypassed. It operates at the highest level of our rule system and enforces the most important development principles.

## Core Critical Rules

### **1. Courage and Complete Work Rule** üí™
- **Purpose**: Ensure ALL work is completed systematically, never stopping at partial results
- **Application**: Applied to every work session, task, and problem-solving situation
- **Enforcement**: Automatic application with verification

### **2. No Premature Victory Declaration Rule** üéØ
- **Purpose**: Prevent premature declarations of success without proper validation and enforce concise progress communication
- **Application**: Applied to every progress report, completion claim, status update, and communication
- **Enforcement**: Automatic validation before any success declaration and conciseness requirements

### **3. No Failing Tests Rule** üß™
- **Purpose**: Ensure zero tolerance for failing tests
- **Application**: Applied to every development session and commit
- **Enforcement**: Automatic test execution and failure blocking

### **4. Boy Scout Rule** üèïÔ∏è
- **Purpose**: Always leave the codebase cleaner than found
- **Application**: Applied to every code modification and cleanup
- **Enforcement**: Automatic cleanup and improvement verification

## Framework Implementation

### **Automatic Rule Application System**

```python
class CoreRuleApplicationFramework:
    """Always-active framework that applies critical rules automatically."""
    
    def __init__(self):
        self.critical_rules = {
            "courage_rule": {
                "file": "development_courage_completion_rule.mdc",
                "priority": "critical",
                "always_apply": True,
                "triggers": ["work_start", "problem_encountered", "progress_report", "work_completion"]
            },
            "no_premature_victory_declaration_rule": {
                "file": "no_premature_victory_declaration_rule.mdc",
                "priority": "critical", 
                "always_apply": True,
                "triggers": ["progress_report", "success_declaration", "completion_claim", "communication", "status_update"]
            },
            "no_failing_tests_rule": {
                "file": "no_failing_tests_rule.mdc",
                "priority": "critical",
                "always_apply": True,
                "triggers": ["development_session", "code_change", "commit_preparation"]
            },
            "boyscout_rule": {
                "file": "boyscout_leave_cleaner_rule.mdc",
                "priority": "critical",
                "always_apply": True,
                "triggers": ["code_modification", "cleanup", "session_end"]
            }
        }
    
    def apply_critical_rules(self, context: dict) -> dict:
        """Apply all critical rules to the current context."""
        
        print("üöÄ APPLYING CRITICAL RULES FRAMEWORK")
        
        # Apply courage rule
        context = self.apply_courage_rule(context)
        
        # Apply no premature victory declaration rule
        context = self.apply_no_premature_victory_declaration_rule(context)
        
        # Apply no failing tests rule
        context = self.apply_no_failing_tests_rule(context)
        
        # Apply boy scout rule
        context = self.apply_boyscout_rule(context)
        
        print("‚úÖ CRITICAL RULES FRAMEWORK APPLIED")
        return context
    
    def apply_courage_rule(self, context: dict) -> dict:
        """Apply courage rule to ensure complete work."""
        
        print("üí™ APPLYING COURAGE RULE")
        
        # Courage declaration
        context["courage_declaration"] = {
            "timestamp": datetime.now(),
            "commitment": "Complete ALL work systematically",
            "no_partial_results": True,
            "systematic_approach": True
        }
        
        # Courage verification
        context["courage_verification"] = {
            "work_complete": False,  # Will be set to True only when 100% complete
            "partial_progress_acceptable": False,
            "systematic_problem_solving": True
        }
        
        print("‚úÖ COURAGE RULE APPLIED")
        return context
    
    def apply_no_premature_victory_declaration_rule(self, context: dict) -> dict:
        """Apply no premature victory declaration rule to prevent false claims and enforce concise communication."""
        
        print("üéØ APPLYING NO PREMATURE VICTORY DECLARATION RULE")
        
        # Success validation and conciseness requirements
        context["success_validation"] = {
            "evidence_required": True,
            "test_validation_required": True,
            "complete_verification_required": True,
            "no_assumptions_allowed": True,
            "concise_communication_required": True,
            "progress_only_output": True,
            "max_words_status_update": 100,
            "max_words_progress_report": 50
        }
        
        # Success declaration guard
        context["success_declaration_guard"] = {
            "validation_complete": False,
            "evidence_provided": False,
            "tests_passing": False,
            "ready_for_declaration": False
        }
        
        print("‚úÖ NO PREMATURE VICTORY DECLARATION RULE APPLIED")
        return context
    
    def apply_no_failing_tests_rule(self, context: dict) -> dict:
        """Apply no failing tests rule to ensure test quality."""
        
        print("üß™ APPLYING NO FAILING TESTS RULE")
        
        # Test execution requirements
        context["test_requirements"] = {
            "all_tests_must_pass": True,
            "zero_tolerance_for_failures": True,
            "test_execution_required": True,
            "failure_blocking": True
        }
        
        # Test validation
        context["test_validation"] = {
            "tests_executed": False,
            "all_tests_passing": False,
            "failures_fixed": False,
            "ready_to_proceed": False
        }
        
        print("‚úÖ NO FAILING TESTS RULE APPLIED")
        return context
    
    def apply_boyscout_rule(self, context: dict) -> dict):
        """Apply boy scout rule to ensure codebase improvement."""
        
        print("üèïÔ∏è APPLYING BOY SCOUT RULE")
        
        # Cleanup requirements
        context["cleanup_requirements"] = {
            "leave_cleaner_than_found": True,
            "proactive_improvements": True,
            "code_quality_enhancement": True,
            "documentation_improvement": True
        }
        
        # Cleanup validation
        context["cleanup_validation"] = {
            "cleanup_performed": False,
            "improvements_made": False,
            "codebase_enhanced": False,
            "ready_for_commit": False
        }
        
        print("‚úÖ BOY SCOUT RULE APPLIED")
        return context
```

### **Rule Application Triggers**

```python
class RuleApplicationTriggers:
    """Automatic triggers for critical rule application."""
    
    def __init__(self):
        self.framework = CoreRuleApplicationFramework()
    
    def on_work_start(self, context: dict) -> dict:
        """Trigger when work begins."""
        
        print("üöÄ WORK START TRIGGER - APPLYING CRITICAL RULES")
        
        # Apply courage rule
        context = self.framework.apply_courage_rule(context)
        
        # Apply no failing tests rule
        context = self.framework.apply_no_failing_tests_rule(context)
        
        # Apply boy scout rule
        context = self.framework.apply_boyscout_rule(context)
        
        return context
    
    def on_progress_report(self, context: dict) -> dict:
        """Trigger when reporting progress."""
        
        print("üìä PROGRESS REPORT TRIGGER - APPLYING CRITICAL RULES")
        
        # Apply no premature success rule
        context = self.framework.apply_no_premature_victory_declaration_rule(context)
        
        # Apply courage rule verification
        context = self.verify_courage_rule_compliance(context)
        
        return context
    
    def on_success_declaration(self, context: dict) -> dict:
        """Trigger when declaring success."""
        
        print("üéâ SUCCESS DECLARATION TRIGGER - APPLYING CRITICAL RULES")
        
        # Apply no premature success rule with strict validation
        context = self.framework.apply_no_premature_victory_declaration_rule(context)
        
        # Validate success declaration
        context = self.validate_success_declaration(context)
        
        return context
    
    def on_work_completion(self, context: dict) -> dict:
        """Trigger when work appears complete."""
        
        print("üèÅ WORK COMPLETION TRIGGER - APPLYING CRITICAL RULES")
        
        # Apply all critical rules for final validation
        context = self.framework.apply_critical_rules(context)
        
        # Final validation
        context = self.validate_complete_success(context)
        
        return context
    
    def verify_courage_rule_compliance(self, context: dict) -> dict:
        """Verify compliance with courage rule."""
        
        if not context.get("courage_declaration"):
            print("‚ùå COURAGE RULE VIOLATION: No courage declaration found")
            context = self.framework.apply_courage_rule(context)
        
        return context
    
    def validate_success_declaration(self, context: dict) -> dict:
        """Validate success declaration against no premature success rule."""
        
        validation = context.get("success_validation", {})
        guard = context.get("success_declaration_guard", {})
        
        if not validation.get("evidence_required"):
            print("‚ùå NO PREMATURE SUCCESS VIOLATION: Evidence required")
            raise ValueError("Success declaration requires evidence")
        
        if not guard.get("validation_complete"):
            print("‚ùå NO PREMATURE SUCCESS VIOLATION: Validation not complete")
            raise ValueError("Success declaration requires complete validation")
        
        return context
    
    def validate_complete_success(self, context: dict) -> dict:
        """Validate complete success against all critical rules."""
        
        # Check courage rule compliance
        courage_verification = context.get("courage_verification", {})
        if not courage_verification.get("work_complete"):
            print("‚ùå COURAGE RULE VIOLATION: Work not complete")
            raise ValueError("Work must be 100% complete")
        
        # Check no premature success compliance
        success_guard = context.get("success_declaration_guard", {})
        if not success_guard.get("ready_for_declaration"):
            print("‚ùå NO PREMATURE SUCCESS VIOLATION: Not ready for declaration")
            raise ValueError("Success declaration not ready")
        
        # Check no failing tests compliance
        test_validation = context.get("test_validation", {})
        if not test_validation.get("all_tests_passing"):
            print("‚ùå NO FAILING TESTS VIOLATION: Tests not passing")
            raise ValueError("All tests must pass")
        
        # Check boy scout rule compliance
        cleanup_validation = context.get("cleanup_validation", {})
        if not cleanup_validation.get("codebase_enhanced"):
            print("‚ùå BOY SCOUT RULE VIOLATION: Codebase not enhanced")
            raise ValueError("Codebase must be enhanced")
        
        print("‚úÖ COMPLETE SUCCESS VALIDATED - ALL CRITICAL RULES COMPLIANT")
        return context
```

### **Automatic Integration System**

```python
class AutomaticRuleIntegration:
    """Automatic integration of critical rules into all workflows."""
    
    def __init__(self):
        self.triggers = RuleApplicationTriggers()
        self.framework = CoreRuleApplicationFramework()
    
    def integrate_into_session_startup(self, context: dict) -> dict:
        """Integrate critical rules into session startup."""
        
        print("üöÄ INTEGRATING CRITICAL RULES INTO SESSION STARTUP")
        
        # Apply all critical rules
        context = self.framework.apply_critical_rules(context)
        
        # Apply work start triggers
        context = self.triggers.on_work_start(context)
        
        return context
    
    def integrate_into_progress_reporting(self, context: dict) -> dict:
        """Integrate critical rules into progress reporting."""
        
        print("üìä INTEGRATING CRITICAL RULES INTO PROGRESS REPORTING")
        
        # Apply progress report triggers
        context = self.triggers.on_progress_report(context)
        
        return context
    
    def integrate_into_success_declaration(self, context: dict) -> dict:
        """Integrate critical rules into success declaration."""
        
        print("üéâ INTEGRATING CRITICAL RULES INTO SUCCESS DECLARATION")
        
        # Apply success declaration triggers
        context = self.triggers.on_success_declaration(context)
        
        return context
    
    def integrate_into_work_completion(self, context: dict) -> dict:
        """Integrate critical rules into work completion."""
        
        print("üèÅ INTEGRATING CRITICAL RULES INTO WORK COMPLETION")
        
        # Apply work completion triggers
        context = self.triggers.on_work_completion(context)
        
        return context
```

## Framework Usage

### **Automatic Application**

The framework automatically applies critical rules to every situation:

```python
# Initialize framework
integration = AutomaticRuleIntegration()

# Session startup with critical rules
context = integration.integrate_into_session_startup(context)

# Progress reporting with critical rules
context = integration.integrate_into_progress_reporting(context)

# Success declaration with critical rules
context = integration.integrate_into_success_declaration(context)

# Work completion with critical rules
context = integration.integrate_into_work_completion(context)
```

### **Manual Application**

For specific situations, apply critical rules manually:

```python
# Initialize framework
framework = CoreRuleApplicationFramework()

# Apply all critical rules
context = framework.apply_critical_rules(context)

# Apply specific rule
context = framework.apply_courage_rule(context)
context = framework.apply_no_premature_victory_declaration_rule(context)
```

## Framework Benefits

### **1. Always Active Critical Rules**
- Courage rule automatically applied to every work session
- No premature success rule automatically applied to every progress report
- No failing tests rule automatically applied to every development session
- Boy scout rule automatically applied to every code modification

### **2. Automatic Integration**
- No manual rule application required
- Rules integrated into all workflows automatically
- Consistent rule application across all situations
- Zero chance of missing critical rules

### **3. Comprehensive Validation**
- Automatic validation of rule compliance
- Prevention of rule violations
- Clear feedback on rule application
- Systematic enforcement of critical principles

### **4. Systematic Organization**
- Clear rule hierarchy and priorities
- Automatic rule discovery and application
- Consistent rule enforcement patterns
- Easy rule management and maintenance

## Framework Enforcement

### **Automatic Enforcement**
- Critical rules applied automatically to every situation
- No manual intervention required
- Consistent enforcement across all contexts
- Zero tolerance for rule violations

### **Validation Enforcement**
- Automatic validation of rule compliance
- Prevention of premature success declarations
- Enforcement of courage-based work completion
- Systematic test quality assurance

### **Integration Enforcement**
- Automatic integration into all workflows
- Seamless rule application without disruption
- Consistent rule behavior across all systems
- Reliable rule enforcement mechanisms

## Framework Monitoring

### **Rule Application Monitoring**
- Track rule application frequency
- Monitor rule compliance rates
- Measure rule effectiveness
- Identify rule application gaps

### **Performance Monitoring**
- Monitor framework performance impact
- Track rule application speed
- Measure system resource usage
- Optimize rule application efficiency

### **Effectiveness Monitoring**
- Monitor rule effectiveness metrics
- Track rule violation rates
- Measure rule compliance improvements
- Validate rule application outcomes

## Framework Maintenance

### **Regular Updates**
- Update rule definitions as needed
- Enhance rule application logic
- Improve rule validation mechanisms
- Optimize rule performance

### **Continuous Improvement**
- Monitor rule effectiveness
- Identify improvement opportunities
- Implement rule enhancements
- Validate rule improvements

### **Documentation Updates**
- Keep framework documentation current
- Update usage examples
- Maintain rule application guides
- Document best practices

## Conclusion

This Core Rule Application Framework ensures that our most critical rules - the Courage Rule and No Premature Success Rule - are always active and properly applied to every situation. It provides automatic integration, comprehensive validation, and systematic enforcement of our core development principles.

**The framework guarantees that:**
- **Courage Rule** is applied to every work session
- **No Premature Success Rule** is applied to every progress report
- **No Failing Tests Rule** is applied to every development session
- **Boy Scout Rule** is applied to every code modification

This ensures our goal of **fully automated, crystal clear code production** with **accurate tests and documentation** and **up-to-date project tracking** following **excellence standards**.



# === user_controlled_success_declaration_rule ===
# User Controlled Success Declaration Rule

**CRITICAL**: In specific situations where the user explicitly states they want to see results first and will generate their own success messages, NEVER generate success declarations, victory messages, or completion summaries. Let the user see the actual work results and decide on success themselves.

## Core Principle

**"User Controls Success When Explicitly Requested"**

When the user says phrases like:
- "we must see the result of your work immediately"
- "never generate success messages in situations like that"
- "i need a rule for that, that I am in control of success"
- "stop this in situations like that"

The user is taking control of success declaration. Respect this control.

## Requirements

### 1. **No Success Messages When User Controls**
- Never generate "‚úÖ Success!" messages
- Never create completion summaries
- Never declare victory or completion
- Never use celebration emojis or success indicators
- Never say "work is complete" or similar phrases

### 2. **Show Results Only**
- Present actual work results
- Show data, files, or outputs
- Display verification results
- Provide evidence of work done
- Let results speak for themselves

### 3. **Wait for User Assessment**
- Let user evaluate the results
- Wait for user's success declaration
- Don't preempt user's judgment
- Respect user's authority in these situations

## Enforcement

This rule is **ALWAYS ACTIVE** and applies when:

- User explicitly states they control success declaration
- User says "never generate success messages"
- User wants to see results first
- User takes authority over completion assessment

## Examples

### ‚ùå FORBIDDEN (When User Controls Success)
```
‚úÖ Rule system optimization complete!
üéâ Success! All issues resolved!
üèÜ Work finished successfully!
```

### ‚úÖ ALLOWED (When User Controls Success)
```
Results of rule system verification:
- 3 core rules with alwaysApply: true
- 38 context rules with alwaysApply: false
- System health score: 80/100
```

## Remember

**"When user controls success, show results and wait."**

**"Let the user decide what constitutes success."**

**"Results speak louder than success declarations."**



# === scientific_communication_rule ===
---
description: "Enforce scientific, rational communication without emotional decorations or marketing language"
category: "core-foundation"
priority: "critical"
alwaysApply: true
globs: ["**/*"]
tags: ['core_foundation', 'communication', 'scientific']
tier: "1"
---

# Scientific Communication Rule

**CRITICAL**: All communication must be scientific, rational, and factual. No emotional decorations, marketing language, or unnecessary embellishments.

## Core Principle

**"Rationality and Clarity Over Emotions and Marketing"**

Communication must be based on facts, evidence, and logical reasoning. Emotional language and marketing-style presentations are prohibited.

## Requirements

### 1. **Factual Communication Only**
- State facts without emotional interpretation
- Use precise, measurable language
- Provide evidence for claims
- Avoid subjective opinions presented as facts

### 2. **No Emotional Decorations**
**FORBIDDEN:**
- Celebration emojis (üéâ, ‚ú®, üöÄ)
- Emotional exclamations ("Amazing!", "Fantastic!")
- Marketing superlatives ("incredible", "revolutionary", "game-changing")
- Dramatic language ("breakthrough", "stunning", "phenomenal")

**REQUIRED:**
- Neutral, descriptive language
- Objective measurements
- Clear, direct statements
- Evidence-based conclusions

### 3. **Scientific Precision**
- Use specific measurements instead of vague terms
- Provide quantifiable results
- State confidence levels and uncertainty
- Document methodology and assumptions

### 4. **Rational Structure**
- Present information logically
- Separate observations from conclusions
- Show cause-and-effect relationships clearly
- Use systematic problem-solving approaches

## Examples

### ‚ùå FORBIDDEN (Emotional/Marketing Style)
```
üéâ Amazing breakthrough! Our revolutionary system delivers incredible results!
‚ú® Fantastic performance improvements that will blow your mind!
üöÄ Game-changing solution with phenomenal efficiency gains!
```

### ‚úÖ REQUIRED (Scientific Style)
```
System performance analysis:
- Rule count reduced from 33 to 5 (84.8% reduction)
- Context detection accuracy: 95% (19/20 test cases)
- File generation time: 0.3 seconds average
- Memory usage decreased by 67%
```

## Communication Standards

### 1. **Data Presentation**
- Always include specific numbers
- Show before/after comparisons
- Provide confidence intervals when applicable
- State sample sizes and test conditions

### 2. **Problem Description**
- Define problems objectively
- Quantify impact where possible
- Identify root causes systematically
- Separate symptoms from causes

### 3. **Solution Documentation**
- Describe methodology clearly
- Show implementation steps
- Provide verification methods
- Document limitations and assumptions

### 4. **Progress Reporting**
- Use measurable metrics
- Show completion percentages
- Report actual vs. expected results
- Identify remaining work objectively

## Enforcement

This rule is **ALWAYS ACTIVE** and applies to:

- All written communication
- All progress reports
- All documentation
- All code comments
- All user interactions

## Remember

**"Facts over feelings, evidence over excitement, clarity over celebration."**

**"Measure twice, communicate once."**

**"Rational discourse produces rational results."**


# === streamlined_git_operations_rule ===
---
description: "Streamlined git operations - execute only commit and push in standard situations"
category: "core-foundation"
priority: "critical"
alwaysApply: true
globs: ["**/*"]
tags: ['core_foundation', 'git_operations', 'streamlined']
tier: "1"
---

# Streamlined Git Operations Rule

**CRITICAL**: In standard development situations, execute only `git commit` and `git push` commands. Avoid unnecessary git status checks, staging commands, or other git operations unless explicitly requested.

## Core Principle

**"Commit and Push Only - Keep It Simple"**

When the user requests git operations for standard development workflow, execute only the essential commands: commit and push. This reduces command overhead and streamlines the development process.

## Standard Git Workflow

### 1. **Standard Commit and Push Sequence**
**REQUIRED for standard situations:**
```bash
git commit -m "descriptive commit message"
git push
```

**FORBIDDEN in standard situations:**
- `git status` (unless explicitly requested)
- `git add .` (assume files are already staged by IDE)
- `git branch` checks
- `git log` reviews
- Multiple git commands in sequence

### 2. **When to Use Standard Workflow**
**Apply streamlined approach when:**
- User requests "commit" or "push" 
- User says "let's commit now"
- User asks "should we commit?"
- Standard development workflow context
- No specific git issues mentioned

**Do NOT use streamlined approach when:**
- User explicitly requests git status
- User mentions merge conflicts
- User asks about branches
- User requests specific git diagnostics
- Error conditions are suspected

### 3. **Commit Message Standards**
**REQUIRED format:**
- Clear, descriptive commit messages
- Present tense, imperative mood
- Include feature/fix context
- Reference user story or issue when applicable

**Examples:**
```
"Implement intelligent context-aware rule system"
"Fix rule metadata for context detection"
"Update documentation for US-E0-010 completion"
"Add comprehensive validation suite for context system"
```

### 4. **Error Handling**
**If commit fails:**
- Show the error message
- Ask user for guidance
- Do not automatically run additional git commands

**If push fails:**
- Show the error message
- Ask user for guidance
- Do not automatically attempt pull or merge

## Implementation Guidelines

### 1. **Command Execution**
```python
# REQUIRED: Streamlined approach
def standard_git_workflow(commit_message):
    run_command(f"git commit -m '{commit_message}'")
    run_command("git push")

# FORBIDDEN: Verbose approach
def verbose_git_workflow():
    run_command("git status")  # Unnecessary
    run_command("git add .")   # Assume already staged
    run_command("git status")  # Redundant
    run_command("git commit -m 'message'")
    run_command("git status")  # Unnecessary
    run_command("git push")
```

### 2. **Context Detection**
**Standard situations:**
- User in normal development flow
- Files already staged by IDE
- No git issues mentioned
- Routine commit and push request

**Non-standard situations:**
- User mentions git problems
- User explicitly requests git status
- User asks about branches or history
- Error conditions present

### 3. **User Communication**
**REQUIRED communication:**
- Confirm commit message before executing
- Report success or failure clearly
- Ask for guidance if errors occur

**FORBIDDEN communication:**
- Verbose git status reports
- Unnecessary command explanations
- Multiple status updates during execution

## Exception Handling

### 1. **When to Break the Rule**
**Use full git workflow when:**
- User explicitly requests `git status`
- User mentions "check status" or "what's changed"
- User asks about branches, history, or conflicts
- Error conditions require diagnosis
- User is troubleshooting git issues

### 2. **Error Recovery**
**If streamlined approach fails:**
- Show the specific error
- Ask user what they want to do
- Do not automatically escalate to verbose commands
- Let user decide next steps

### 3. **User Override**
**User can override by:**
- Explicitly requesting specific git commands
- Mentioning they want to see status
- Asking for git diagnostics
- Indicating non-standard workflow needs

## Benefits

### 1. **Efficiency**
- Faster commit and push cycles
- Reduced command overhead
- Streamlined development workflow
- Less terminal output clutter

### 2. **Simplicity**
- Clear, predictable behavior
- Fewer commands to execute
- Reduced cognitive load
- Focus on development, not git mechanics

### 3. **Reliability**
- Consistent behavior in standard cases
- Predictable command sequence
- Clear error handling
- User maintains control

## Integration with Context System

### 1. **Git Context Detection**
**When @git keyword used:**
- Apply streamlined approach by default
- Execute commit and push only
- Avoid unnecessary git commands

### 2. **Other Context Integration**
**When other contexts active:**
- Still apply streamlined git when git operations requested
- Maintain context-appropriate behavior
- Keep git operations simple regardless of context

### 3. **Rule Priority**
**This rule takes precedence for:**
- Standard git operations
- Routine commit and push requests
- Normal development workflow

## Enforcement

This rule is **ALWAYS ACTIVE** and applies to:

- All git operation requests
- Standard development workflows
- Routine commit and push cycles
- Normal development contexts

## Remember

**"Commit and push - that's it. Keep git simple."**

**"Let the IDE handle staging, we handle committing."**

**"Two commands, not twenty."**


# === error_handling_no_silent_errors_rule ===
---
description: "Never use silent error handling, mock fallbacks, or placeholder data that masks real parsing failures. All errors must be exposed and addressed properly."
category: "quality-standards"
priority: "high"
alwaysApply: false
contexts: ["DEBUGGING", "CODING", "TESTING", "DEFAULT"]
globs: ["**/*"]
tags: ["error-handling", "quality", "reliability", "debugging"]
tier: "2"
---

# No Silent Errors and Mock Fallbacks Rule

**CRITICAL**: Never use silent error handling, mock fallbacks, or placeholder data that masks real parsing failures. All errors must be exposed and addressed properly to ensure system reliability and quality.

## Core Requirements

### 1. Error Exposure Requirements
**MANDATORY**: All errors must be exposed, never silenced
```python
# CORRECT: Expose errors immediately
try:
    result = parser.parse(response)
    return result
except Exception as e:
    logger.error(f"Parsing failed: {e}")
    raise OutputParserException(f"Failed to parse response: {e}")

# INCORRECT: Silent error handling with fallbacks
try:
    result = parser.parse(response)
    return result
except Exception as e:
    logger.warning(f"Parsing failed, using fallback: {e}")
    return get_fallback_data()  # NEVER DO THIS
```

### 2. Fallback Prohibition
**FORBIDDEN**: Mock fallbacks, placeholder data, or silent error recovery
```python
# FORBIDDEN: Mock fallback data
def _get_fallback_data(self):
    return {
        "status": "completed",
        "message": "Fallback data used",
        "data": {}  # Empty/mock data
    }

# FORBIDDEN: Silent error handling
except Exception as e:
    logger.warning(f"Error occurred: {e}")
    return {}  # Silent failure
```

### 3. Error Propagation Requirements

#### **Immediate Error Exposure**
```python
# MANDATORY: Always raise exceptions for parsing failures
def parse_response(self, response: str) -> Dict[str, Any]:
    try:
        # Primary parsing attempt
        return self._primary_parse(response)
    except Exception as e:
        # Log the error and re-raise
        self.logger.error(f"Primary parsing failed: {e}")
        raise OutputParserException(f"Failed to parse response: {e}")
```

#### **No Graceful Degradation**
```python
# FORBIDDEN: Graceful degradation with fallbacks
def parse_with_fallbacks(self, response: str):
    try:
        return self._primary_parse(response)
    except Exception as e:
        try:
            return self._secondary_parse(response)  # FORBIDDEN
        except Exception as e2:
            return self._get_fallback_data()  # FORBIDDEN
```

### 4. Error Handling Standards

#### **Exception Types**
```python
# MANDATORY: Use specific exception types
class OutputParserException(Exception):
    """Raised when output parsing fails."""
    pass

class ValidationException(Exception):
    """Raised when data validation fails."""
    pass

class FormatException(Exception):
    """Raised when output format is invalid."""
    pass
```

#### **Error Context Requirements**
```python
# MANDATORY: Include context in error messages
def parse_response(self, response: str, agent_type: str) -> Dict[str, Any]:
    try:
        return self._parse(response)
    except Exception as e:
        error_msg = f"Parsing failed for {agent_type}: {e}. Response: {response[:200]}..."
        raise OutputParserException(error_msg)
```

### 5. Logging Requirements

#### **Error Logging Standards**
```python
# MANDATORY: Comprehensive error logging
import logging

logger = logging.getLogger(__name__)

def parse_response(self, response: str) -> Dict[str, Any]:
    try:
        return self._parse(response)
    except Exception as e:
        logger.error(f"Parsing failed: {e}")
        logger.error(f"Response content: {response}")
        logger.error(f"Agent type: {self.agent_type}")
        logger.error(f"Parser configuration: {self.config}")
        raise  # Re-raise the exception
```

#### **No Warning-Only Logging**
```python
# FORBIDDEN: Warning-only logging for errors
except Exception as e:
    logger.warning(f"Parsing failed, continuing with fallback: {e}")  # FORBIDDEN
    return fallback_data

# CORRECT: Error logging with exception raising
except Exception as e:
    logger.error(f"Parsing failed: {e}")
    raise OutputParserException(f"Parsing failed: {e}")
```

### 6. Testing Requirements

#### **Error Testing**
```python
# MANDATORY: Test error conditions
def test_parsing_failure_exposure():
    """Test that parsing failures are properly exposed."""
    parser = OutputParser()
    
    # Test with invalid response
    with pytest.raises(OutputParserException):
        parser.parse("invalid response")
    
    # Test with malformed JSON
    with pytest.raises(OutputParserException):
        parser.parse('{"incomplete": json')
    
    # Test with missing required fields
    with pytest.raises(ValidationException):
        parser.parse('{"partial": "data"}')
```

#### **No Fallback Testing**
```python
# FORBIDDEN: Test fallback mechanisms
def test_fallback_mechanism():  # FORBIDDEN
    parser = OutputParser()
    result = parser.parse("invalid response")
    assert result == fallback_data  # FORBIDDEN

# CORRECT: Test error exposure
def test_error_exposure():
    parser = OutputParser()
    with pytest.raises(OutputParserException):
        parser.parse("invalid response")
```

### 7. Agent Integration Requirements

#### **Agent Error Handling**
```python
# MANDATORY: Agents must fail on parsing errors
def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
    try:
        response = await self._generate_response(state)
        parsed_response = self.parser.parse(response)
        return self._update_state(state, parsed_response)
    except OutputParserException as e:
        # Log the error and fail the agent
        self.logger.error(f"Agent {self.name} failed due to parsing error: {e}")
        raise AgentExecutionException(f"Agent {self.name} failed: {e}")
```

#### **No Agent Fallbacks**
```python
# FORBIDDEN: Agent fallback mechanisms
def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
    try:
        response = await self._generate_response(state)
        parsed_response = self.parser.parse(response)
        return self._update_state(state, parsed_response)
    except Exception as e:
        # FORBIDDEN: Agent fallback
        return self._get_fallback_state(state)  # NEVER DO THIS
```

### 8. Workflow Integration Requirements

#### **Workflow Error Propagation**
```python
# MANDATORY: Workflow must fail on agent errors
async def execute_workflow(self, state: Dict[str, Any]) -> Dict[str, Any]:
    for agent in self.agents:
        try:
            state = await agent.execute(state)
        except AgentExecutionException as e:
            # Log and fail the entire workflow
            self.logger.error(f"Workflow failed due to agent error: {e}")
            raise WorkflowExecutionException(f"Workflow failed: {e}")
    
    return state
```

#### **No Workflow Fallbacks**
```python
# FORBIDDEN: Workflow fallback mechanisms
async def execute_workflow(self, state: Dict[str, Any]) -> Dict[str, Any]:
    for agent in self.agents:
        try:
            state = await agent.execute(state)
        except Exception as e:
            # FORBIDDEN: Workflow fallback
            state = self._get_fallback_state(state)  # NEVER DO THIS
            continue  # NEVER DO THIS
    
    return state
```

### 9. Configuration Requirements

#### **Parser Configuration**
```python
# MANDATORY: Configure parsers to fail on errors
class OutputParser:
    def __init__(self, fail_on_error: bool = True):
        self.fail_on_error = True  # Always true, no option to disable
        self.allow_fallbacks = False  # Always false
        self.silent_errors = False  # Always false
```

#### **No Fallback Configuration**
# Code example removed for brevity


### 10. Code Review Requirements

#### **Mandatory Checks**
- [ ] No fallback data or mock responses
- [ ] No silent error handling
- [ ] All exceptions are properly raised
- [ ] Error logging is comprehensive
- [ ] No graceful degradation mechanisms
- [ ] No placeholder or default data

#### **Forbidden Patterns**
# Code example removed for brevity


### 11. Implementation Guidelines

#### **Error Handling Template**
# Code example removed for brevity


#### **Agent Error Handling Template**
# Code example removed for brevity


### 12. Benefits

- **Improved Reliability**: No hidden failures or silent errors
- **Better Debugging**: All errors are exposed and logged
- **Quality Assurance**: Forces proper error handling and validation
- **System Transparency**: Clear visibility into all failures
- **Maintenance**: Easier to identify and fix issues

### 13. Enforcement

This rule is **CONDITIONALLY APPLIED** based on context.

**Violations of this rule require immediate remediation and removal of all fallback mechanisms.**

### 14. Migration Guide

#### **Removing Existing Fallbacks**
1. **Identify Fallback Code**: Find all fallback mechanisms
2. **Replace with Exceptions**: Convert fallbacks to proper exceptions
3. **Update Error Handling**: Ensure all errors are properly logged and raised
4. **Update Tests**: Remove fallback tests, add error exposure tests
5. **Update Documentation**: Remove references to fallback mechanisms

#### **Example Migration**
# Code example removed for brevity


**Remember**: The goal is to expose all errors, not hide them. This ensures system reliability and forces proper error handling and validation.



# === testing_test_monitoring_rule ===
---
description: "Auto-generated description for testing_test_monitoring_rule.mdc"
category: "testing-standards"
priority: "high"
alwaysApply: false
contexts: ['TESTING', 'CODING', 'DEBUGGING']
globs: ["**/*"]
tags: ['testing_standards', 'testing', 'validation']
tier: "2"
---
# Test Monitoring Rule

# Test Monitoring Rule

**CRITICAL**: Implement automated test monitoring with immediate error detection and systematic bug fixing workflow to maintain zero failing tests policy.

## Core Requirements

### 1. Automated Test Monitoring
**MANDATORY**: Monitor all test executions automatically with immediate error detection.

**Monitoring Requirements**:
- **Real-time Monitoring**: Monitor test execution in real-time
- **Immediate Error Detection**: Detect and report errors immediately
- **Comprehensive Logging**: Log all test execution details
- **Performance Tracking**: Track test execution time and performance
- **Failure Analysis**: Analyze test failures for root causes

**Implementation**:
```python
# ‚úÖ CORRECT: Automated Test Monitoring
import pytest
import time
import logging
from typing import Dict, List, Optional
from datetime import datetime
import subprocess
import sys

class TestMonitor:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.test_results = {}
        self.failure_analysis = FailureAnalyzer()
        self.alert_system = AlertSystem()
        self.performance_tracker = PerformanceTracker()
    
    def monitor_test_execution(self, test_path: str = None) -> TestExecutionReport:
        """Monitor test execution with comprehensive tracking"""
        start_time = time.time()
        execution_report = TestExecutionReport()
        
        try:
            # Run tests with monitoring
            test_results = self.run_tests_with_monitoring(test_path)
            
            # Analyze results
            execution_report = self.analyze_test_results(test_results)
            
            # Check for failures
            if execution_report.has_failures():
                self.handle_test_failures(execution_report)
            
            # Track performance
            execution_time = time.time() - start_time
            self.performance_tracker.record_execution(test_path, execution_time, execution_report)
            
            return execution_report
            
        except Exception as e:
            self.logger.error(f"Test monitoring failed: {e}")
            execution_report.add_error(f"Monitoring error: {e}")
            return execution_report
    
    def run_tests_with_monitoring(self, test_path: str = None) -> Dict:
        """Run tests with comprehensive monitoring"""
        cmd = ["python", "-m", "pytest"]
        
        if test_path:
            cmd.append(test_path)
        
        # Add monitoring options
        cmd.extend([
            "--tb=short",  # Short traceback format
            "--strict-markers",  # Strict marker validation
            "--disable-warnings",  # Disable warnings for cleaner output
            "--durations=10",  # Show 10 slowest tests
            "--maxfail=1",  # Stop on first failure
            "-v"  # Verbose output
        ])
        
        try:
            # Run tests with real-time output capture
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            # Monitor output in real-time
            test_output = []
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    test_output.append(output.strip())
                    # Check for immediate failures
                    if self.detect_immediate_failure(output):
                        self.alert_system.send_immediate_alert(f"Test failure detected: {output}")
            
            # Get return code
            return_code = process.poll()
            
            return {
                "return_code": return_code,
                "output": test_output,
                "success": return_code == 0
            }
            
        except Exception as e:
            self.logger.error(f"Test execution failed: {e}")
            return {
                "return_code": -1,
                "output": [f"Test execution error: {e}"],
                "success": False
            }
    
    def detect_immediate_failure(self, output_line: str) -> bool:
        """Detect immediate test failures in output"""
        failure_indicators = [
            "FAILED",
            "ERROR",
            "AssertionError",
            "Exception:",
            "Traceback (most recent call last):"
        ]
        
        return any(indicator in output_line for indicator in failure_indicators)
    
    def analyze_test_results(self, test_results: Dict) -> TestExecutionReport:
        """Analyze test execution results"""
        report = TestExecutionReport()
        
        if not test_results["success"]:
            # Parse failures from output
            failures = self.parse_failures_from_output(test_results["output"])
            report.add_failures(failures)
            
            # Analyze failure patterns
            failure_analysis = self.failure_analysis.analyze_failures(failures)
            report.add_failure_analysis(failure_analysis)
        
        # Parse test statistics
        stats = self.parse_test_statistics(test_results["output"])
        report.add_statistics(stats)
        
        return report
    
    def parse_failures_from_output(self, output: List[str]) -> List[TestFailure]:
        """Parse test failures from pytest output"""
        failures = []
        current_failure = None
        
        for line in output:
            if "FAILED" in line or "ERROR" in line:
                # Start of new failure
                if current_failure:
                    failures.append(current_failure)
                
                current_failure = TestFailure(
                    test_name=self.extract_test_name(line),
                    failure_type="FAILED" if "FAILED" in line else "ERROR",
                    error_message=line
                )
            elif current_failure and line.strip():
                # Add to current failure details
                current_failure.add_detail(line)
        
        # Add last failure
        if current_failure:
            failures.append(current_failure)
        
        return failures
    
    def extract_test_name(self, line: str) -> str:
        """Extract test name from failure line"""
        # Look for test file and function name
        import re
        
        # Pattern: test_file.py::test_function
        pattern = r'([^:]+)::([^:]+)'
        match = re.search(pattern, line)
        
        if match:
            return f"{match.group(1)}::{match.group(2)}"
        
        return "unknown_test"
```

### 2. Immediate Error Detection and Alerting
**MANDATORY**: Implement immediate error detection with instant alerting and response.

**Error Detection Requirements**:
- **Instant Detection**: Detect errors as they occur
- **Immediate Alerting**: Send alerts immediately upon error detection
- **Error Classification**: Classify errors by severity and type
- **Context Preservation**: Preserve error context for analysis
- **Escalation Procedures**: Escalate critical errors automatically

**Implementation**:
```python
# ‚úÖ CORRECT: Immediate Error Detection
class ErrorDetector:
    def __init__(self):
        self.alert_system = AlertSystem()
        self.error_classifier = ErrorClassifier()
        self.context_preserver = ContextPreserver()
        self.escalation_manager = EscalationManager()
    
    def detect_and_alert(self, error: Exception, context: Dict = None) -> None:
        """Detect error and send immediate alert"""
        # Classify error
        error_classification = self.error_classifier.classify_error(error)
        
        # Preserve context
        error_context = self.context_preserver.preserve_context(error, context)
        
        # Create alert
        alert = Alert(
            type="test_failure",
            severity=error_classification.severity,
            message=str(error),
            context=error_context,
            timestamp=datetime.now()
        )
        
        # Send immediate alert
        self.alert_system.send_alert(alert)
        
        # Check for escalation
        if error_classification.severity == "critical":
            self.escalation_manager.escalate_error(alert)

class AlertSystem:
    def __init__(self):
        self.alert_channels = {
            "console": ConsoleAlertChannel(),
            "log": LogAlertChannel(),
            "email": EmailAlertChannel(),
            "slack": SlackAlertChannel()
        }
    
    def send_alert(self, alert: Alert) -> bool:
        """Send alert through all configured channels"""
        success = True
        
        for channel_name, channel in self.alert_channels.items():
            try:
                channel.send(alert)
            except Exception as e:
                logging.error(f"Failed to send alert through {channel_name}: {e}")
                success = False
        
        return success
    
    def send_immediate_alert(self, message: str) -> None:
        """Send immediate alert for critical issues"""
        alert = Alert(
            type="immediate",
            severity="high",
            message=message,
            timestamp=datetime.now()
        )
        
        # Send to console immediately
        self.alert_channels["console"].send(alert)
        
        # Send to other channels
        self.send_alert(alert)

class ErrorClassifier:
    def classify_error(self, error: Exception) -> ErrorClassification:
        """Classify error by severity and type"""
        error_type = type(error).__name__
        error_message = str(error)
        
        # Determine severity
        severity = self.determine_severity(error_type, error_message)
        
        # Determine category
        category = self.determine_category(error_type, error_message)
        
        return ErrorClassification(
            severity=severity,
            category=category,
            error_type=error_type,
            error_message=error_message
        )
    
    def determine_severity(self, error_type: str, error_message: str) -> str:
        """Determine error severity"""
        critical_errors = [
            "AssertionError",
            "ImportError",
            "ModuleNotFoundError",
            "SyntaxError",
            "IndentationError"
        ]
        
        high_errors = [
            "ValueError",
            "TypeError",
            "AttributeError",
            "KeyError",
            "IndexError"
        ]
        
        if error_type in critical_errors:
            return "critical"
        elif error_type in high_errors:
            return "high"
        else:
            return "medium"
    
    def determine_category(self, error_type: str, error_message: str) -> str:
        """Determine error category"""
        if "test" in error_message.lower():
            return "test_failure"
        elif "import" in error_message.lower():
            return "import_error"
        elif "syntax" in error_message.lower():
            return "syntax_error"
        elif "assertion" in error_message.lower():
            return "assertion_failure"
        else:
            return "general_error"
```

### 3. Systematic Bug Fixing Workflow
**MANDATORY**: Implement systematic bug fixing workflow for immediate error resolution.

**Bug Fixing Requirements**:
- **Root Cause Analysis**: Analyze failures for root causes
- **Automated Fixes**: Apply automated fixes where possible
- **Manual Intervention**: Escalate to manual intervention when needed
- **Fix Validation**: Validate fixes before deployment
- **Prevention Measures**: Implement measures to prevent recurrence

**Implementation**:
```python
# ‚úÖ CORRECT: Systematic Bug Fixing
class BugFixer:
    def __init__(self):
        self.root_cause_analyzer = RootCauseAnalyzer()
        self.automated_fixer = AutomatedFixer()
        self.fix_validator = FixValidator()
        self.prevention_manager = PreventionManager()
    
    def fix_test_failures(self, failures: List[TestFailure]) -> FixReport:
        """Systematically fix test failures"""
        fix_report = FixReport()
        
        for failure in failures:
            try:
                # Analyze root cause
                root_cause = self.root_cause_analyzer.analyze_failure(failure)
                
                # Attempt automated fix
                if root_cause.is_automatically_fixable():
                    fix_result = self.automated_fixer.apply_fix(failure, root_cause)
                    
                    if fix_result.success:
                        # Validate fix
                        validation_result = self.fix_validator.validate_fix(failure)
                        
                        if validation_result.success:
                            fix_report.add_successful_fix(failure, fix_result)
                        else:
                            fix_report.add_failed_validation(failure, validation_result)
                    else:
                        fix_report.add_failed_fix(failure, fix_result)
                else:
                    # Escalate to manual intervention
                    fix_report.add_manual_intervention_needed(failure, root_cause)
                
                # Implement prevention measures
                prevention_measures = self.prevention_manager.create_prevention_measures(failure, root_cause)
                fix_report.add_prevention_measures(failure, prevention_measures)
                
            except Exception as e:
                fix_report.add_error(failure, f"Bug fixing error: {e}")
        
        return fix_report

class RootCauseAnalyzer:
    def analyze_failure(self, failure: TestFailure) -> RootCause:
        """Analyze test failure for root cause"""
        # Analyze error message
        error_patterns = self.analyze_error_patterns(failure.error_message)
        
        # Analyze test context
        context_analysis = self.analyze_test_context(failure)
        
        # Analyze recent changes
        change_analysis = self.analyze_recent_changes(failure)
        
        # Determine root cause
        root_cause = self.determine_root_cause(error_patterns, context_analysis, change_analysis)
        
        return root_cause
    
    def analyze_error_patterns(self, error_message: str) -> List[ErrorPattern]:
        """Analyze error message for patterns"""
        patterns = []
        
        # Check for common error patterns
        if "AssertionError" in error_message:
            patterns.append(ErrorPattern("assertion_failure", "Test assertion failed"))
        
        if "ImportError" in error_message:
            patterns.append(ErrorPattern("import_error", "Module import failed"))
        
        if "AttributeError" in error_message:
            patterns.append(ErrorPattern("attribute_error", "Object attribute access failed"))
        
        if "TypeError" in error_message:
            patterns.append(ErrorPattern("type_error", "Type mismatch or incorrect usage"))
        
        return patterns
    
    def determine_root_cause(self, error_patterns: List[ErrorPattern], 
                           context_analysis: Dict, change_analysis: Dict) -> RootCause:
        """Determine root cause from analysis"""
        # Prioritize by pattern severity
        if any(p.pattern_type == "import_error" for p in error_patterns):
            return RootCause("dependency_issue", "Missing or incorrect dependency")
        
        if any(p.pattern_type == "assertion_failure" for p in error_patterns):
            return RootCause("test_logic_error", "Test logic or expectation error")
        
        if any(p.pattern_type == "attribute_error" for p in error_patterns):
            return RootCause("api_change", "API or interface change")
        
        # Check for recent changes
        if change_analysis.get("recent_changes"):
            return RootCause("recent_change", "Recent code change caused regression")
        
        return RootCause("unknown", "Unknown root cause")

class AutomatedFixer:
    def apply_fix(self, failure: TestFailure, root_cause: RootCause) -> FixResult:
        """Apply automated fix for test failure"""
        try:
            if root_cause.cause_type == "dependency_issue":
                return self.fix_dependency_issue(failure)
            
            elif root_cause.cause_type == "test_logic_error":
                return self.fix_test_logic(failure)
            
            elif root_cause.cause_type == "api_change":
                return self.fix_api_change(failure)
            
            else:
                return FixResult(success=False, message="No automated fix available")
                
        except Exception as e:
            return FixResult(success=False, message=f"Fix application failed: {e}")
    
    def fix_dependency_issue(self, failure: TestFailure) -> FixResult:
        """Fix dependency-related issues"""
        # Check if it's a missing import
        if "ImportError" in failure.error_message:
            # Try to add missing import
            missing_module = self.extract_missing_module(failure.error_message)
            if missing_module:
                return self.add_missing_import(failure.test_file, missing_module)
        
        return FixResult(success=False, message="Dependency fix not implemented")
    
    def fix_test_logic(self, failure: TestFailure) -> FixResult:
        """Fix test logic issues"""
        # This would require more sophisticated analysis
        # For now, return manual intervention needed
        return FixResult(success=False, message="Test logic fix requires manual intervention")
```

### 4. Performance Tracking and Optimization
**MANDATORY**: Track test performance and optimize for faster execution.

**Performance Requirements**:
- **Execution Time Tracking**: Track test execution times
- **Performance Regression Detection**: Detect performance regressions
- **Test Parallelization**: Optimize test execution with parallelization
- **Resource Usage Monitoring**: Monitor resource usage during tests
- **Performance Optimization**: Optimize slow tests

**Implementation**:
```python
# ‚úÖ CORRECT: Performance Tracking
class PerformanceTracker:
    def __init__(self):
        self.performance_database = PerformanceDatabase()
        self.regression_detector = RegressionDetector()
        self.optimizer = TestOptimizer()
    
    def record_execution(self, test_path: str, execution_time: float, 
                        report: TestExecutionReport) -> None:
        """Record test execution performance"""
        performance_record = PerformanceRecord(
            test_path=test_path,
            execution_time=execution_time,
            test_count=report.get_test_count(),
            failure_count=report.get_failure_count(),
            timestamp=datetime.now()
        )
        
        # Store performance record
        self.performance_database.store_record(performance_record)
        
        # Check for performance regressions
        regression = self.regression_detector.check_regression(performance_record)
        if regression:
            self.alert_system.send_alert(Alert(
                type="performance_regression",
                severity="medium",
                message=f"Performance regression detected: {regression.description}"
            ))
    
    def optimize_test_execution(self, test_path: str) -> OptimizationResult:
        """Optimize test execution performance"""
        # Analyze current performance
        performance_data = self.performance_database.get_performance_data(test_path)
        
        # Identify optimization opportunities
        opportunities = self.optimizer.identify_opportunities(performance_data)
        
        # Apply optimizations
        optimization_result = self.optimizer.apply_optimizations(test_path, opportunities)
        
        return optimization_result

class RegressionDetector:
    def check_regression(self, performance_record: PerformanceRecord) -> Optional[Regression]:
        """Check for performance regression"""
        # Get historical performance data
        historical_data = self.performance_database.get_historical_data(
            performance_record.test_path, 
            days=7
        )
        
        if not historical_data:
            return None
        
        # Calculate baseline
        baseline_execution_time = self.calculate_baseline(historical_data)
        
        # Check for regression
        if performance_record.execution_time > baseline_execution_time * 1.5:
            return Regression(
                test_path=performance_record.test_path,
                current_time=performance_record.execution_time,
                baseline_time=baseline_execution_time,
                degradation_percentage=(
                    (performance_record.execution_time - baseline_execution_time) / 
                    baseline_execution_time * 100
                )
            )
        
        return None
    
    def calculate_baseline(self, historical_data: List[PerformanceRecord]) -> float:
        """Calculate performance baseline"""
        if not historical_data:
            return 0.0
        
        # Use median for baseline to avoid outliers
        execution_times = [record.execution_time for record in historical_data]
        execution_times.sort()
        
        n = len(execution_times)
        if n % 2 == 0:
            median = (execution_times[n//2 - 1] + execution_times[n//2]) / 2
        else:
            median = execution_times[n//2]
        
        return median
```

# Bottom-Up Testing Rule

**CRITICAL**: Always follow the Bottom-Up Testing methodology when running larger test suites. Test individual components in isolation first, fix issues at the lowest level, and only then run the complete test suite.

## Core Bottom-Up Testing Requirements

### 1. Test Isolation First
**MANDATORY**: Always test individual components in isolation before running complete test suites.

**Isolation Testing Process**:
- **Individual Test Execution**: Run each test method/function in isolation
- **Component-Level Testing**: Test individual classes, modules, or functions separately
- **Dependency Isolation**: Mock external dependencies to isolate the component under test
- **Single Responsibility Testing**: Test one specific functionality at a time
- **Clear Test Boundaries**: Define clear boundaries for what each test covers

**Implementation**:
```python
# ‚úÖ CORRECT: Bottom-Up Testing Approach
class BottomUpTestRunner:
    def __init__(self):
        self.test_monitor = TestMonitor()
        self.isolation_results = {}
        self.integration_results = {}
    
    def run_bottom_up_tests(self, test_suite_path: str) -> TestExecutionReport:
        """Execute tests using bottom-up methodology"""
        execution_report = TestExecutionReport()
        
        # Step 1: Identify individual test components
        test_components = self.identify_test_components(test_suite_path)
        
        # Step 2: Test each component in isolation
        for component in test_components:
            isolation_result = self.test_component_in_isolation(component)
            self.isolation_results[component] = isolation_result
            
            # Fix issues at component level
            if isolation_result.has_failures():
                self.fix_component_issues(component, isolation_result)
                # Re-test after fixes
                isolation_result = self.test_component_in_isolation(component)
                self.isolation_results[component] = isolation_result
        
        # Step 3: Only run complete suite after all components pass
        if self.all_components_passing():
            integration_result = self.run_complete_test_suite(test_suite_path)
            self.integration_results[test_suite_path] = integration_result
            execution_report.add_integration_result(integration_result)
        else:
            execution_report.add_error("Cannot run integration tests - component failures exist")
        
        return execution_report
    
    def test_component_in_isolation(self, component: str) -> TestResult:
        """Test a single component in complete isolation"""
        # Mock all external dependencies
        with patch_all_dependencies(component):
            # Run only this component's tests
            result = self.run_isolated_tests(component)
            return result
    
    def fix_component_issues(self, component: str, test_result: TestResult) -> None:
        """Fix issues found in component testing"""
        for failure in test_result.get_failures():
            # Apply targeted fixes for this specific component
            fix = self.analyze_and_fix_failure(component, failure)
            if fix:
                self.apply_fix(component, fix)
```

### 2. Component-Level Testing Strategy

**Individual Component Testing**:
```python
# ‚úÖ CORRECT: Component Isolation Testing
def test_individual_component():
    """Test a single component in isolation"""
    # 1. Mock all external dependencies
    with patch('external.dependency') as mock_dep:
        with patch('another.dependency') as mock_dep2:
            
            # 2. Set up isolated test environment
            isolated_config = create_isolated_config()
            
            # 3. Test the component
            component = Component(isolated_config)
            result = component.execute(test_input)
            
            # 4. Verify results
            assert result.is_valid()
            assert result.contains_expected_data()
            
            # 5. Verify no external calls were made
            mock_dep.assert_not_called()
            mock_dep2.assert_not_called()
```

### 3. Progressive Integration Testing

**Integration Testing Process**:
```python
# ‚úÖ CORRECT: Progressive Integration
class ProgressiveIntegrationTester:
    def run_progressive_integration(self, components: List[str]) -> TestResult:
        """Run integration tests progressively"""
        result = TestResult()
        
        # Start with smallest integration
        for i in range(1, len(components) + 1):
            component_group = components[:i]
            
            # Test this group of components
            group_result = self.test_component_group(component_group)
            
            if group_result.has_failures():
                # Fix issues at this integration level
                self.fix_integration_issues(component_group, group_result)
                # Re-test after fixes
                group_result = self.test_component_group(component_group)
            
            result.add_group_result(component_group, group_result)
            
            # Only continue if this group passes
            if group_result.has_failures():
                break
        
        return result
```

### 4. Error Handling and Fixing

**Component-Level Error Handling**:
```python
# ‚úÖ CORRECT: Component Error Handling
def handle_component_error(component: str, error: Exception) -> FixResult:
    """Handle errors at component level"""
    # 1. Analyze the error
    error_analysis = analyze_error(component, error)
    
    # 2. Identify the root cause
    root_cause = identify_root_cause(error_analysis)
    
    # 3. Apply targeted fix
    fix = create_targeted_fix(component, root_cause)
    
    # 4. Apply the fix
    fix_result = apply_fix(component, fix)
    
    # 5. Verify the fix
    verification_result = verify_fix(component, fix)
    
    return FixResult(
        component=component,
        error=error,
        fix_applied=fix,
        fix_successful=verification_result.is_successful()
    )
```

### 5. Test Execution Workflow

**Bottom-Up Test Execution**:
```python
# ‚úÖ CORRECT: Bottom-Up Workflow
def execute_bottom_up_testing(test_suite: str) -> TestExecutionReport:
    """Execute tests using bottom-up methodology"""
    
    # Phase 1: Component Isolation Testing
    print("Phase 1: Testing individual components in isolation...")
    component_results = {}
    
    for component in get_test_components(test_suite):
        print(f"  Testing component: {component}")
        result = test_component_in_isolation(component)
        component_results[component] = result
        
        if result.has_failures():
            print(f"  ‚ùå Component {component} has failures - fixing...")
            fix_result = fix_component_issues(component, result)
            if fix_result.is_successful():
                print(f"  ‚úÖ Component {component} fixed successfully")
                # Re-test after fix
                result = test_component_in_isolation(component)
                component_results[component] = result
            else:
                print(f"  ‚ùå Failed to fix component {component}")
                return TestExecutionReport(error=f"Component {component} cannot be fixed")
        else:
            print(f"  ‚úÖ Component {component} passed")
    
    # Phase 2: Integration Testing
    print("Phase 2: Running integration tests...")
    if all_components_passing(component_results):
        integration_result = run_integration_tests(test_suite)
        return TestExecutionReport(
            component_results=component_results,
            integration_result=integration_result
        )
    else:
        return TestExecutionReport(
            error="Cannot run integration tests - component failures exist",
            component_results=component_results
        )
```

## Integration Failure Protocol

### **CRITICAL: Integration Fails ‚Üí Units First Rule**
**MANDATORY**: When integration tests fail, immediately fall back to unit testing approach.

```python
# ‚úÖ CORRECT: Integration Failure Protocol
class IntegrationFailureProtocol:
    """Handle integration test failures with systematic unit-first approach."""
    
    def handle_integration_failure(self, integration_result: TestResult) -> RecoveryResult:
        """Execute systematic recovery from integration failures."""
        
        print("üö® INTEGRATION TESTS FAILED - EXECUTING UNIT-FIRST PROTOCOL")
        print("=" * 60)
        
        # Step 1: Immediately stop integration testing
        self.stop_integration_testing()
        
        # Step 2: Identify all components involved in failure
        failed_components = self.extract_failed_components(integration_result)
        print(f"üìã Identified {len(failed_components)} components involved in failure")
        
        # Step 3: Test each component in complete isolation
        component_results = {}
        for component in failed_components:
            print(f"\nüîç Testing component in isolation: {component}")
            
            # Complete isolation with all dependencies mocked
            isolation_result = self.test_component_in_complete_isolation(component)
            component_results[component] = isolation_result
            
            if isolation_result.has_failures():
                print(f"‚ùå Component {component} has unit-level failures")
                
                # Fix at unit level immediately
                unit_fix_result = self.fix_unit_level_issues(component, isolation_result)
                
                if unit_fix_result.successful:
                    print(f"‚úÖ Component {component} unit issues fixed")
                    # Re-test in isolation
                    isolation_result = self.test_component_in_complete_isolation(component)
                    component_results[component] = isolation_result
                else:
                    print(f"‚ùå Failed to fix unit issues in {component}")
                    return RecoveryResult(
                        success=False,
                        message=f"Cannot fix unit-level issues in {component}",
                        failed_component=component
                    )
            else:
                print(f"‚úÖ Component {component} passes in isolation")
        
        # Step 4: Progressive integration testing
        print(f"\nüîÑ Starting progressive integration...")
        progressive_result = self.run_progressive_integration(failed_components)
        
        if progressive_result.successful:
            print("‚úÖ Progressive integration successful - issue resolved")
            return RecoveryResult(success=True, method="unit_first_recovery")
        else:
            print("‚ùå Progressive integration still failing")
            return RecoveryResult(
                success=False,
                message="Integration issues persist after unit fixes",
                requires_step_back_analysis=True
            )
    
    def test_component_in_complete_isolation(self, component: str) -> TestResult:
        """Test component with ALL external dependencies mocked."""
        
        # Identify all external dependencies
        dependencies = self.identify_external_dependencies(component)
        
        # Mock every single external dependency
        with ExternalDependencyMocker(dependencies) as mocker:
            # Create isolated test environment
            isolated_env = self.create_completely_isolated_environment(component)
            
            # Run only this component's tests
            result = self.run_component_tests_only(component, isolated_env)
            
            # Verify no external calls were made
            mocker.verify_no_external_calls()
            
            return result
    
    def run_progressive_integration(self, components: List[str]) -> ProgressiveResult:
        """Run progressive integration starting with smallest groups."""
        
        # Start with pairs of components
        for i in range(len(components) - 1):
            for j in range(i + 1, len(components)):
                component_pair = [components[i], components[j]]
                
                print(f"  Testing pair: {component_pair}")
                pair_result = self.test_component_pair(component_pair)
                
                if pair_result.has_failures():
                    print(f"  ‚ùå Pair {component_pair} failed - fixing integration")
                    fix_result = self.fix_pair_integration_issues(component_pair, pair_result)
                    
                    if not fix_result.successful:
                        return ProgressiveResult(
                            successful=False,
                            failed_at_level="pair",
                            failed_components=component_pair
                        )
        
        # Then test larger groups progressively
        for group_size in range(3, len(components) + 1):
            groups = self.generate_component_groups(components, group_size)
            
            for group in groups:
                print(f"  Testing group of {group_size}: {group}")
                group_result = self.test_component_group(group)
                
                if group_result.has_failures():
                    print(f"  ‚ùå Group {group} failed at size {group_size}")
                    fix_result = self.fix_group_integration_issues(group, group_result)
                    
                    if not fix_result.successful:
                        return ProgressiveResult(
                            successful=False,
                            failed_at_level=f"group_of_{group_size}",
                            failed_components=group
                        )
        
        return ProgressiveResult(successful=True)

# ‚úÖ CORRECT: Integration-to-Unit Fallback Workflow
def integration_to_unit_fallback_workflow(test_suite: str) -> TestExecutionReport:
    """Systematic fallback from integration to unit testing."""
    
    execution_report = TestExecutionReport()
    
    try:
        # Step 1: Attempt integration tests first
        print("üöÄ Attempting integration tests...")
        integration_result = run_integration_tests(test_suite)
        
        if integration_result.successful:
            print("‚úÖ Integration tests passed")
            execution_report.add_integration_success(integration_result)
            return execution_report
        
        # Step 2: Integration failed - execute fallback protocol
        print("‚ùå Integration tests failed - executing unit-first fallback")
        
        failure_protocol = IntegrationFailureProtocol()
        recovery_result = failure_protocol.handle_integration_failure(integration_result)
        
        if recovery_result.success:
            print("‚úÖ Recovery successful via unit-first approach")
            execution_report.add_recovery_success(recovery_result)
        else:
            print("‚ùå Recovery failed - requires step back analysis")
            execution_report.add_recovery_failure(recovery_result)
            
            # Trigger step back analysis for profound problem analysis
            if recovery_result.requires_step_back_analysis:
                step_back_result = execute_step_back_protocol({
                    'problem_context': 'integration_testing_failure',
                    'failed_components': recovery_result.failed_component,
                    'step_back_reason': 'unit_first_recovery_failed'
                })
                execution_report.add_step_back_analysis(step_back_result)
        
        return execution_report
        
    except Exception as e:
        execution_report.add_error(f"Fallback workflow failed: {e}")
        return execution_report

# ‚úÖ CORRECT: Automated Integration-Unit Testing Decision
class AutomatedTestingDecision:
    """Automatically decide between integration-first vs unit-first testing."""
    
    def decide_testing_approach(self, context: Dict[str, Any]) -> TestingApproach:
        """Decide optimal testing approach based on context."""
        
        # Analyze recent test history
        recent_failures = self.analyze_recent_test_failures()
        
        # Check component complexity
        component_complexity = self.assess_component_complexity(context)
        
        # Check integration complexity
        integration_complexity = self.assess_integration_complexity(context)
        
        # Decision logic
        if recent_failures.has_frequent_integration_failures():
            return TestingApproach(
                primary="unit_first",
                reasoning="Recent integration failures suggest unit-first approach"
            )
        
        elif component_complexity.is_high() or integration_complexity.is_high():
            return TestingApproach(
                primary="unit_first", 
                reasoning="High complexity suggests unit-first approach"
            )
        
        elif context.get('new_feature') or context.get('major_changes'):
            return TestingApproach(
                primary="unit_first",
                reasoning="New features/major changes benefit from unit-first approach"
            )
        
        else:
            return TestingApproach(
                primary="integration_first",
                fallback="unit_first",
                reasoning="Stable system allows integration-first with unit fallback"
            )
```

### **Integration Failure Escalation Matrix**

```python
# ‚úÖ CORRECT: Escalation matrix for integration failures
class IntegrationFailureEscalation:
    """Escalation matrix for handling different types of integration failures."""
    
    ESCALATION_MATRIX = {
        "unit_passes_integration_fails": {
            "approach": "progressive_integration",
            "focus": "interface_boundaries",
            "escalation_time": "15_minutes"
        },
        "unit_fails_integration_fails": {
            "approach": "unit_first_mandatory",
            "focus": "component_isolation", 
            "escalation_time": "immediate"
        },
        "intermittent_integration_failures": {
            "approach": "environmental_analysis",
            "focus": "timing_and_state",
            "escalation_time": "30_minutes"
        },
        "performance_integration_failures": {
            "approach": "performance_unit_testing",
            "focus": "resource_constraints",
            "escalation_time": "45_minutes"
        },
        "dependency_integration_failures": {
            "approach": "dependency_isolation",
            "focus": "external_services",
            "escalation_time": "20_minutes"
        }
    }
    
    def escalate_failure(self, failure_type: str, context: Dict[str, Any]) -> EscalationResult:
        """Escalate integration failure based on type and context."""
        
        if failure_type not in self.ESCALATION_MATRIX:
            failure_type = "unit_fails_integration_fails"  # Default to most conservative
        
        escalation_config = self.ESCALATION_MATRIX[failure_type]
        
        # Execute escalation approach
        if escalation_config["approach"] == "unit_first_mandatory":
            return self.execute_mandatory_unit_first(context)
        elif escalation_config["approach"] == "progressive_integration":
            return self.execute_progressive_integration(context)
        # ... other approaches
        
        return EscalationResult(success=False, message="Unknown escalation approach")
```

## Integration with Development Workflow

### Pre-Commit Test Monitoring
```python
# ‚úÖ CORRECT: Pre-Commit Integration
class PreCommitTestMonitor:
    def __init__(self):
        self.test_monitor = TestMonitor()
        self.bug_fixer = BugFixer()
    
    def run_pre_commit_tests(self) -> bool:
        """Run tests before commit with monitoring"""
        # Run tests with monitoring
        execution_report = self.test_monitor.monitor_test_execution()
        
        # Check for failures
        if execution_report.has_failures():
            # Attempt to fix failures
            fix_report = self.bug_fixer.fix_test_failures(execution_report.get_failures())
            
            # If fixes were successful, run tests again
            if fix_report.has_successful_fixes():
                execution_report = self.test_monitor.monitor_test_execution()
        
        # Return success only if no failures
        return not execution_report.has_failures()
```

### Continuous Test Monitoring
```python
# ‚úÖ CORRECT: Continuous Monitoring
class ContinuousTestMonitor:
    def __init__(self):
        self.test_monitor = TestMonitor()
        self.scheduler = TestScheduler()
    
    def start_continuous_monitoring(self) -> None:
        """Start continuous test monitoring"""
        while True:
            try:
                # Run scheduled tests
                scheduled_tests = self.scheduler.get_scheduled_tests()
                
                for test_path in scheduled_tests:
                    execution_report = self.test_monitor.monitor_test_execution(test_path)
                    
                    # Handle any failures
                    if execution_report.has_failures():
                        self.handle_continuous_failures(execution_report)
                
                # Wait for next cycle
                time.sleep(300)  # Check every 5 minutes
                
            except Exception as e:
                logging.error(f"Continuous monitoring error: {e}")
                time.sleep(60)  # Wait 1 minute before retrying
```

## Benefits

### Immediate Error Detection
- **Real-time Monitoring**: Detect errors as they occur
- **Instant Alerting**: Immediate notification of failures
- **Rapid Response**: Quick identification and resolution of issues

### Automated Bug Fixing
- **Systematic Approach**: Systematic approach to bug fixing
- **Automated Fixes**: Apply fixes automatically where possible
- **Prevention Measures**: Implement measures to prevent recurrence

### Performance Optimization
- **Performance Tracking**: Track and optimize test performance
- **Regression Detection**: Detect performance regressions early
- **Continuous Improvement**: Continuously improve test efficiency

### Quality Assurance
- **Zero Failing Tests**: Maintain zero failing tests policy
- **Quality Gates**: Ensure code quality through test monitoring
- **Reliability**: Improve system reliability through comprehensive testing

## Enforcement

This rule is **CRITICAL** and must be followed for all:
- Test execution and monitoring
- Error detection and alerting
- Bug fixing and resolution
- Performance optimization
- Quality assurance processes

**Violations of this rule require immediate remediation and test monitoring restoration.**
description: "Auto-generated description"
globs: ["**/*"]
alwaysApply: false
---


