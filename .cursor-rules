# Auto-reload trigger: 1756623131
# Context-Aware Rules (Working System)
# Context: AGILE
# Total Rules: 7
# Generated from: @agile scrum master commit...
# Timestamp: 31.08.2025 08:52


# === safety_first_principle ===
---
description: "Critical safety principle that must always be applied to prevent disasters"
category: "core-foundation"
priority: "critical"
alwaysApply: true
globs: ["**/*"]
tags: ['core_foundation', 'safety', 'critical']
tier: "1"
---

# Safety First Principle

**CRITICAL**: Always prioritize safety over speed, convenience, or automation. If we shoot ourselves in the foot, we are not efficient.

## Core Principle

**"Safety First, Efficiency Second"**

Every development decision, automation, script, or process must be evaluated for safety before implementation. The cost of fixing disasters far exceeds the time saved by unsafe shortcuts.

## Safety Requirements

### 1. **No Automatic File Moving**
**MANDATORY**: Never automatically move, rename, or delete files without explicit human approval
```python
# FORBIDDEN: Automatic file operations
def organize_files():
    for file in files:
        move_file(file, new_location)  # DANGEROUS

# REQUIRED: Safe validation only
def validate_file_organization():
    issues = find_organization_issues()
    return generate_fix_suggestions(issues)  # SAFE
```

### 2. **No Destructive Operations**
**MANDATORY**: All destructive operations must require explicit confirmation
```python
# FORBIDDEN: Silent destructive operations
def cleanup():
    delete_all_temp_files()  # DANGEROUS

# REQUIRED: Safe with confirmation
def safe_cleanup():
    temp_files = find_temp_files()
    if confirm_deletion(temp_files):
        delete_files(temp_files)
    else:
        print("Cleanup cancelled - safety first")
```

### 3. **Validation Before Action**
**MANDATORY**: Always validate before taking any action
```python
# REQUIRED: Validate first
def safe_operation():
    # 1. Validate current state
    if not validate_current_state():
        raise SafetyException("Invalid state detected")
    
    # 2. Check prerequisites
    if not check_prerequisites():
        raise SafetyException("Prerequisites not met")
    
    # 3. Perform operation
    perform_operation()
    
    # 4. Validate result
    if not validate_result():
        raise SafetyException("Operation failed validation")
```

### 4. **Rollback Capability**
**MANDATORY**: Every operation must have a clear rollback path
```python
# REQUIRED: Always provide rollback
def safe_database_update():
    # 1. Create backup
    backup = create_backup()
    
    # 2. Perform update
    try:
        perform_update()
        validate_update()
    except Exception as e:
        # 3. Rollback on failure
        rollback_from_backup(backup)
        raise SafetyException(f"Update failed, rolled back: {e}")
```

## Safety Patterns

### 1. **Read-Only by Default**
```python
# REQUIRED: Start with read-only operations
def safe_analysis():
    # Read and analyze only
    data = read_data()
    analysis = analyze_data(data)
    return generate_report(analysis)  # No modifications
```

### 2. **Dry-Run Mode**
```python
# REQUIRED: Always support dry-run
def safe_operation(dry_run=True):
    if dry_run:
        return simulate_operation()
    else:
        return perform_actual_operation()
```

### 3. **Explicit Confirmation**
```python
# REQUIRED: Always require explicit confirmation for dangerous operations
def dangerous_operation():
    print("⚠️  WARNING: This operation will delete files")
    print("Files to be deleted:", list_files_to_delete())
    
    confirmation = input("Type 'YES' to confirm: ")
    if confirmation != "YES":
        print("Operation cancelled - safety first")
        return False
    
    return perform_deletion()
```

## Safety Checklist

Before any operation, verify:

- [ ] **No automatic file moving/deletion**
- [ ] **Destructive operations require explicit confirmation**
- [ ] **Validation performed before action**
- [ ] **Rollback path exists**
- [ ] **Dry-run mode available**
- [ ] **Read-only analysis first**
- [ ] **Explicit confirmation for dangerous operations**

## Implementation Examples

### Safe File Operations
```python
def safe_file_operation():
    # 1. Analyze current state
    current_files = scan_directory()
    
    # 2. Generate recommendations (read-only)
    recommendations = analyze_file_organization(current_files)
    
    # 3. Present options to user
    print("File organization recommendations:")
    for rec in recommendations:
        print(f"- {rec}")
    
    # 4. Wait for explicit approval
    if user_approves():
        perform_approved_operations()
    else:
        print("Operation cancelled - safety first")
```

### Safe Database Operations
```python
def safe_database_operation():
    # 1. Create backup
    backup = create_database_backup()
    
    # 2. Validate operation
    if not validate_operation_parameters():
        raise SafetyException("Invalid parameters")
    
    # 3. Perform operation with rollback
    try:
        result = perform_database_operation()
        validate_result(result)
        return result
    except Exception as e:
        rollback_from_backup(backup)
        raise SafetyException(f"Operation failed: {e}")
```

## Safety Enforcement

This rule is **ALWAYS ACTIVE** and applies to:

- All file operations
- All database operations
- All system modifications
- All automation scripts
- All deployment operations
- All configuration changes

**Remember**: It's better to be slow and safe than fast and sorry. Safety first, always.



# === intelligent_context_aware_rule_system ===
---
description: "Intelligent context-aware rule selection system that automatically detects development context and applies only relevant rules"
category: "core-foundation"
priority: "critical"
alwaysApply: true
globs: ["**/*"]
tags: ['core_foundation', 'context_awareness', 'rule_selection']
tier: "1"
---


# Intelligent Context-Aware Rule System

**CRITICAL**: Automatically select and apply only the most relevant rules based on current development context, reducing cognitive overhead while maintaining excellence standards. This system serves as the foundation for future agent swarm coordination.

## Description

This rule implements intelligent, context-aware rule selection that automatically detects development context and applies only the most relevant rules. It reduces the active rule set from 39+ rules to 6-10 focused rules per session, achieving 75-85% efficiency improvement while maintaining quality standards.

**Agent Swarm Foundation**: This system serves as the prototype for future agent swarm coordination, where context detection becomes agent selection and rule sets become agent behavioral DNA.

## Core Requirements

### 1. **Dual Context Detection System**
**MANDATORY**: Support both automatic detection and explicit user control

```yaml
context_detection_modes:
  automatic_detection:
    method: "Pattern matching on user queries, files, and directory context"
    confidence_threshold: 0.7
    fallback: "DEFAULT mode if confidence < threshold"
    
  explicit_control:
    method: "Keyword-based user intention signaling"
    priority: "Always overrides automatic detection"
    format: "@keyword at start of user message"
    
  hybrid_approach:
    - "Check for explicit keywords first"
    - "Fall back to automatic detection if no keywords"
    - "Provide transparency on detection method used"
```

### 2. **Context Categories and Rule Sets**
**MANDATORY**: Predefined contexts with optimized rule sets

```yaml
context_categories:
  DEFAULT:
    keywords: ["@default", "@all", "(no keyword)"]
    auto_detect: ["general", "unclear", "mixed"]
    rules: ["safety_first", "no_premature_victory", "boyscout", "context_awareness", "philosophy_separation"]
    agent_future: "GeneralCoordinatorAgent"
    
  CODING:
    keywords: ["@code", "@implement", "@build", "@develop"]
    auto_detect: ["implement", "code", "function", "class", "*.py", "*.js", "*.ts", "src/"]
    rules: ["safety_first", "tdd", "clean_code", "error_handling", "boyscout", "live_documentation"]
    agent_future: "DeveloperAgent"
    
  ARCHITECTURE:
    keywords: ["@design", "@architecture", "@system", "@structure"]
    auto_detect: ["architecture", "design", "system", "structure", "pattern", "docs/architecture/"]
    rules: ["safety_first", "foundational_development", "systematic_construction", "documentation_excellence", "technical_precision"]
    agent_future: "ArchitectAgent"
    
  DEBUGGING:
    keywords: ["@debug", "@troubleshoot", "@fix", "@solve"]
    auto_detect: ["debug", "error", "bug", "issue", "problem", "failing", "logs/"]
    rules: ["safety_first", "systematic_problem_solving", "no_silent_errors", "error_exposure", "test_monitoring"]
    agent_future: "DebuggingAgent"
    
  TESTING:
    keywords: ["@test", "@testing", "@qa", "@validate"]
    auto_detect: ["test", "testing", "*test*.py", "pytest", "unittest", "tests/"]
    rules: ["safety_first", "tdd", "test_monitoring", "no_failing_tests", "test_coverage", "quality_validation"]
    agent_future: "QAAgent"
    
  AGILE:
    keywords: ["@agile", "@sprint", "@story", "@backlog"]
    auto_detect: ["sprint", "backlog", "story", "agile", "scrum", "docs/agile/"]
    rules: ["safety_first", "agile_artifacts_maintenance", "live_documentation_updates", "sprint_management", "user_story_management"]
    agent_future: "ScrumMasterAgent"
    
  GIT_OPERATIONS:
    keywords: ["@git", "@commit", "@push", "@merge"]
    auto_detect: ["git", "commit", "push", "merge", "pull request", "PR", ".git/"]
    rules: ["safety_first", "automated_git_protection", "clean_commit_messages", "merge_validation", "deployment_safety"]
    agent_future: "DevOpsAgent"
    
  DOCUMENTATION:
    keywords: ["@docs", "@document", "@readme", "@guide"]
    auto_detect: ["document", "docs", "readme", "guide", "manual", "*.md", "docs/"]
    rules: ["safety_first", "documentation_excellence", "live_documentation_updates", "clear_communication", "user_experience"]
    agent_future: "TechnicalWriterAgent"
    
  PERFORMANCE:
    keywords: ["@optimize", "@performance", "@benchmark", "@speed"]
    auto_detect: ["optimize", "performance", "speed", "efficiency", "benchmark", "profiling"]
    rules: ["safety_first", "performance_monitoring", "benchmark_validation", "optimization_validation", "scalability_testing"]
    agent_future: "PerformanceAgent"
    
  SECURITY:
    keywords: ["@security", "@secure", "@vulnerability", "@audit"]
    auto_detect: ["security", "secure", "vulnerability", "auth", "encryption", "audit"]
    rules: ["safety_first", "security_vulnerability_assessment", "secure_coding", "penetration_testing", "compliance_validation"]
    agent_future: "SecurityAgent"
```

### 3. **Context Detection Algorithm**
**MANDATORY**: Simple, reliable pattern matching

```python
def detect_context(user_message, open_files, current_directory):
    """
    Detect development context using dual detection system.
    
    Args:
        user_message: User's input message
        open_files: List of currently open files
        current_directory: Current working directory
        
    Returns:
        ContextResult with detected context and confidence
    """
    
    # Step 1: Check for explicit keywords (highest priority)
    explicit_context = check_explicit_keywords(user_message)
    if explicit_context:
        return ContextResult(
            context=explicit_context,
            method="explicit_keyword",
            confidence=1.0,
            reasoning=f"User specified {explicit_context} with keyword"
        )
    
    # Step 2: Automatic detection using pattern matching
    context_scores = {}
    
    # Analyze user message
    message_lower = user_message.lower()
    for context, config in CONTEXT_CATEGORIES.items():
        score = 0
        for pattern in config["auto_detect"]:
            if pattern in message_lower:
                score += 2
        context_scores[context] = score
    
    # Analyze open files
    for file_path in open_files:
        for context, config in CONTEXT_CATEGORIES.items():
            for pattern in config["auto_detect"]:
                if pattern in file_path.lower():
                    context_scores[context] += 1
    
    # Analyze directory context
    dir_lower = current_directory.lower()
    for context, config in CONTEXT_CATEGORIES.items():
        for pattern in config["auto_detect"]:
            if pattern in dir_lower:
                context_scores[context] += 1
    
    # Select best context
    best_context = max(context_scores.items(), key=lambda x: x[1])
    confidence = min(best_context[1] / 5.0, 1.0)  # Normalize to 0-1
    
    if confidence >= 0.7:
        return ContextResult(
            context=best_context[0],
            method="auto_detected",
            confidence=confidence,
            reasoning=f"Auto-detected based on patterns (confidence: {confidence:.1f})"
        )
    else:
        return ContextResult(
            context="DEFAULT",
            method="fallback",
            confidence=0.5,
            reasoning="Low confidence in detection, using DEFAULT mode"
        )

def check_explicit_keywords(message):
    """Check for explicit @keywords in user message."""
    message_lower = message.lower()
    
    keyword_map = {
        "@code": "CODING", "@implement": "CODING", "@build": "CODING", "@develop": "CODING",
        "@design": "ARCHITECTURE", "@architecture": "ARCHITECTURE", "@system": "ARCHITECTURE",
        "@debug": "DEBUGGING", "@troubleshoot": "DEBUGGING", "@fix": "DEBUGGING", "@solve": "DEBUGGING",
        "@test": "TESTING", "@testing": "TESTING", "@qa": "TESTING", "@validate": "TESTING",
        "@agile": "AGILE", "@sprint": "AGILE", "@story": "AGILE", "@backlog": "AGILE",
        "@git": "GIT_OPERATIONS", "@commit": "GIT_OPERATIONS", "@push": "GIT_OPERATIONS", "@merge": "GIT_OPERATIONS",
        "@docs": "DOCUMENTATION", "@document": "DOCUMENTATION", "@readme": "DOCUMENTATION",
        "@optimize": "PERFORMANCE", "@performance": "PERFORMANCE", "@benchmark": "PERFORMANCE",
        "@security": "SECURITY", "@secure": "SECURITY", "@vulnerability": "SECURITY",
        "@default": "DEFAULT", "@all": "DEFAULT"
    }
    
    for keyword, context in keyword_map.items():
        if keyword in message_lower:
            return context
    
    return None
```

### 4. **Rule Application System**
**MANDATORY**: Apply selected rules with transparency

```python
def apply_context_aware_rules(context_result):
    """
    Apply rules based on detected context.
    
    Args:
        context_result: Result from context detection
        
    Returns:
        RuleApplicationResult with active rules and metrics
    """
    
    context = context_result.context
    rule_config = CONTEXT_CATEGORIES[context]
    active_rules = rule_config["rules"]
    
    # Calculate efficiency metrics
    total_available_rules = 39  # Current total rule count
    active_rule_count = len(active_rules)
    efficiency_improvement = ((total_available_rules - active_rule_count) / total_available_rules) * 100
    
    # Log context detection and rule selection
    print(f"🎯 **Context Detected**: {context}")
    print(f"📋 **Detection Method**: {context_result.method}")
    print(f"🔍 **Reasoning**: {context_result.reasoning}")
    print(f"📊 **Rules Active**: {active_rule_count} rules loaded")
    print(f"⚡ **Efficiency**: {efficiency_improvement:.0f}% reduction from full rule set")
    print(f"🤖 **Future Agent**: {rule_config['agent_future']}")
    print(f"📝 **Active Rules**: {', '.join(active_rules)}")
    
    return RuleApplicationResult(
        context=context,
        active_rules=active_rules,
        total_rules_available=total_available_rules,
        efficiency_improvement=efficiency_improvement,
        agent_future=rule_config["agent_future"],
        detection_confidence=context_result.confidence
    )
```

### 5. **Agent Swarm Foundation**
**MANDATORY**: Design for future agent swarm coordination

```yaml
agent_swarm_preparation:
  rule_to_agent_mapping:
    - "Each context category maps to future specialized agent"
    - "Rule sets become agent behavioral DNA"
    - "Context detection becomes agent selection logic"
    - "Optimization patterns become swarm coordination intelligence"
    
  scalability_design:
    - "Context detection scales from single AI to multi-agent orchestration"
    - "Rule application scales from individual behavior to swarm coordination"
    - "Efficiency improvements compound across agent swarm"
    - "Learning and optimization scale to collective intelligence"
    
  coordination_protocols:
    - "Shared context understanding across agents"
    - "Inter-agent communication using same context categories"
    - "Coordinated rule application for complex multi-agent tasks"
    - "Swarm-wide optimization using same efficiency metrics"
```

### 6. **Performance and Efficiency**
**MANDATORY**: Achieve significant efficiency improvements

```yaml
efficiency_targets:
  rule_reduction: "75-85% reduction in active rules per session"
  startup_performance: "50% faster session initialization"
  cognitive_load: "80% reduction in rule complexity per task"
  context_accuracy: "90%+ correct context detection"
  
efficiency_monitoring:
  - "Track rule usage patterns and effectiveness"
  - "Monitor context detection accuracy over time"
  - "Measure performance improvements in real sessions"
  - "Collect user satisfaction with focused rule sets"
```

### 7. **Learning and Optimization**
**MANDATORY**: Continuous improvement of context detection

```yaml
learning_system:
  pattern_recognition:
    - "Learn from successful context detections"
    - "Identify patterns that improve detection accuracy"
    - "Adapt detection algorithms based on usage data"
    - "Optimize rule sets based on effectiveness metrics"
    
  feedback_integration:
    - "Collect feedback on context detection accuracy"
    - "Learn from manual context corrections"
    - "Improve auto-detection patterns over time"
    - "Optimize rule combinations for better outcomes"
```

## Implementation Guidelines

### 1. **Rule Metadata Modification**
**CRITICAL**: To enable context-aware rule loading, modify rule metadata as follows:

```yaml
# OLD STRUCTURE (always loads)
# NEW STRUCTURE (context-aware)
```

### 2. **Context-Specific Rule Categories**
**MANDATORY**: Organize rules by context for proper loading:

#### **Always Apply Rules (Tier 1)**
- `safety_first_principle` - Always loaded for safety
- `intelligent_context_aware_rule_system` - The system itself
- `core_rule_application_framework` - Framework for rule application

#### **Context-Dependent Rules (Tier 2)**
- **CODING**: `xp_test_first_development_rule`, `development_core_principles_rule`
- **DEBUGGING**: `development_systematic_problem_solving_rule`, `development_error_exposure_rule`
- **AGILE**: `agile_artifacts_maintenance_rule`, `agile_sprint_management_rule`
- **DOCUMENTATION**: `documentation_live_updates_rule`, `rule_document_excellence_rule`
- **RESEARCH**: `development_context_awareness_excellence_rule`, `development_clear_communication_rule`

### 3. **Session Startup Process**
```yaml
startup_sequence:
  1_initialize: "Load intelligent context-aware rule system"
  2_detect: "Analyze initial context from user query and environment"
  3_select: "Choose appropriate rule set based on context"
  4_apply: "Activate selected rules and provide transparency"
  5_monitor: "Track context changes during session"
  6_adapt: "Adjust rule set if context changes significantly"
```

### 2. **Context Change Handling**
```yaml
context_adaptation:
  trigger_conditions:
    - "User explicitly changes context with new @keyword"
    - "File context changes significantly (different file types)"
    - "Activity type changes (coding → debugging → documentation)"
    
  adaptation_process:
    - "Detect context change"
    - "Evaluate need for rule set adjustment"
    - "Smoothly transition to new rule set"
    - "Maintain continuity of ongoing work"
```

### 3. **Quality Assurance**
```yaml
quality_maintenance:
  critical_rules_always_active:
    - "safety_first_principle always included"
    - "Core quality standards never compromised"
    - "Essential safety checks always performed"
    
  context_validation:
    - "Validate context detection accuracy"
    - "Ensure appropriate rules for detected context"
    - "Monitor for context detection failures"
    - "Provide fallback to DEFAULT mode when uncertain"
```

## Benefits

### **Immediate Benefits**
- **75-85% Rule Reduction**: From 39 rules to 6-10 focused rules per session
- **50% Faster Startup**: Reduced rule processing overhead
- **Improved Focus**: Only relevant rules for current work
- **Better Performance**: Less cognitive load, faster responses

### **Strategic Benefits**
- **Agent Swarm Foundation**: Architecture ready for multi-agent coordination
- **Scalable Design**: Patterns that work for single AI and agent swarms
- **Learning Capability**: System improves over time
- **Future-Proof**: Designed for evolution to autonomous agent teams

### **User Experience Benefits**
- **Explicit Control**: @keywords for precise rule selection
- **Intelligent Assistance**: Automatic context detection when needed
- **Transparency**: Clear visibility into rule selection reasoning
- **Flexibility**: Easy context switching during development

## Enforcement

This rule is **CONDITIONALLY APPLIED** based on context.

**The intelligent context-aware rule system is the foundation for both current efficiency and future agent swarm coordination.**

## Remember

**"Context awareness enables precision."**

**"Focused rules deliver better results than scattered rules."**

**"Today's rule system is tomorrow's agent swarm DNA."**

**"Efficiency improvements compound across the entire system."**

This system transforms rule management from overwhelming complexity to intelligent precision, while laying the foundation for the future of autonomous software development.
**"Context awareness enables precision."**

**"Focused rules deliver better results than scattered rules."**

**"Today's rule system is tomorrow's agent swarm DNA."**

**"Efficiency improvements compound across the entire system."**

This system transforms rule management from overwhelming complexity to intelligent precision, while laying the foundation for the future of autonomous software development.


# === core_rule_application_framework ===
---
description: "Auto-generated description for core_rule_application_framework.mdc"
category: "core-foundation"
priority: "critical"
alwaysApply: true
globs: ["**/*"]
tags: ['core_foundation']
tier: "2"
---

# Core Rule Application Framework

**CRITICAL**: This framework ensures that critical rules are automatically applied to every situation, task, and development session. The Courage Rule and No Premature Victory Declaration Rule are ALWAYS active.

## Framework Overview

This framework provides automatic rule application that ensures critical rules are never missed, forgotten, or bypassed. It operates at the highest level of our rule system and enforces the most important development principles.

## Core Critical Rules

### **1. Courage and Complete Work Rule** 💪
- **Purpose**: Ensure ALL work is completed systematically, never stopping at partial results
- **Application**: Applied to every work session, task, and problem-solving situation
- **Enforcement**: Automatic application with verification

### **2. No Premature Victory Declaration Rule** 🎯
- **Purpose**: Prevent premature declarations of success without proper validation and enforce concise progress communication
- **Application**: Applied to every progress report, completion claim, status update, and communication
- **Enforcement**: Automatic validation before any success declaration and conciseness requirements

### **3. No Failing Tests Rule** 🧪
- **Purpose**: Ensure zero tolerance for failing tests
- **Application**: Applied to every development session and commit
- **Enforcement**: Automatic test execution and failure blocking

### **4. Boy Scout Rule** 🏕️
- **Purpose**: Always leave the codebase cleaner than found
- **Application**: Applied to every code modification and cleanup
- **Enforcement**: Automatic cleanup and improvement verification

## Framework Implementation

### **Automatic Rule Application System**

```python
class CoreRuleApplicationFramework:
    """Always-active framework that applies critical rules automatically."""
    
    def __init__(self):
        self.critical_rules = {
            "courage_rule": {
                "file": "development_courage_completion_rule.mdc",
                "priority": "critical",
                "always_apply": True,
                "triggers": ["work_start", "problem_encountered", "progress_report", "work_completion"]
            },
            "no_premature_victory_declaration_rule": {
                "file": "no_premature_victory_declaration_rule.mdc",
                "priority": "critical", 
                "always_apply": True,
                "triggers": ["progress_report", "success_declaration", "completion_claim", "communication", "status_update"]
            },
            "no_failing_tests_rule": {
                "file": "no_failing_tests_rule.mdc",
                "priority": "critical",
                "always_apply": True,
                "triggers": ["development_session", "code_change", "commit_preparation"]
            },
            "boyscout_rule": {
                "file": "boyscout_leave_cleaner_rule.mdc",
                "priority": "critical",
                "always_apply": True,
                "triggers": ["code_modification", "cleanup", "session_end"]
            }
        }
    
    def apply_critical_rules(self, context: dict) -> dict:
        """Apply all critical rules to the current context."""
        
        print("🚀 APPLYING CRITICAL RULES FRAMEWORK")
        
        # Apply courage rule
        context = self.apply_courage_rule(context)
        
        # Apply no premature victory declaration rule
        context = self.apply_no_premature_victory_declaration_rule(context)
        
        # Apply no failing tests rule
        context = self.apply_no_failing_tests_rule(context)
        
        # Apply boy scout rule
        context = self.apply_boyscout_rule(context)
        
        print("✅ CRITICAL RULES FRAMEWORK APPLIED")
        return context
    
    def apply_courage_rule(self, context: dict) -> dict:
        """Apply courage rule to ensure complete work."""
        
        print("💪 APPLYING COURAGE RULE")
        
        # Courage declaration
        context["courage_declaration"] = {
            "timestamp": datetime.now(),
            "commitment": "Complete ALL work systematically",
            "no_partial_results": True,
            "systematic_approach": True
        }
        
        # Courage verification
        context["courage_verification"] = {
            "work_complete": False,  # Will be set to True only when 100% complete
            "partial_progress_acceptable": False,
            "systematic_problem_solving": True
        }
        
        print("✅ COURAGE RULE APPLIED")
        return context
    
    def apply_no_premature_victory_declaration_rule(self, context: dict) -> dict:
        """Apply no premature victory declaration rule to prevent false claims and enforce concise communication."""
        
        print("🎯 APPLYING NO PREMATURE VICTORY DECLARATION RULE")
        
        # Success validation and conciseness requirements
        context["success_validation"] = {
            "evidence_required": True,
            "test_validation_required": True,
            "complete_verification_required": True,
            "no_assumptions_allowed": True,
            "concise_communication_required": True,
            "progress_only_output": True,
            "max_words_status_update": 100,
            "max_words_progress_report": 50
        }
        
        # Success declaration guard
        context["success_declaration_guard"] = {
            "validation_complete": False,
            "evidence_provided": False,
            "tests_passing": False,
            "ready_for_declaration": False
        }
        
        print("✅ NO PREMATURE VICTORY DECLARATION RULE APPLIED")
        return context
    
    def apply_no_failing_tests_rule(self, context: dict) -> dict:
        """Apply no failing tests rule to ensure test quality."""
        
        print("🧪 APPLYING NO FAILING TESTS RULE")
        
        # Test execution requirements
        context["test_requirements"] = {
            "all_tests_must_pass": True,
            "zero_tolerance_for_failures": True,
            "test_execution_required": True,
            "failure_blocking": True
        }
        
        # Test validation
        context["test_validation"] = {
            "tests_executed": False,
            "all_tests_passing": False,
            "failures_fixed": False,
            "ready_to_proceed": False
        }
        
        print("✅ NO FAILING TESTS RULE APPLIED")
        return context
    
    def apply_boyscout_rule(self, context: dict) -> dict):
        """Apply boy scout rule to ensure codebase improvement."""
        
        print("🏕️ APPLYING BOY SCOUT RULE")
        
        # Cleanup requirements
        context["cleanup_requirements"] = {
            "leave_cleaner_than_found": True,
            "proactive_improvements": True,
            "code_quality_enhancement": True,
            "documentation_improvement": True
        }
        
        # Cleanup validation
        context["cleanup_validation"] = {
            "cleanup_performed": False,
            "improvements_made": False,
            "codebase_enhanced": False,
            "ready_for_commit": False
        }
        
        print("✅ BOY SCOUT RULE APPLIED")
        return context
```

### **Rule Application Triggers**

```python
class RuleApplicationTriggers:
    """Automatic triggers for critical rule application."""
    
    def __init__(self):
        self.framework = CoreRuleApplicationFramework()
    
    def on_work_start(self, context: dict) -> dict:
        """Trigger when work begins."""
        
        print("🚀 WORK START TRIGGER - APPLYING CRITICAL RULES")
        
        # Apply courage rule
        context = self.framework.apply_courage_rule(context)
        
        # Apply no failing tests rule
        context = self.framework.apply_no_failing_tests_rule(context)
        
        # Apply boy scout rule
        context = self.framework.apply_boyscout_rule(context)
        
        return context
    
    def on_progress_report(self, context: dict) -> dict:
        """Trigger when reporting progress."""
        
        print("📊 PROGRESS REPORT TRIGGER - APPLYING CRITICAL RULES")
        
        # Apply no premature success rule
        context = self.framework.apply_no_premature_victory_declaration_rule(context)
        
        # Apply courage rule verification
        context = self.verify_courage_rule_compliance(context)
        
        return context
    
    def on_success_declaration(self, context: dict) -> dict:
        """Trigger when declaring success."""
        
        print("🎉 SUCCESS DECLARATION TRIGGER - APPLYING CRITICAL RULES")
        
        # Apply no premature success rule with strict validation
        context = self.framework.apply_no_premature_victory_declaration_rule(context)
        
        # Validate success declaration
        context = self.validate_success_declaration(context)
        
        return context
    
    def on_work_completion(self, context: dict) -> dict:
        """Trigger when work appears complete."""
        
        print("🏁 WORK COMPLETION TRIGGER - APPLYING CRITICAL RULES")
        
        # Apply all critical rules for final validation
        context = self.framework.apply_critical_rules(context)
        
        # Final validation
        context = self.validate_complete_success(context)
        
        return context
    
    def verify_courage_rule_compliance(self, context: dict) -> dict:
        """Verify compliance with courage rule."""
        
        if not context.get("courage_declaration"):
            print("❌ COURAGE RULE VIOLATION: No courage declaration found")
            context = self.framework.apply_courage_rule(context)
        
        return context
    
    def validate_success_declaration(self, context: dict) -> dict:
        """Validate success declaration against no premature success rule."""
        
        validation = context.get("success_validation", {})
        guard = context.get("success_declaration_guard", {})
        
        if not validation.get("evidence_required"):
            print("❌ NO PREMATURE SUCCESS VIOLATION: Evidence required")
            raise ValueError("Success declaration requires evidence")
        
        if not guard.get("validation_complete"):
            print("❌ NO PREMATURE SUCCESS VIOLATION: Validation not complete")
            raise ValueError("Success declaration requires complete validation")
        
        return context
    
    def validate_complete_success(self, context: dict) -> dict:
        """Validate complete success against all critical rules."""
        
        # Check courage rule compliance
        courage_verification = context.get("courage_verification", {})
        if not courage_verification.get("work_complete"):
            print("❌ COURAGE RULE VIOLATION: Work not complete")
            raise ValueError("Work must be 100% complete")
        
        # Check no premature success compliance
        success_guard = context.get("success_declaration_guard", {})
        if not success_guard.get("ready_for_declaration"):
            print("❌ NO PREMATURE SUCCESS VIOLATION: Not ready for declaration")
            raise ValueError("Success declaration not ready")
        
        # Check no failing tests compliance
        test_validation = context.get("test_validation", {})
        if not test_validation.get("all_tests_passing"):
            print("❌ NO FAILING TESTS VIOLATION: Tests not passing")
            raise ValueError("All tests must pass")
        
        # Check boy scout rule compliance
        cleanup_validation = context.get("cleanup_validation", {})
        if not cleanup_validation.get("codebase_enhanced"):
            print("❌ BOY SCOUT RULE VIOLATION: Codebase not enhanced")
            raise ValueError("Codebase must be enhanced")
        
        print("✅ COMPLETE SUCCESS VALIDATED - ALL CRITICAL RULES COMPLIANT")
        return context
```

### **Automatic Integration System**

```python
class AutomaticRuleIntegration:
    """Automatic integration of critical rules into all workflows."""
    
    def __init__(self):
        self.triggers = RuleApplicationTriggers()
        self.framework = CoreRuleApplicationFramework()
    
    def integrate_into_session_startup(self, context: dict) -> dict:
        """Integrate critical rules into session startup."""
        
        print("🚀 INTEGRATING CRITICAL RULES INTO SESSION STARTUP")
        
        # Apply all critical rules
        context = self.framework.apply_critical_rules(context)
        
        # Apply work start triggers
        context = self.triggers.on_work_start(context)
        
        return context
    
    def integrate_into_progress_reporting(self, context: dict) -> dict:
        """Integrate critical rules into progress reporting."""
        
        print("📊 INTEGRATING CRITICAL RULES INTO PROGRESS REPORTING")
        
        # Apply progress report triggers
        context = self.triggers.on_progress_report(context)
        
        return context
    
    def integrate_into_success_declaration(self, context: dict) -> dict:
        """Integrate critical rules into success declaration."""
        
        print("🎉 INTEGRATING CRITICAL RULES INTO SUCCESS DECLARATION")
        
        # Apply success declaration triggers
        context = self.triggers.on_success_declaration(context)
        
        return context
    
    def integrate_into_work_completion(self, context: dict) -> dict:
        """Integrate critical rules into work completion."""
        
        print("🏁 INTEGRATING CRITICAL RULES INTO WORK COMPLETION")
        
        # Apply work completion triggers
        context = self.triggers.on_work_completion(context)
        
        return context
```

## Framework Usage

### **Automatic Application**

The framework automatically applies critical rules to every situation:

```python
# Initialize framework
integration = AutomaticRuleIntegration()

# Session startup with critical rules
context = integration.integrate_into_session_startup(context)

# Progress reporting with critical rules
context = integration.integrate_into_progress_reporting(context)

# Success declaration with critical rules
context = integration.integrate_into_success_declaration(context)

# Work completion with critical rules
context = integration.integrate_into_work_completion(context)
```

### **Manual Application**

For specific situations, apply critical rules manually:

```python
# Initialize framework
framework = CoreRuleApplicationFramework()

# Apply all critical rules
context = framework.apply_critical_rules(context)

# Apply specific rule
context = framework.apply_courage_rule(context)
context = framework.apply_no_premature_victory_declaration_rule(context)
```

## Framework Benefits

### **1. Always Active Critical Rules**
- Courage rule automatically applied to every work session
- No premature success rule automatically applied to every progress report
- No failing tests rule automatically applied to every development session
- Boy scout rule automatically applied to every code modification

### **2. Automatic Integration**
- No manual rule application required
- Rules integrated into all workflows automatically
- Consistent rule application across all situations
- Zero chance of missing critical rules

### **3. Comprehensive Validation**
- Automatic validation of rule compliance
- Prevention of rule violations
- Clear feedback on rule application
- Systematic enforcement of critical principles

### **4. Systematic Organization**
- Clear rule hierarchy and priorities
- Automatic rule discovery and application
- Consistent rule enforcement patterns
- Easy rule management and maintenance

## Framework Enforcement

### **Automatic Enforcement**
- Critical rules applied automatically to every situation
- No manual intervention required
- Consistent enforcement across all contexts
- Zero tolerance for rule violations

### **Validation Enforcement**
- Automatic validation of rule compliance
- Prevention of premature success declarations
- Enforcement of courage-based work completion
- Systematic test quality assurance

### **Integration Enforcement**
- Automatic integration into all workflows
- Seamless rule application without disruption
- Consistent rule behavior across all systems
- Reliable rule enforcement mechanisms

## Framework Monitoring

### **Rule Application Monitoring**
- Track rule application frequency
- Monitor rule compliance rates
- Measure rule effectiveness
- Identify rule application gaps

### **Performance Monitoring**
- Monitor framework performance impact
- Track rule application speed
- Measure system resource usage
- Optimize rule application efficiency

### **Effectiveness Monitoring**
- Monitor rule effectiveness metrics
- Track rule violation rates
- Measure rule compliance improvements
- Validate rule application outcomes

## Framework Maintenance

### **Regular Updates**
- Update rule definitions as needed
- Enhance rule application logic
- Improve rule validation mechanisms
- Optimize rule performance

### **Continuous Improvement**
- Monitor rule effectiveness
- Identify improvement opportunities
- Implement rule enhancements
- Validate rule improvements

### **Documentation Updates**
- Keep framework documentation current
- Update usage examples
- Maintain rule application guides
- Document best practices

## Conclusion

This Core Rule Application Framework ensures that our most critical rules - the Courage Rule and No Premature Success Rule - are always active and properly applied to every situation. It provides automatic integration, comprehensive validation, and systematic enforcement of our core development principles.

**The framework guarantees that:**
- **Courage Rule** is applied to every work session
- **No Premature Success Rule** is applied to every progress report
- **No Failing Tests Rule** is applied to every development session
- **Boy Scout Rule** is applied to every code modification

This ensures our goal of **fully automated, crystal clear code production** with **accurate tests and documentation** and **up-to-date project tracking** following **excellence standards**.



# === user_controlled_success_declaration_rule ===
# User Controlled Success Declaration Rule

**CRITICAL**: In specific situations where the user explicitly states they want to see results first and will generate their own success messages, NEVER generate success declarations, victory messages, or completion summaries. Let the user see the actual work results and decide on success themselves.

## Core Principle

**"User Controls Success When Explicitly Requested"**

When the user says phrases like:
- "we must see the result of your work immediately"
- "never generate success messages in situations like that"
- "i need a rule for that, that I am in control of success"
- "stop this in situations like that"

The user is taking control of success declaration. Respect this control.

## Requirements

### 1. **No Success Messages When User Controls**
- Never generate "✅ Success!" messages
- Never create completion summaries
- Never declare victory or completion
- Never use celebration emojis or success indicators
- Never say "work is complete" or similar phrases

### 2. **Show Results Only**
- Present actual work results
- Show data, files, or outputs
- Display verification results
- Provide evidence of work done
- Let results speak for themselves

### 3. **Wait for User Assessment**
- Let user evaluate the results
- Wait for user's success declaration
- Don't preempt user's judgment
- Respect user's authority in these situations

## Enforcement

This rule is **ALWAYS ACTIVE** and applies when:

- User explicitly states they control success declaration
- User says "never generate success messages"
- User wants to see results first
- User takes authority over completion assessment

## Examples

### ❌ FORBIDDEN (When User Controls Success)
```
✅ Rule system optimization complete!
🎉 Success! All issues resolved!
🏆 Work finished successfully!
```

### ✅ ALLOWED (When User Controls Success)
```
Results of rule system verification:
- 3 core rules with alwaysApply: true
- 38 context rules with alwaysApply: false
- System health score: 80/100
```

## Remember

**"When user controls success, show results and wait."**

**"Let the user decide what constitutes success."**

**"Results speak louder than success declarations."**



# === scientific_communication_rule ===
---
description: "Enforce scientific, rational communication without emotional decorations or marketing language"
category: "core-foundation"
priority: "critical"
alwaysApply: true
globs: ["**/*"]
tags: ['core_foundation', 'communication', 'scientific']
tier: "1"
---

# Scientific Communication Rule

**CRITICAL**: All communication must be scientific, rational, and factual. No emotional decorations, marketing language, or unnecessary embellishments.

## Core Principle

**"Rationality and Clarity Over Emotions and Marketing"**

Communication must be based on facts, evidence, and logical reasoning. Emotional language and marketing-style presentations are prohibited.

## Requirements

### 1. **Factual Communication Only**
- State facts without emotional interpretation
- Use precise, measurable language
- Provide evidence for claims
- Avoid subjective opinions presented as facts

### 2. **No Emotional Decorations**
**FORBIDDEN:**
- Celebration emojis (🎉, ✨, 🚀)
- Emotional exclamations ("Amazing!", "Fantastic!")
- Marketing superlatives ("incredible", "revolutionary", "game-changing")
- Dramatic language ("breakthrough", "stunning", "phenomenal")

**REQUIRED:**
- Neutral, descriptive language
- Objective measurements
- Clear, direct statements
- Evidence-based conclusions

### 3. **Scientific Precision**
- Use specific measurements instead of vague terms
- Provide quantifiable results
- State confidence levels and uncertainty
- Document methodology and assumptions

### 4. **Rational Structure**
- Present information logically
- Separate observations from conclusions
- Show cause-and-effect relationships clearly
- Use systematic problem-solving approaches

## Examples

### ❌ FORBIDDEN (Emotional/Marketing Style)
```
🎉 Amazing breakthrough! Our revolutionary system delivers incredible results!
✨ Fantastic performance improvements that will blow your mind!
🚀 Game-changing solution with phenomenal efficiency gains!
```

### ✅ REQUIRED (Scientific Style)
```
System performance analysis:
- Rule count reduced from 33 to 5 (84.8% reduction)
- Context detection accuracy: 95% (19/20 test cases)
- File generation time: 0.3 seconds average
- Memory usage decreased by 67%
```

## Communication Standards

### 1. **Data Presentation**
- Always include specific numbers
- Show before/after comparisons
- Provide confidence intervals when applicable
- State sample sizes and test conditions

### 2. **Problem Description**
- Define problems objectively
- Quantify impact where possible
- Identify root causes systematically
- Separate symptoms from causes

### 3. **Solution Documentation**
- Describe methodology clearly
- Show implementation steps
- Provide verification methods
- Document limitations and assumptions

### 4. **Progress Reporting**
- Use measurable metrics
- Show completion percentages
- Report actual vs. expected results
- Identify remaining work objectively

## Enforcement

This rule is **ALWAYS ACTIVE** and applies to:

- All written communication
- All progress reports
- All documentation
- All code comments
- All user interactions

## Remember

**"Facts over feelings, evidence over excitement, clarity over celebration."**

**"Measure twice, communicate once."**

**"Rational discourse produces rational results."**


# === agile_artifacts_maintenance_rule ===
---
description: "Auto-generated description for agile_artifacts_maintenance_rule.mdc"
category: "agile-methodology"
priority: "high"
alwaysApply: false
contexts: ['AGILE', 'DEFAULT']
globs: ["**/*"]
tags: ['agile_methodology', 'agile', 'project_management']
tier: "2"
---
# Agile Artifacts Maintenance Rule

**CRITICAL**: Automatically maintain all agile artifacts (product backlog, sprint backlog, user stories, velocity tracking, burndown charts) in real-time to ensure maximum quality project management and maximum efficiency in agile processes.

## Description
This rule enforces automatic maintenance of all agile artifacts through intelligent detection of changes and automated updates. It applies holistic-detailed thinking to balance comprehensive agile tracking with efficient automation, ensuring our agile process is always current, accurate, and valuable for project management.

## Core Requirements

### 1. **Agile Artifact Detection and Monitoring**
**MANDATORY**: Systematically monitor and detect changes requiring agile updates
```python
# REQUIRED: Agile artifact monitoring system
class AgileArtifactMonitor:
    """Holistic-detailed system for monitoring agile artifacts."""
    
    def __init__(self):
        self.artifact_types = {
            "product_backlog": self._monitor_product_backlog,
            "sprint_backlog": self._monitor_sprint_backlog,
            "user_stories": self._monitor_user_stories,
            "velocity_tracking": self._monitor_velocity,
            "burndown_charts": self._monitor_burndown,
            "sprint_planning": self._monitor_sprint_planning,
            "retrospectives": self._monitor_retrospectives,
            "definition_of_done": self._monitor_definition_of_done
        }
        
        self.update_triggers = {
            "user_story_completed": 1.0,
            "sprint_completed": 1.0,
            "feature_completed": 0.9,
            "bug_fixed": 0.7,
            "test_coverage_improved": 0.6,
            "documentation_updated": 0.5,
            "code_quality_improved": 0.6,
            "performance_optimized": 0.7,
            "security_enhanced": 0.8,
            "technical_debt_reduced": 0.6
        }
    
    def monitor_agile_artifacts(self, session_context: dict) -> AgileMonitoringResult:
        """Monitor all agile artifacts for required updates."""
        
        print("📊 MONITORING AGILE ARTIFACTS")
        print("=" * 50)
        
        monitoring_results = {}
        update_required = False
        priority_updates = []
        
        # Monitor each artifact type
        for artifact_type, monitor_func in self.artifact_types.items():
            print(f"\n📋 Monitoring: {artifact_type}")
            result = monitor_func(session_context)
            monitoring_results[artifact_type] = result
            
            if result.update_required:
                update_required = True
                priority_updates.append({
                    "artifact": artifact_type,
                    "priority": result.priority,
                    "reason": result.reason,
                    "updates": result.required_updates
                })
                print(f"   🔴 UPDATE REQUIRED: {result.reason}")
            else:
                print(f"   ✅ UP TO DATE: {artifact_type}")
        
        # Sort updates by priority
        priority_updates.sort(key=lambda x: x["priority"], reverse=True)
        
        print(f"\n📊 MONITORING SUMMARY:")
        print(f"   Artifacts Monitored: {len(self.artifact_types)}")
        print(f"   Updates Required: {len(priority_updates)}")
        print(f"   Overall Status: {'🔴 UPDATES NEEDED' if update_required else '✅ ALL CURRENT'}")
        
        return AgileMonitoringResult(
            update_required=update_required,
            priority_updates=priority_updates,
            monitoring_results=monitoring_results
        )
    
    def _monitor_product_backlog(self, context: dict) -> ArtifactMonitoringResult:
        """Monitor product backlog for required updates."""
        
        updates_needed = []
        priority = 0
        
        # Check for completed user stories
        if context.get("user_story_completed"):
            updates_needed.append("mark_user_story_completed")
            priority = max(priority, 0.9)
        
        # Check for new features or requirements
        if context.get("new_feature_identified"):
            updates_needed.append("add_new_user_story")
            priority = max(priority, 0.8)
        
        # Check for priority changes
        if context.get("priority_changes"):
            updates_needed.append("update_story_priorities")
            priority = max(priority, 0.7)
        
        # Check for effort estimation updates
        if context.get("effort_estimation_updated"):
            updates_needed.append("update_story_points")
            priority = max(priority, 0.6)
        
        # Check for epic completion
        if context.get("epic_completed"):
            updates_needed.append("mark_epic_completed")
            priority = max(priority, 1.0)
        
        return ArtifactMonitoringResult(
            update_required=len(updates_needed) > 0,
            priority=priority,
            reason=f"Product backlog updates: {', '.join(updates_needed)}" if updates_needed else "No updates needed",
            required_updates=updates_needed
        )
    
    def _monitor_user_stories(self, context: dict) -> ArtifactMonitoringResult:
        """Monitor user stories for status and progress updates."""
        
        updates_needed = []
        priority = 0
        
        # Check for story completion
        if context.get("user_story_completed"):
            updates_needed.append("mark_story_completed")
            priority = max(priority, 1.0)
        
        # Check for story progress
        if context.get("story_progress_made"):
            updates_needed.append("update_story_progress")
            priority = max(priority, 0.7)
        
        # Check for acceptance criteria updates
        if context.get("acceptance_criteria_met"):
            updates_needed.append("update_acceptance_criteria")
            priority = max(priority, 0.8)
        
        # Check for story status changes
        if context.get("story_status_changed"):
            updates_needed.append("update_story_status")
            priority = max(priority, 0.9)
        
        # Check for blockers or impediments
        if context.get("story_blocked"):
            updates_needed.append("add_story_blocker")
            priority = max(priority, 0.9)
        
        return ArtifactMonitoringResult(
            update_required=len(updates_needed) > 0,
            priority=priority,
            reason=f"User story updates: {', '.join(updates_needed)}" if updates_needed else "No updates needed",
            required_updates=updates_needed
        )
    
    def _monitor_velocity(self, context: dict) -> ArtifactMonitoringResult:
        """Monitor velocity tracking for updates."""
        
        updates_needed = []
        priority = 0
        
        # Check for sprint completion
        if context.get("sprint_completed"):
            updates_needed.append("calculate_sprint_velocity")
            priority = max(priority, 1.0)
        
        # Check for story point changes
        if context.get("story_points_updated"):
            updates_needed.append("recalculate_velocity")
            priority = max(priority, 0.8)
        
        # Check for team changes
        if context.get("team_capacity_changed"):
            updates_needed.append("update_team_capacity")
            priority = max(priority, 0.7)
        
        return ArtifactMonitoringResult(
            update_required=len(updates_needed) > 0,
            priority=priority,
            reason=f"Velocity updates: {', '.join(updates_needed)}" if updates_needed else "No updates needed",
            required_updates=updates_needed
        )
```

### 2. **Automated Agile Update System**
**MANDATORY**: Automatically update agile artifacts based on detected changes
```python
# REQUIRED: Automated agile update system
class AutomatedAgileUpdater:
    """Automated system for updating agile artifacts."""
    
    def __init__(self):
        self.update_handlers = {
            "product_backlog": self._update_product_backlog,
            "sprint_backlog": self._update_sprint_backlog,
            "user_stories": self._update_user_stories,
            "velocity_tracking": self._update_velocity_tracking,
            "burndown_charts": self._update_burndown_charts,
            "sprint_planning": self._update_sprint_planning
        }
        
        self.backlog_file = "docs/agile/planning/product_backlog.md"
        self.sprint_files = "docs/agile/templates/sprint_planning/"
        self.velocity_file = "docs/agile/execution/velocity_tracking.md"
        self.burndown_file = "docs/agile/execution/burndown_charts.md"
    
    def execute_agile_updates(self, monitoring_result: AgileMonitoringResult,
                             session_context: dict) -> AgileUpdateResult:
        """Execute all required agile artifact updates."""
        
        print("🔄 EXECUTING AGILE ARTIFACT UPDATES")
        print("=" * 50)
        
        update_results = {}
        all_successful = True
        
        # Execute updates in priority order
        for update_item in monitoring_result.priority_updates:
            artifact_type = update_item["artifact"]
            updates = update_item["updates"]
            
            print(f"\n📋 Updating: {artifact_type}")
            print(f"   Priority: {update_item['priority']}")
            print(f"   Reason: {update_item['reason']}")
            
            # Execute update
            update_handler = self.update_handlers.get(artifact_type)
            if update_handler:
                result = update_handler(updates, session_context)
                update_results[artifact_type] = result
                
                if result.success:
                    print(f"   ✅ SUCCESS: {result.summary}")
                else:
                    print(f"   ❌ FAILED: {result.error}")
                    all_successful = False
            else:
                print(f"   ❌ NO HANDLER: No update handler for {artifact_type}")
                all_successful = False
        
        print(f"\n📊 UPDATE SUMMARY:")
        print(f"   Total Updates: {len(monitoring_result.priority_updates)}")
        print(f"   Successful: {len([r for r in update_results.values() if r.success])}")
        print(f"   Failed: {len([r for r in update_results.values() if not r.success])}")
        print(f"   Overall Status: {'✅ ALL SUCCESSFUL' if all_successful else '❌ SOME FAILED'}")
        
        return AgileUpdateResult(
            success=all_successful,
            update_results=update_results,
            total_updates=len(monitoring_result.priority_updates)
        )
    
    def _update_product_backlog(self, updates: List[str], context: dict) -> UpdateResult:
        """Update the product backlog file."""
        
        try:
            # Read current product backlog
            with open(self.backlog_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            modified_content = content
            changes_made = []
            
            # Process each update
            for update in updates:
                if update == "mark_user_story_completed":
                    story_id = context.get("completed_story_id")
                    if story_id:
                        # Mark story as completed
                        pattern = f"(#### \\*\\*{story_id}:.*?)\\n\\*\\*Priority\\*\\*:"
                        replacement = f"\\1 ✅ **COMPLETED**\\n**Priority**:"
                        modified_content = re.sub(pattern, replacement, modified_content, flags=re.MULTILINE | re.DOTALL)
                        changes_made.append(f"Marked {story_id} as completed")
                
                elif update == "add_new_user_story":
                    new_story = context.get("new_user_story")
                    if new_story:
                        # Add new user story to appropriate epic
                        epic_section = self._find_epic_section(modified_content, new_story.get("epic"))
                        if epic_section:
                            story_text = self._format_user_story(new_story)
                            modified_content = self._insert_user_story(modified_content, epic_section, story_text)
                            changes_made.append(f"Added new user story: {new_story.get('id')}")
                
                elif update == "update_story_priorities":
                    priority_updates = context.get("priority_updates", {})
                    for story_id, new_priority in priority_updates.items():
                        # Update story priority
                        pattern = f"(#### \\*\\*{story_id}:.*?\\n\\*\\*Priority\\*\\*:).*?(\\|)"
                        replacement = f"\\1 {new_priority} \\2"
                        modified_content = re.sub(pattern, replacement, modified_content, flags=re.MULTILINE | re.DOTALL)
                        changes_made.append(f"Updated {story_id} priority to {new_priority}")
                
                elif update == "mark_epic_completed":
                    epic_id = context.get("completed_epic_id")
                    if epic_id:
                        # Mark epic as completed
                        pattern = f"(## 🚀 \\*\\*{epic_id}:.*?)\\n\\*\\*Priority\\*\\*:"
                        replacement = f"\\1 ✅ **COMPLETED**\\n**Priority**:"
                        modified_content = re.sub(pattern, replacement, modified_content, flags=re.MULTILINE | re.DOTALL)
                        changes_made.append(f"Marked epic {epic_id} as completed")
            
            # Write updated content if changes were made
            if changes_made:
                with open(self.backlog_file, 'w', encoding='utf-8') as f:
                    f.write(modified_content)
                
                return UpdateResult(
                    success=True,
                    summary=f"Product backlog updated: {', '.join(changes_made)}",
                    changes_made=len(changes_made)
                )
            else:
                return UpdateResult(
                    success=True,
                    summary="No changes needed for product backlog",
                    changes_made=0
                )
                
        except Exception as e:
            return UpdateResult(
                success=False,
                error=f"Failed to update product backlog: {e}",
                changes_made=0
            )
    
    def _update_user_stories(self, updates: List[str], context: dict) -> UpdateResult:
        """Update user story statuses and progress."""
        
        changes_made = []
        
        try:
            # Process user story updates
            for update in updates:
                if update == "mark_story_completed":
                    story_id = context.get("completed_story_id")
                    if story_id:
                        # Update story status in product backlog
                        result = self._mark_story_completed_in_backlog(story_id)
                        if result:
                            changes_made.append(f"Marked story {story_id} as completed")
                
                elif update == "update_story_progress":
                    progress_updates = context.get("story_progress", {})
                    for story_id, progress in progress_updates.items():
                        result = self._update_story_progress_in_backlog(story_id, progress)
                        if result:
                            changes_made.append(f"Updated {story_id} progress to {progress}%")
                
                elif update == "update_acceptance_criteria":
                    criteria_updates = context.get("acceptance_criteria_updates", {})
                    for story_id, criteria in criteria_updates.items():
                        result = self._update_acceptance_criteria_in_backlog(story_id, criteria)
                        if result:
                            changes_made.append(f"Updated acceptance criteria for {story_id}")
            
            return UpdateResult(
                success=True,
                summary=f"User stories updated: {', '.join(changes_made)}" if changes_made else "No user story updates needed",
                changes_made=len(changes_made)
            )
            
        except Exception as e:
            return UpdateResult(
                success=False,
                error=f"Failed to update user stories: {e}",
                changes_made=len(changes_made)
            )
    
    def _update_velocity_tracking(self, updates: List[str], context: dict) -> UpdateResult:
        """Update velocity tracking metrics."""
        
        changes_made = []
        
        try:
            for update in updates:
                if update == "calculate_sprint_velocity":
                    sprint_id = context.get("completed_sprint_id")
                    completed_points = context.get("completed_story_points", 0)
                    
                    if sprint_id and completed_points:
                        # Update velocity tracking file
                        velocity_data = self._calculate_and_record_velocity(sprint_id, completed_points)
                        changes_made.append(f"Recorded velocity for {sprint_id}: {completed_points} points")
                
                elif update == "recalculate_velocity":
                    # Recalculate average velocity
                    new_average = self._recalculate_average_velocity()
                    changes_made.append(f"Recalculated average velocity: {new_average}")
            
            return UpdateResult(
                success=True,
                summary=f"Velocity tracking updated: {', '.join(changes_made)}" if changes_made else "No velocity updates needed",
                changes_made=len(changes_made)
            )
            
        except Exception as e:
            return UpdateResult(
                success=False,
                error=f"Failed to update velocity tracking: {e}",
                changes_made=len(changes_made)
            )
```

### 3. **Agile Metrics and Analytics**
**MANDATORY**: Automatically calculate and update agile metrics
```python
# REQUIRED: Agile metrics and analytics system
class AgileMetricsCalculator:
    """Calculate and maintain agile metrics automatically."""
    
    def __init__(self):
        self.metrics_config = {
            "velocity": {
                "calculation_method": "average_last_3_sprints",
                "minimum_sprints": 2,
                "weight_recent": 0.6
            },
            "burndown": {
                "update_frequency": "daily",
                "include_weekends": False,
                "track_scope_changes": True
            },
            "cycle_time": {
                "track_stages": ["todo", "in_progress", "review", "done"],
                "exclude_blocked_time": True
            },
            "lead_time": {
                "start_point": "backlog_entry",
                "end_point": "production_release"
            }
        }
    
    def calculate_agile_metrics(self, context: dict) -> MetricsResult:
        """Calculate all agile metrics based on current data."""
        
        print("📊 CALCULATING AGILE METRICS")
        print("=" * 50)
        
        metrics = {}
        
        # Calculate velocity metrics
        velocity_metrics = self._calculate_velocity_metrics(context)
        metrics.update(velocity_metrics)
        
        # Calculate burndown metrics
        burndown_metrics = self._calculate_burndown_metrics(context)
        metrics.update(burndown_metrics)
        
        # Calculate cycle time metrics
        cycle_time_metrics = self._calculate_cycle_time_metrics(context)
        metrics.update(cycle_time_metrics)
        
        # Calculate quality metrics
        quality_metrics = self._calculate_quality_metrics(context)
        metrics.update(quality_metrics)
        
        # Calculate predictability metrics
        predictability_metrics = self._calculate_predictability_metrics(context)
        metrics.update(predictability_metrics)
        
        print(f"📊 CALCULATED METRICS:")
        for metric_name, value in metrics.items():
            print(f"   {metric_name}: {value}")
        
        return MetricsResult(
            metrics=metrics,
            calculation_timestamp=datetime.now(),
            data_quality=self._assess_data_quality(context)
        )
    
    def _calculate_velocity_metrics(self, context: dict) -> Dict[str, float]:
        """Calculate velocity-related metrics."""
        
        sprint_data = context.get("sprint_history", [])
        if len(sprint_data) < 2:
            return {"velocity_average": 0, "velocity_trend": 0}
        
        # Extract completed points from last sprints
        recent_velocities = [sprint.get("completed_points", 0) for sprint in sprint_data[-5:]]
        
        # Calculate average velocity
        if len(recent_velocities) >= 3:
            # Weighted average (more recent sprints have higher weight)
            weights = [0.4, 0.3, 0.3] if len(recent_velocities) == 3 else [0.3, 0.25, 0.25, 0.2]
            velocity_average = sum(v * w for v, w in zip(recent_velocities[-len(weights):], weights))
        else:
            velocity_average = sum(recent_velocities) / len(recent_velocities)
        
        # Calculate velocity trend
        if len(recent_velocities) >= 2:
            velocity_trend = (recent_velocities[-1] - recent_velocities[-2]) / recent_velocities[-2]
        else:
            velocity_trend = 0
        
        return {
            "velocity_average": round(velocity_average, 1),
            "velocity_trend": round(velocity_trend * 100, 1),  # Percentage
            "velocity_consistency": self._calculate_velocity_consistency(recent_velocities)
        }
    
    def _calculate_burndown_metrics(self, context: dict) -> Dict[str, float]:
        """Calculate burndown-related metrics."""
        
        current_sprint = context.get("current_sprint", {})
        if not current_sprint:
            return {}
        
        total_points = current_sprint.get("total_points", 0)
        completed_points = current_sprint.get("completed_points", 0)
        days_elapsed = current_sprint.get("days_elapsed", 0)
        total_days = current_sprint.get("total_days", 10)  # Default 2-week sprint
        
        if total_points == 0:
            return {}
        
        # Calculate ideal burndown
        ideal_remaining = total_points * (1 - days_elapsed / total_days)
        actual_remaining = total_points - completed_points
        
        # Calculate burndown metrics
        burndown_variance = actual_remaining - ideal_remaining
        completion_rate = completed_points / total_points if total_points > 0 else 0
        projected_completion = (completed_points / days_elapsed * total_days) if days_elapsed > 0 else 0
        
        return {
            "burndown_variance": round(burndown_variance, 1),
            "completion_rate": round(completion_rate * 100, 1),
            "projected_completion": round(min(projected_completion / total_points * 100, 100), 1),
            "on_track": abs(burndown_variance) <= total_points * 0.1  # Within 10%
        }
```

### 4. **Integration with Development Workflow**
**MANDATORY**: Integrate agile maintenance with development workflow
```python
# REQUIRED: Development workflow integration
class AgileWorkflowIntegration:
    """Integration of agile maintenance with development workflow."""
    
    def __init__(self):
        self.monitor = AgileArtifactMonitor()
        self.updater = AutomatedAgileUpdater()
        self.metrics_calculator = AgileMetricsCalculator()
        
        self.integration_points = {
            "code_commit": self._handle_code_commit,
            "test_completion": self._handle_test_completion,
            "feature_completion": self._handle_feature_completion,
            "bug_fix": self._handle_bug_fix,
            "sprint_start": self._handle_sprint_start,
            "sprint_end": self._handle_sprint_end,
            "story_status_change": self._handle_story_status_change
        }
    
    def integrate_with_workflow(self, workflow_event: str, context: dict) -> IntegrationResult:
        """Integrate agile maintenance with workflow events."""
        
        print(f"🔄 INTEGRATING AGILE WITH WORKFLOW EVENT: {workflow_event}")
        print("=" * 60)
        
        # Handle specific workflow event
        handler = self.integration_points.get(workflow_event)
        if not handler:
            return IntegrationResult(
                success=False,
                reason=f"No handler for workflow event: {workflow_event}"
            )
        
        # Execute event-specific handling
        event_result = handler(context)
        
        # Always check for general agile updates
        monitoring_result = self.monitor.monitor_agile_artifacts(context)
        
        if monitoring_result.update_required:
            # Execute agile updates
            update_result = self.updater.execute_agile_updates(monitoring_result, context)
            
            # Calculate updated metrics
            metrics_result = self.metrics_calculator.calculate_agile_metrics(context)
            
            return IntegrationResult(
                success=update_result.success,
                event_result=event_result,
                agile_updates=update_result,
                metrics=metrics_result.metrics
            )
        else:
            return IntegrationResult(
                success=True,
                event_result=event_result,
                reason="No agile updates required"
            )
    
    def _handle_feature_completion(self, context: dict) -> EventResult:
        """Handle feature completion event."""
        
        feature_id = context.get("feature_id")
        user_story_id = context.get("user_story_id")
        
        if user_story_id:
            # Mark user story as completed
            context["user_story_completed"] = True
            context["completed_story_id"] = user_story_id
            
            # Update story points if provided
            if "story_points" in context:
                context["completed_story_points"] = context["story_points"]
            
            return EventResult(
                success=True,
                summary=f"Feature {feature_id} completed, user story {user_story_id} marked for completion"
            )
        
        return EventResult(
            success=False,
            reason="No user story ID provided for feature completion"
        )
    
    def _handle_sprint_end(self, context: dict) -> EventResult:
        """Handle sprint end event."""
        
        sprint_id = context.get("sprint_id")
        if not sprint_id:
            return EventResult(success=False, reason="No sprint ID provided")
        
        # Calculate sprint metrics
        completed_stories = context.get("completed_stories", [])
        total_points_completed = sum(story.get("points", 0) for story in completed_stories)
        
        # Update context for velocity calculation
        context["sprint_completed"] = True
        context["completed_sprint_id"] = sprint_id
        context["completed_story_points"] = total_points_completed
        
        # Mark all completed stories
        for story in completed_stories:
            context[f"story_{story['id']}_completed"] = True
        
        return EventResult(
            success=True,
            summary=f"Sprint {sprint_id} ended with {total_points_completed} points completed"
        )
```

## Implementation Strategy

### 1. **Holistic Integration**
**MANDATORY**: Integrate with all existing systems holistically
- Integrate with automated Git protection rule
- Integrate with documentation live updates rule
- Integrate with test execution and quality validation
- Integrate with rule compliance monitoring
- Integrate with continuous improvement processes

### 2. **Maximum Quality and Efficiency Balance**
**MANDATORY**: Optimize for both maximum quality and maximum efficiency
- Quality: Accurate tracking, comprehensive metrics, reliable automation
- Efficiency: Automated updates, intelligent triggers, minimal manual work
- Balance: Smart detection, configurable thresholds, adaptive behavior

### 3. **Detailed Implementation**
**MANDATORY**: Apply detailed thinking to all aspects
- Comprehensive monitoring of all agile artifacts
- Intelligent update triggers based on development events
- Accurate metrics calculation with trend analysis
- Full integration with development workflow events
- Real-time maintenance of agile process health

## Benefits

- **Always Current**: Agile artifacts are always up-to-date and accurate
- **Maximum Quality**: Comprehensive tracking ensures nothing is missed
- **Maximum Efficiency**: Automated maintenance eliminates manual work
- **Intelligent Behavior**: Smart detection and updates adapt to context
- **Full Integration**: Works seamlessly with development workflow
- **Holistic Approach**: Considers all aspects of agile process management

## Enforcement

This rule is **CONDITIONALLY APPLIED** based on context.

### Critical Checkpoints:
- [ ] Agile artifact monitoring system active
- [ ] Automated update triggers configured
- [ ] Integration with development workflow validated
- [ ] Metrics calculation working properly
- [ ] Real-time maintenance functional
- [ ] Quality and accuracy verified

### Success Metrics:
- 100% agile artifacts maintained automatically
- Zero manual agile artifact updates required
- 95%+ accuracy in agile tracking and metrics
- Real-time updates within 5 minutes of events
- Seamless integration with all workflows

**Remember: Agile artifacts are the foundation of our project management. They must always be current, accurate, and valuable for decision-making.**


# === agile_sprint_management_rule ===
---
description: "Auto-generated description for agile_sprint_management_rule.mdc"
category: "agile-methodology"
priority: "high"
alwaysApply: false
contexts: ['AGILE', 'DEFAULT']
globs: ["**/*"]
tags: ['agile_methodology', 'agile', 'project_management']
tier: "2"
---
# Agile Sprint Management Rule

**CRITICAL**: Always follow Agile sprint methodology with 2-week iterations, daily standups, and continuous delivery principles. This rule integrates with the existing TDD philosophy and systematic problem-solving approach.

## Core Sprint Requirements

### 1. Sprint Planning Integration
**MANDATORY**: Every development session starts with sprint planning and backlog refinement.

```python
# Sprint Planning Template - Integrated with TDD
class SprintPlanning:
    def __init__(self, sprint_number: int, duration_days: int = 14):
        self.sprint_number = sprint_number
        self.duration_days = duration_days
        self.user_stories = []
        self.sprint_goal = ""
        self.velocity_target = 0
        self.start_date = None
        self.end_date = None
        self.definition_of_ready = [
            "user_story_has_clear_acceptance_criteria",
            "user_story_is_estimable",
            "user_story_has_test_scenarios_defined",
            "user_story_fits_sprint_capacity",
            "user_story_has_clear_definition_of_done"
        ]
        self.definition_of_done = [
            "code_is_written_and_tested",
            "all_tests_pass",
            "code_review_is_completed",
            "documentation_is_updated",
            "acceptance_criteria_are_met",
            "performance_requirements_are_met",
            "security_requirements_are_met"
        ]
    
    def add_user_story(self, title: str, description: str, story_points: int, 
                      acceptance_criteria: List[str], test_scenarios: List[str]):
        """Add user story to sprint backlog with TDD integration"""
        story = {
            "title": title,
            "description": description,
            "story_points": story_points,
            "status": "planned",
            "acceptance_criteria": acceptance_criteria,
            "test_scenarios": test_scenarios,  # TDD integration
            "tasks": [],
            "test_coverage_target": 90,  # Aligns with TDD rule
            "definition_of_ready_met": False,
            "definition_of_done_met": False
        }
        self.user_stories.append(story)
    
    def set_sprint_goal(self, goal: str, success_metrics: List[str]):
        """Define sprint goal and measurable success criteria"""
        self.sprint_goal = goal
        self.success_metrics = success_metrics
    
    def validate_sprint_readiness(self) -> Dict[str, bool]:
        """Validate sprint readiness using systematic problem-solving approach"""
        validation_results = {}
        
        # Check each definition of ready criterion
        for criterion in self.definition_of_ready:
            validation_results[criterion] = self._validate_criterion(criterion)
        
        # Check sprint capacity vs story points
        total_story_points = sum(story["story_points"] for story in self.user_stories)
        validation_results["capacity_appropriate"] = total_story_points <= self.velocity_target * 1.2
        
        return validation_results
```

### 2. Daily Standup Integration
**MANDATORY**: Daily progress tracking and blocker identification with systematic problem-solving.

```python
# Daily Standup Template - Integrated with Error Exposure Rule
class DailyStandup:
    def __init__(self, sprint_number: int, day_number: int):
        self.sprint_number = sprint_number
        self.day_number = day_number
        self.team_members = []
        self.blockers = []
        self.velocity_update = 0
        self.sprint_goal_progress = 0.0
        self.error_reports = []  # Aligns with Error Exposure Rule
    
    def conduct_standup(self) -> Dict[str, Any]:
        """Conduct daily standup with three questions and error exposure"""
        standup_data = {
            "yesterday_completed": [],
            "today_planned": [],
            "blockers": [],
            "velocity_update": 0,
            "sprint_goal_progress": 0.0,
            "errors_exposed": [],  # Error Exposure Rule integration
            "tdd_status": {}  # TDD Rule integration
        }
        
        # Collect team member updates
        for member in self.team_members:
            member_update = self._get_member_update(member)
            standup_data["yesterday_completed"].extend(member_update["completed"])
            standup_data["today_planned"].extend(member_update["planned"])
            standup_data["blockers"].extend(member_update["blockers"])
            standup_data["errors_exposed"].extend(member_update["errors"])
            standup_data["tdd_status"][member] = member_update["tdd_status"]
        
        # Update velocity and progress
        standup_data["velocity_update"] = self._calculate_velocity_update()
        standup_data["sprint_goal_progress"] = self._calculate_sprint_progress()
        
        return standup_data
    
    def _get_member_update(self, member: str) -> Dict[str, Any]:
        """Get individual team member update with TDD and error exposure"""
        return {
            "completed": [],  # Completed tasks with test coverage
            "planned": [],    # Planned tasks with test scenarios
            "blockers": [],   # Blockers that need systematic resolution
            "errors": [],     # Errors exposed (Error Exposure Rule)
            "tdd_status": {   # TDD compliance status
                "tests_written_first": True,
                "test_coverage_met": True,
                "refactoring_completed": True
            }
        }
```

### 3. Sprint Backlog Management
**MANDATORY**: Maintain prioritized sprint backlog with continuous validation.

```python
# Sprint Backlog Management - Integrated with Continuous Validation Rule
class SprintBacklog:
    def __init__(self, sprint_number: int):
        self.sprint_number = sprint_number
        self.user_stories = []
        self.sprint_goal = ""
        self.velocity_target = 0
        self.daily_progress = []
        self.validation_checks = []  # Continuous Validation Rule integration
    
    def add_story_to_sprint(self, story: Dict[str, Any]) -> bool:
        """Add user story to current sprint with validation"""
        # Validate story readiness (Continuous Validation Rule)
        if not self._validate_story_readiness(story):
            raise ValueError(f"Story '{story['title']}' does not meet definition of ready")
        
        # Validate sprint capacity
        if not self._validate_sprint_capacity(story):
            raise ValueError(f"Story '{story['title']}' exceeds sprint capacity")
        
        story["status"] = "in_sprint"
        self.user_stories.append(story)
        
        # Add validation check
        self.validation_checks.append({
            "type": "story_added",
            "story_title": story["title"],
            "timestamp": datetime.now(),
            "validation_passed": True
        })
        
        return True
    
    def update_story_status(self, story_title: str, new_status: str, 
                           validation_data: Dict[str, Any] = None):
        """Update story status with continuous validation"""
        for story in self.user_stories:
            if story["title"] == story_title:
                old_status = story["status"]
                story["status"] = new_status
                
                # Validate status transition
                if not self._validate_status_transition(old_status, new_status):
                    raise ValueError(f"Invalid status transition: {old_status} -> {new_status}")
                
                # Add validation check
                self.validation_checks.append({
                    "type": "status_update",
                    "story_title": story_title,
                    "old_status": old_status,
                    "new_status": new_status,
                    "validation_data": validation_data,
                    "timestamp": datetime.now()
                })
                break
    
    def calculate_sprint_progress(self) -> Dict[str, Any]:
        """Calculate sprint completion with comprehensive metrics"""
        completed_stories = [s for s in self.user_stories if s["status"] == "done"]
        in_progress_stories = [s for s in self.user_stories if s["status"] == "in_progress"]
        
        total_story_points = sum(s["story_points"] for s in self.user_stories)
        completed_story_points = sum(s["story_points"] for s in completed_stories)
        
        progress_data = {
            "completion_percentage": len(completed_stories) / len(self.user_stories) * 100,
            "story_points_completed": completed_story_points,
            "story_points_total": total_story_points,
            "velocity_current": completed_story_points / max(1, len(self.daily_progress)),
            "velocity_target": self.velocity_target,
            "stories_completed": len(completed_stories),
            "stories_in_progress": len(in_progress_stories),
            "stories_remaining": len(self.user_stories) - len(completed_stories),
            "validation_checks_passed": len([c for c in self.validation_checks if c.get("validation_passed", False)]),
            "validation_checks_total": len(self.validation_checks)
        }
        
        return progress_data
```

## Implementation Guidelines

### 1. Sprint Planning Process
- **Story Point Estimation**: Use Fibonacci sequence (1, 2, 3, 5, 8, 13, 21) with team consensus
- **Velocity Calculation**: Track completed story points per sprint with historical data
- **Capacity Planning**: Consider team availability, technical debt allocation (20%), and learning time
- **Definition of Ready**: Clear criteria for story readiness with TDD integration
- **Sprint Goal Setting**: Measurable, achievable goals with success metrics

### 2. Daily Standup Process
- **Time Limit**: Maximum 15 minutes with focused discussion
- **Three Questions**: What did you complete? What will you do? Any blockers?
- **Blocker Resolution**: Immediate escalation using systematic problem-solving approach
- **Progress Tracking**: Update burndown chart and velocity metrics daily
- **Error Exposure**: Report all errors and issues immediately (Error Exposure Rule)
- **TDD Status**: Track test-first development compliance

### 3. Sprint Execution
- **Continuous Integration**: Commit and test frequently with automated CI/CD
- **Definition of Done**: Clear completion criteria aligned with quality standards
- **Technical Debt Management**: Allocate 20% capacity for refactoring and maintenance
- **Quality Gates**: Automated testing, code review, and performance validation
- **Continuous Validation**: Validate every step and decision continuously

### 4. Sprint Review and Retrospective
- **Sprint Review**: Demonstrate completed work to stakeholders
- **Sprint Retrospective**: Identify improvements and action items
- **Velocity Analysis**: Track and analyze velocity trends
- **Process Improvement**: Implement improvements in next sprint
- **Knowledge Sharing**: Document learnings and best practices

## Integration with Existing Rules

### TDD Integration
- **Test-First Development**: All user stories must include test scenarios
- **Test Coverage**: Maintain 90%+ test coverage for all completed work
- **Refactoring**: Continuous refactoring with test validation
- **Test Automation**: Automated test execution in CI/CD pipeline

### Error Exposure Integration
- **Immediate Error Reporting**: Report all errors in daily standups
- **No Silent Failures**: Expose all issues immediately
- **Error Tracking**: Track error resolution and prevention
- **Systematic Problem-Solving**: Use systematic approach for blocker resolution

### Continuous Validation Integration
- **Story Validation**: Validate story readiness and completion
- **Process Validation**: Validate sprint processes and metrics
- **Quality Validation**: Validate code quality and performance
- **Progress Validation**: Validate sprint progress and velocity

### Framework Integration
- **LangChain/LangGraph**: Use for automated sprint management
- **Streamlit**: Use for sprint dashboard and metrics visualization
- **Pytest**: Use for automated testing and validation
- **Mermaid**: Use for sprint burndown charts and process diagrams

## Enforcement

This rule is **CONDITIONALLY APPLIED** based on context.

**Violations require immediate sprint adjustment and process improvement.**

## Benefits

- **Structured Agility**: Formal Agile processes that complement TDD approach
- **Quality Assurance**: XP practices that enhance existing quality standards
- **Team Collaboration**: Improved communication and coordination
- **Continuous Improvement**: Regular retrospectives and process optimization
- **Predictable Delivery**: Velocity-based planning and capacity management
- **Risk Mitigation**: Early blocker identification and resolution
- **Stakeholder Satisfaction**: Regular demonstrations and feedback cycles
description: "Auto-generated description"
globs: ["**/*"]
alwaysApply: false
---


