{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0a0972",
   "metadata": {},
   "source": [
    "## Dev MCP Tool Suite v1\n",
    "\n",
    "This notebook is an **instructive reference** for using our development MCP servers:\n",
    "\n",
    "- **Direct tool calls (no LLM)**: sanity-check servers and tool schemas\n",
    "- **LLM + DeepAgents examples (Gemini 2.5 Flash)**: realistic agent workflows that use these MCP tools\n",
    "\n",
    "### Local servers (FastMCP, streamable HTTP)\n",
    "- dev_repo: `http://127.0.0.1:8100/mcp`\n",
    "fastmcp run dev_repo_server.py --transport streamable-http --port 8100\n",
    "\n",
    "- dev_search: `http://127.0.0.1:8101/mcp`\n",
    "fastmcp run dev_search_server.py --transport streamable-http --port 8101\n",
    "\n",
    "- dev_docs: `http://127.0.0.1:8104/mcp`\n",
    "fastmcp run dev_docs_server.py --transport streamable-http --port 8104\n",
    "\n",
    "### Remote server (optional)\n",
    "- github: `https://api.githubcopilot.com/mcp/` (set `GITHUB_TOKEN`)\n",
    "\n",
    "### Gemini (for LLM examples)\n",
    "Set one of:\n",
    "- `GEMINI_API_KEY`\n",
    "- `GOOGLE_API_KEY`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb44114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PROJECT_ROOT=C:\\Users\\pogawal\\WorkFolder\\Documents\\Python\\ai-dev-agent\n",
      "[INFO] sys.executable=c:\\App\\Anaconda\\envs\\langgraph\\python.exe\n",
      "[INFO] GITHUB_TOKEN is set (GitHub MCP enabled)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "\n",
    "def find_project_root(start: Path | None = None) -> Path:\n",
    "    cursor = (start or Path.cwd()).resolve()\n",
    "    for candidate in [cursor, *cursor.parents]:\n",
    "        if (candidate / \"utils\" / \"mcp\" / \"fastmcp\").is_dir():\n",
    "            return candidate\n",
    "    raise RuntimeError(f\"Could not detect project root from {cursor}\")\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "print(f\"[INFO] PROJECT_ROOT={PROJECT_ROOT}\")\n",
    "print(f\"[INFO] sys.executable={os.fspath(Path(os.__file__).resolve()) if False else __import__('sys').executable}\")\n",
    "\n",
    "# Optional remote GitHub MCP server (official)\n",
    "GITHUB_SERVER_CONFIG = None\n",
    "token = os.getenv(\"GITHUB_TOKEN\")\n",
    "if token:\n",
    "    GITHUB_SERVER_CONFIG = {\n",
    "        \"transport\": \"streamable_http\",\n",
    "        \"url\": \"https://api.githubcopilot.com/mcp/\",\n",
    "        \"headers\": {\"Authorization\": f\"Bearer {token}\"},\n",
    "    }\n",
    "    print(\"[INFO] GITHUB_TOKEN is set (GitHub MCP enabled)\")\n",
    "else:\n",
    "    print(\"[INFO] GITHUB_TOKEN not set (GitHub MCP skipped)\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER: Load tools from MCP server with timeout\n",
    "# =============================================================================\n",
    "async def get_tools_for_server(server_name: str, server_config: dict, timeout_sec: float = 10.0):\n",
    "    \"\"\"Load tools from a single MCP server.\n",
    "    \n",
    "    Args:\n",
    "        server_name: Name of the server (for logging)\n",
    "        server_config: Config dict with transport, command/url, etc.\n",
    "        timeout_sec: Timeout for loading tools\n",
    "        \n",
    "    Returns:\n",
    "        List of LangChain tools from the server\n",
    "    \"\"\"\n",
    "    client = MultiServerMCPClient({server_name: server_config})\n",
    "    return await asyncio.wait_for(client.get_tools(), timeout=timeout_sec)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER: Call a tool directly (for testing)\n",
    "# =============================================================================\n",
    "async def call_tool(tools: list, tool_name: str, args: dict, timeout_sec: float = 10.0):\n",
    "    \"\"\"Call a specific tool by name with timeout.\n",
    "    \n",
    "    Args:\n",
    "        tools: List of tools from get_tools_for_server\n",
    "        tool_name: Name of tool to call (e.g., 'file.exists')\n",
    "        args: Arguments to pass to the tool\n",
    "        timeout_sec: Timeout for tool execution\n",
    "        \n",
    "    Returns:\n",
    "        Tool result\n",
    "    \"\"\"\n",
    "    tool = next((t for t in tools if t.name == tool_name), None)\n",
    "    if not tool:\n",
    "        raise ValueError(f\"Tool '{tool_name}' not found. Available: {[t.name for t in tools]}\")\n",
    "    return await asyncio.wait_for(tool.ainvoke(args), timeout=timeout_sec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb93979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] HTTP transport configured (servers must be running separately)\n",
      "  dev_repo: http://127.0.0.1:8100/mcp\n",
      "  dev_search: http://127.0.0.1:8101/mcp\n",
      "  dev_docs: http://127.0.0.1:8104/mcp\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MCP SERVER CONFIGURATIONS - HTTP Transport\n",
    "# =============================================================================\n",
    "# NOTE: STDIO transport doesn't work in Jupyter (no fileno on notebook streams).\n",
    "# Using HTTP transport with servers started separately.\n",
    "#\n",
    "# Start servers in terminal BEFORE running this notebook:\n",
    "#   python utils/mcp/fastmcp/start_notebook_mcp_servers.py\n",
    "# =============================================================================\n",
    "\n",
    "LOCAL_SERVER_CONFIGS = {\n",
    "    \"dev_repo\": {\"transport\": \"streamable_http\", \"url\": \"http://127.0.0.1:8100/mcp\"},\n",
    "    \"dev_search\": {\"transport\": \"streamable_http\", \"url\": \"http://127.0.0.1:8101/mcp\"},\n",
    "    \"dev_docs\": {\"transport\": \"streamable_http\", \"url\": \"http://127.0.0.1:8104/mcp\"},\n",
    "}\n",
    "\n",
    "print('[INFO] HTTP transport configured (servers must be running separately)')\n",
    "for name, config in LOCAL_SERVER_CONFIGS.items():\n",
    "    print(f\"  {name}: {config['url']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779fa067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting dev_repo (pid=15480) on http://127.0.0.1:8100/mcp\n",
      "[INFO] Starting dev_search (pid=12932) on http://127.0.0.1:8101/mcp\n",
      "[INFO] Starting dev_docs (pid=19388) on http://127.0.0.1:8104/mcp\n",
      "[INFO] MCP servers started by this notebook: {'dev_repo': 15480, 'dev_search': 12932, 'dev_docs': 19388}\n",
      "dev_repo\n",
      "  transport: streamable_http\n",
      "  url: http://127.0.0.1:8100/mcp\n",
      "dev_search\n",
      "  transport: streamable_http\n",
      "  url: http://127.0.0.1:8101/mcp\n",
      "dev_docs\n",
      "  transport: streamable_http\n",
      "  url: http://127.0.0.1:8104/mcp\n"
     ]
    }
   ],
   "source": [
    "# --- Start local FastMCP servers required by this notebook (idempotent) ---\n",
    "# This starts ONLY what is not already reachable, and returns process handles\n",
    "# only for servers started by this notebook.\n",
    "from utils.mcp.fastmcp.start_notebook_mcp_servers import start_required_servers\n",
    "\n",
    "# Keep a single global handle so re-running the cell doesn't spawn duplicates.\n",
    "# Note: if you manually stop servers outside the notebook, set this to None and re-run.\n",
    "if \"_MCP_NOTEBOOK_SERVER_PROCS\" not in globals():\n",
    "    _MCP_NOTEBOOK_SERVER_PROCS = None\n",
    "\n",
    "if not _MCP_NOTEBOOK_SERVER_PROCS:\n",
    "    _MCP_NOTEBOOK_SERVER_PROCS = start_required_servers(project_root=PROJECT_ROOT)\n",
    "else:\n",
    "    print(\"[INFO] MCP servers already started by this notebook; skipping start\")\n",
    "\n",
    "if _MCP_NOTEBOOK_SERVER_PROCS:\n",
    "    started = {name: p.pid for name, p in _MCP_NOTEBOOK_SERVER_PROCS.items()}\n",
    "    print(\"[INFO] MCP servers started by this notebook:\", started)\n",
    "else:\n",
    "    print(\"[INFO] No local servers were started (they were already running)\")\n",
    "\n",
    "# --- Connection configs (used below) ---\n",
    "for name, cfg in LOCAL_SERVER_CONFIGS.items():\n",
    "    print(name)\n",
    "    print(\"  transport:\", cfg.get(\"transport\"))\n",
    "    print(\"  url:\", cfg.get(\"url\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e88fde38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mcp.client.streamable_http:Received session ID: c94c684f39ea4e3e9de50bff3f38b722\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-11-25\n",
      "INFO:mcp.client.streamable_http:Received session ID: 66cc39e774224445b126b062b86e5967\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-11-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] dev_repo tools: ['file.list_directory', 'file.read', 'file.search_content', 'file.get_info', 'file.exists']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mcp.client.streamable_http:Received session ID: e9bca59a1f5a48ff9a688821cb2c83e6\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-11-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"success\":true,\"file_path\":\"C:\\\\Users\\\\pogawal\\\\WorkFolder\\\\Documents\\\\Python\\\\ai-dev-agent\\\\docs\\\\agile\\\\sprints\\\\current_sprint.md\",\"exists\":true,\"is_file\":true,\"is_dir\":false,\"is_symlink\":false,\"timestamp\":\"2026-01-12T16:32:39.202578\"}\n",
      "{\"success\":true,\"file_path\":\"C:\\\\Users\\\\pogawal\\\\WorkFolder\\\\Documents\\\\Python\\\\ai-dev-agent\\\\docs\\\\agile\\\\sprints\\\\current_sprint.md\",\"content\":\"# Current Sprint: Sprint 8 - DeepAgents + MCP Tooling Foundation\\n\\n**Sprint Number**: 8  \\n**Sprint Name**: DeepAgents + MCP Agent-Building Sprint  \\n**Duration**: 2 weeks (14 days)  \\n**Start Date**: 2025-12-23  \\n**End Date**: 2026-01-06  \\n**Current Date**: 2025-12-23 (Day 1)  \\n**Status**: **ACTIVE - WEEK 1**\\n\\n---\\n\\n## Sprint Goal\\n\\nBuild a reliable foundation for **DeepAgents-based agents** that use **MCP tools** (local FastMCP servers + remote MCP servers), aligned with the proven patterns in `tests/deep_agents/deep_agents_mcp.ipynb`.\\n\\n---\\n\\n## Quick Status\\n\\n**Current Phase**: Week 1 - MCP Tooling + DeepAgents Baseline  \\n**Sprint Day**: Day 1 of 14  \\n**Points Completed**: 0 / 34 (0%)  \\n**Points In Progress**: 0  \\n**Points Remaining**: 34  \\n\",\"line_count\":25,\"size_bytes\":746,\"encoding\":\"utf-8\",\"timestamp\":\"2026-01-12T16:32:39.300194\"}\n"
     ]
    }
   ],
   "source": [
    "# --- dev_repo: fast smoke calls ---\n",
    "dev_repo_tools = await get_tools_for_server(\"dev_repo\", LOCAL_SERVER_CONFIGS[\"dev_repo\"])\n",
    "print(\"[INFO] dev_repo tools:\", [t.name for t in dev_repo_tools])\n",
    "\n",
    "current_sprint_path = str(PROJECT_ROOT / \"docs/agile/sprints/current_sprint.md\")\n",
    "print(await call_tool(dev_repo_tools, \"file.exists\", {\"file_path\": current_sprint_path}, timeout_sec=8))\n",
    "print(\n",
    "    await call_tool(\n",
    "        dev_repo_tools,\n",
    "        \"file.read\",\n",
    "        {\"file_path\": current_sprint_path, \"start_line\": 1, \"end_line\": 25},\n",
    "        timeout_sec=8,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80008ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mcp.client.streamable_http:Received session ID: c33e11816cea494dbf4fd1fe0c28fc3d\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-11-25\n",
      "INFO:mcp.client.streamable_http:Received session ID: b25f2691000c4938878a16769b5fcb6e\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-11-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] dev_search tools: ['file.search_content']\n",
      "{\"success\":true,\"search_text\":\"US-MCP-002\",\"directory\":\"C:\\\\Users\\\\pogawal\\\\WorkFolder\\\\Documents\\\\Python\\\\ai-dev-agent\\\\docs\\\\agile\\\\sprints\\\\sprint_8\",\"total_matches\":0,\"results\":[],\"truncated\":false,\"timestamp\":\"2026-01-12T16:25:46.727110\"}\n"
     ]
    }
   ],
   "source": [
    "# --- dev_search: fast search call ---\n",
    "dev_search_tools = await get_tools_for_server(\"dev_search\", LOCAL_SERVER_CONFIGS[\"dev_search\"])\n",
    "print(\"[INFO] dev_search tools:\", [t.name for t in dev_search_tools])\n",
    "\n",
    "sprint8_dir = str(PROJECT_ROOT / \"docs/agile/sprints/sprint_8\")\n",
    "print(\n",
    "    await call_tool(\n",
    "        dev_search_tools,\n",
    "        \"file.search_content\",\n",
    "        {\"query\": \"US-MCP-002\", \"directory\": sprint8_dir},\n",
    "        timeout_sec=12,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2966972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mcp.client.streamable_http:Received session ID: adec8982d3cf499d9538d8daf8b8be02\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-11-25\n",
      "INFO:mcp.client.streamable_http:Received session ID: 2c8a127268de4bccb5f2e8e21a2275c3\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-11-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] dev_docs tools: ['link.scan_all', 'link.validate', 'link.generate_report', 'link.heal']\n",
      "{\"success\":true,\"data\":{\"valid_links\":[],\"broken_links\":[],\"external_links\":[],\"suspicious_links\":[],\"summary\":{\"total_links\":0,\"valid_count\":0,\"broken_count\":0,\"external_count\":0,\"validation_rate\":0.0}},\"message\":\"Validated 0 links: 0 valid, 0 broken\"}\n"
     ]
    }
   ],
   "source": [
    "# --- dev_docs: keep this scoped (full docs/agile can be slow) ---\n",
    "dev_docs_tools = await get_tools_for_server(\"dev_docs\", LOCAL_SERVER_CONFIGS[\"dev_docs\"])\n",
    "print(\"[INFO] dev_docs tools:\", [t.name for t in dev_docs_tools])\n",
    "\n",
    "sprint8_dir = str(PROJECT_ROOT / \"docs/agile/sprints/sprint_8\")\n",
    "print(await call_tool(dev_docs_tools, \"link.validate\", {\"target_directory\": sprint8_dir}, timeout_sec=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c859611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mcp.client.streamable_http:Received session ID: 9921607f-67d5-4e84-bbf2-0204db3c2041\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-11-25\n",
      "INFO:mcp.client.streamable_http:GET stream disconnected, reconnecting in 1000ms...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] github tool count: 37\n",
      "- add_comment_to_pending_review\n",
      "- add_issue_comment\n",
      "- assign_copilot_to_issue\n",
      "- create_branch\n",
      "- create_or_update_file\n",
      "- create_pull_request\n",
      "- create_repository\n",
      "- delete_file\n",
      "- fork_repository\n",
      "- get_commit\n",
      "- get_file_contents\n",
      "- get_label\n",
      "- get_latest_release\n",
      "- get_me\n",
      "- get_release_by_tag\n",
      "- get_tag\n",
      "- issue_read\n",
      "- issue_write\n",
      "- list_branches\n",
      "- list_commits\n",
      "- list_issues\n",
      "- list_pull_requests\n",
      "- list_releases\n",
      "- list_tags\n",
      "- merge_pull_request\n",
      "- pull_request_read\n",
      "- pull_request_review_write\n",
      "- push_files\n",
      "- request_copilot_review\n",
      "- search_code\n"
     ]
    }
   ],
   "source": [
    "# --- github (remote): list tools only (fast) ---\n",
    "if GITHUB_SERVER_CONFIG is None:\n",
    "    print(\"[SKIP] GitHub MCP (set GITHUB_TOKEN to enable)\")\n",
    "else:\n",
    "    github_tools = await get_tools_for_server(\"github\", GITHUB_SERVER_CONFIG)\n",
    "    print(f\"[INFO] github tool count: {len(github_tools)}\")\n",
    "    for t in github_tools[:30]:\n",
    "        print(\"-\", t.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c52115a8c",
   "metadata": {},
   "source": [
    "### GitHub MCP (remote) smoke test\n",
    "\n",
    "This is a minimal end-to-end test of the remote GitHub MCP server.\n",
    "\n",
    "- Requires `GITHUB_TOKEN` (otherwise the test is skipped).\n",
    "- Verifies a few expected tool names exist.\n",
    "- Calls `get_me` (read-only) and asserts the response is non-empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20b17afa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def _parse_maybe_json(value):\n",
    "    \"\"\"Parse JSON if the MCP tool returned a JSON string; otherwise return as-is.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return json.loads(value)\n",
    "        except Exception:\n",
    "            return value\n",
    "    return value\n",
    "\n",
    "\n",
    "if GITHUB_SERVER_CONFIG is None:\n",
    "    print(\"[SKIP] GitHub MCP smoke test (set GITHUB_TOKEN to enable)\")\n",
    "else:\n",
    "    github_tools = await get_tools_for_server(\"github\", GITHUB_SERVER_CONFIG)\n",
    "    tool_names = {t.name for t in github_tools}\n",
    "\n",
    "    # Keep this small and stable: only verify a few read-only primitives exist\n",
    "    required = {\"get_me\", \"search_code\", \"get_file_contents\"}\n",
    "    missing = sorted(required - tool_names)\n",
    "    assert not missing, (\n",
    "        f\"Missing expected GitHub MCP tools: {missing}. \"\n",
    "        f\"Tool count={len(tool_names)}\"\n",
    "    )\n",
    "\n",
    "    res = await call_tool(github_tools, \"get_me\", {}, timeout_sec=20)\n",
    "    if \"error\" in res:\n",
    "        raise RuntimeError(res[\"error\"])\n",
    "\n",
    "    payload = _parse_maybe_json(res.get(\"result\"))\n",
    "    assert payload, f\"Empty get_me response: {payload!r}\"\n",
    "\n",
    "    print(\"[INFO] get_me response type:\", type(payload).__name__)\n",
    "    if isinstance(payload, dict):\n",
    "        print(\"[INFO] get_me keys (sample):\", sorted(payload.keys())[:30])\n",
    "    else:\n",
    "        print(\"[INFO] get_me (truncated):\", str(payload)[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deepmcpagent-intro",
   "metadata": {},
   "source": [
    "## LangGraph MCP Agent - Our LangChain 1.x Implementation\n",
    "\n",
    "This section uses our custom `utils/mcp/langgraph_mcp_agent.py` module which:\n",
    "\n",
    "1. **Discovers tools** from FastMCP servers via HTTP/SSE\n",
    "2. **Converts JSON Schema** to Pydantic models for LangChain compatibility\n",
    "3. **Creates fresh connections** for each tool call (avoids stale connection issues)\n",
    "4. **Builds a ReAct agent** using LangGraph 1.x API\n",
    "\n",
    "This is inspired by DeepMCPAgent but rewritten to be compatible with LangChain 1.x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deepmcpagent-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai._api_client:Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n",
      "INFO:mcp.client.streamable_http:Received session ID: 1de8231fccec4fca832854f713e46f7a\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-11-25\n",
      "INFO:mcp.client.streamable_http:Received session ID: ad336c33f14d4126b246b7a50e12e7b4\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-11-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] LLM ready: gemini-2.5-flash-lite\n",
      "[1] Building MCP Agent with tools from dev_repo server...\n",
      "[2] Discovered MCP tools:\n",
      "    - file.list_directory: \n",
      "    - file.read: \n",
      "    - file.search_content: \n",
      "    - file.get_info: \n",
      "    - file.exists: \n",
      "[3] Testing: Does C:\\Users\\pogawal\\WorkFolder\\Documents\\Python\\ai-dev-agent\\README.md exist?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\App\\Anaconda\\envs\\langgraph\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2748: DeprecationWarning: The 'convert_system_message_to_human' parameter is deprecated and will be removed in a future version. Use system instructions instead.\n",
      "  system_instruction, history = _parse_chat_history(\n",
      "c:\\App\\Anaconda\\envs\\langgraph\\Lib\\site-packages\\google\\genai\\_api_client.py:744: DeprecationWarning: Inheritance class AiohttpClientSession from ClientSession is discouraged\n",
      "  class AiohttpClientSession(aiohttp.ClientSession):  # type: ignore[misc]\n",
      "INFO:mcp.client.streamable_http:Received session ID: 6598f60abb5f4fcdae81e5ea2eac1e28\n",
      "INFO:mcp.client.streamable_http:Negotiated protocol version: 2025-11-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TOOL] Invoking: file.exists with {'file_path': 'C:\\\\Users\\\\pogawal\\\\WorkFolder\\\\Documents\\\\Python\\\\ai-dev-agent\\\\README.md'}\n",
      "[TOOL] Result from file.exists: {'success': True, 'file_path': 'C:\\\\Users\\\\pogawal\\\\WorkFolder\\\\Documents\\\\Python\\\\ai-dev-agent\\\\README.md', 'exists': True, 'is_file': True, 'is_dir': False, 'is_symlink': False, 'timestamp': '2026-01-12T16:33:33.545720'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\App\\Anaconda\\envs\\langgraph\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2748: DeprecationWarning: The 'convert_system_message_to_human' parameter is deprecated and will be removed in a future version. Use system instructions instead.\n",
      "  system_instruction, history = _parse_chat_history(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] Agent response:\n",
      "    human: Check if this file exists: C:\\Users\\pogawal\\WorkFolder\\Documents\\Python\\ai-dev-agent\\README.md\n",
      "    tool: CallToolResult(content=[TextContent(type='text', text='{\"success\":true,\"file_path\":\"C:\\\\\\\\Users\\\\\\\\pogawal\\\\\\\\WorkFolder\\\\\\\\Documents\\\\\\\\Python\\\\\\\\ai-dev-agent\\\\\\\\README.md\",\"exists\":true,\"is_file\":true,\"is_dir\":false,\"is_symlink\":false,\"timestamp\":\"2026-01-12T16:33:33.545720\"}', annotations=None, meta=None)], structured_content={'success': True, 'file_path': 'C:\\\\Users\\\\pogawal\\\\WorkFolder\\\\Documents\\\\Python\\\\ai-dev-agent\\\\README.md', 'exists': True, 'is_file': True, 'is_dir': False, 'is_symlin\n",
      "    ai: The file C:\\Users\\pogawal\\WorkFolder\\Documents\\Python\\ai-dev-agent\\README.md exists.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LangGraph MCP Agent - Using Our LangChain 1.x Compatible Implementation\n",
    "# =============================================================================\n",
    "# This uses utils/mcp/langgraph_mcp_agent.py which is compatible with\n",
    "# LangChain 1.x and creates fresh connections for each tool call.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, str(PROJECT_ROOT))  # Ensure utils is importable\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from utils.mcp.langgraph_mcp_agent import build_mcp_agent, MCPServerConfig\n",
    "\n",
    "# Setup LLM (Gemini 2.5 Flash, temperature=0 for determinism)\n",
    "api_key = os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError('GEMINI_API_KEY or GOOGLE_API_KEY not set')\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True,\n",
    "    google_api_key=api_key,\n",
    ")\n",
    "print(f'[INFO] LLM ready: {llm.model}')\n",
    "\n",
    "# Configure MCP servers using our MCPServerConfig\n",
    "mcp_servers = {\n",
    "    \"dev_repo\": MCPServerConfig(\n",
    "        url=\"http://127.0.0.1:8100/mcp\",\n",
    "        transport=\"streamable-http\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "print('[1] Building MCP Agent with tools from dev_repo server...')\n",
    "graph, loader = await build_mcp_agent(\n",
    "    servers=mcp_servers,\n",
    "    model=llm,\n",
    "    instructions=\"You are a file inspector. Use tools to check if files exist.\",\n",
    "    trace_tools=True,  # Show tool invocations\n",
    ")\n",
    "\n",
    "# Show discovered tools\n",
    "print('[2] Discovered MCP tools:')\n",
    "tool_infos = await loader.list_tool_info()\n",
    "for info in tool_infos:\n",
    "    desc = info.description[:60] + '...' if len(info.description) > 60 else info.description\n",
    "    print(f'    - {info.name}: {desc}')\n",
    "\n",
    "# Test with file.exists on README.md\n",
    "test_file = str(PROJECT_ROOT / \"README.md\")\n",
    "print(f'[3] Testing: Does {test_file} exist?')\n",
    "\n",
    "result = await graph.ainvoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": f\"Check if this file exists: {test_file}\"}]\n",
    "})\n",
    "\n",
    "print('[4] Agent response:')\n",
    "for msg in result.get('messages', []):\n",
    "    if hasattr(msg, 'content') and msg.content:\n",
    "        print(f\"    {msg.type}: {msg.content[:500]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c6904",
   "metadata": {},
   "source": [
    "### GitHub MCP repo showcase: `gitwalter/ai-dev-agent`\n",
    "\n",
    "This section illustrates **read-only** GitHub MCP operations against the repository at `https://github.com/gitwalter/ai-dev-agent`:\n",
    "\n",
    "- Fetch and print `README.md` (first characters)\n",
    "- List branches\n",
    "- List recent commits (small sample)\n",
    "\n",
    "If `GITHUB_TOKEN` is not set, this section is skipped.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a6564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "REPO_OWNER = \"gitwalter\"\n",
    "REPO_NAME = \"ai-dev-agent\"\n",
    "REPO_FULL_NAME = f\"{REPO_OWNER}/{REPO_NAME}\"\n",
    "DEFAULT_REF = \"main\"\n",
    "\n",
    "\n",
    "def _parse_maybe_json(value: Any) -> Any:\n",
    "    \"\"\"Parse JSON if the MCP tool returned a JSON string; otherwise return as-is.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return json.loads(value)\n",
    "        except Exception:\n",
    "            return value\n",
    "    return value\n",
    "\n",
    "\n",
    "def _get_tool(tools: list, tool_name: str):\n",
    "    \"\"\"Find a tool object by name or raise with a clear error.\"\"\"\n",
    "    tool = next((t for t in tools if t.name == tool_name), None)\n",
    "    if tool is None:\n",
    "        raise KeyError(f\"Tool not found: {tool_name}. Available: {[t.name for t in tools]}\")\n",
    "    return tool\n",
    "\n",
    "\n",
    "def _schema_field_names(tool: Any) -> set[str] | None:\n",
    "    \"\"\"Best-effort extraction of input field names from a LangChain tool args schema.\"\"\"\n",
    "    schema = getattr(tool, \"args_schema\", None)\n",
    "    if schema is None:\n",
    "        return None\n",
    "\n",
    "    # Pydantic v2\n",
    "    model_fields = getattr(schema, \"model_fields\", None)\n",
    "    if isinstance(model_fields, dict):\n",
    "        return set(model_fields.keys())\n",
    "\n",
    "    # Pydantic v1\n",
    "    fields = getattr(schema, \"__fields__\", None)\n",
    "    if isinstance(fields, dict):\n",
    "        return set(fields.keys())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _repo_args_for(tool_name: str) -> dict[str, Any]:\n",
    "    \"\"\"Build repo-identifying args for a given tool based on its schema.\"\"\"\n",
    "    tool = _get_tool(github_tools, tool_name)\n",
    "    field_names = _schema_field_names(tool)\n",
    "\n",
    "    if field_names is None:\n",
    "        # Fall back to the most common GitHub tool signature.\n",
    "        # If this is wrong, the MCP server should return a clear validation error.\n",
    "        return {\"owner\": REPO_OWNER, \"repo\": REPO_NAME}\n",
    "\n",
    "    if \"owner\" in field_names and \"repo\" in field_names:\n",
    "        return {\"owner\": REPO_OWNER, \"repo\": REPO_NAME}\n",
    "\n",
    "    # Some tools accept a single full-name field.\n",
    "    for full_name_field in (\"repository\", \"repo_full_name\", \"full_name\"):\n",
    "        if full_name_field in field_names:\n",
    "            return {full_name_field: REPO_FULL_NAME}\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"Cannot infer repo-identifying args for tool '{tool_name}'. \"\n",
    "        f\"Fields={sorted(field_names)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _file_args_for(tool_name: str, *, path: str, ref: str | None) -> dict[str, Any]:\n",
    "    \"\"\"Build args for tools that read a file from a repo.\"\"\"\n",
    "    tool = _get_tool(github_tools, tool_name)\n",
    "    field_names = _schema_field_names(tool)\n",
    "\n",
    "    args = _repo_args_for(tool_name)\n",
    "\n",
    "    # File path\n",
    "    if field_names is None:\n",
    "        args[\"path\"] = path\n",
    "    else:\n",
    "        if \"path\" in field_names:\n",
    "            args[\"path\"] = path\n",
    "        elif \"file_path\" in field_names:\n",
    "            args[\"file_path\"] = path\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"Cannot infer file path arg for tool '{tool_name}'. Fields={sorted(field_names)}\"\n",
    "            )\n",
    "\n",
    "    # Optional ref/branch\n",
    "    if ref is not None and field_names is not None:\n",
    "        for ref_field in (\"ref\", \"branch\"):\n",
    "            if ref_field in field_names:\n",
    "                args[ref_field] = ref\n",
    "                break\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def _limit_args_for(tool_name: str, *, limit: int) -> dict[str, Any]:\n",
    "    \"\"\"Add an optional limit/per_page argument if the tool schema supports it.\"\"\"\n",
    "    tool = _get_tool(github_tools, tool_name)\n",
    "    field_names = _schema_field_names(tool)\n",
    "\n",
    "    args = _repo_args_for(tool_name)\n",
    "\n",
    "    if field_names is None:\n",
    "        # If we can't see schema, don't guess; just let the server default.\n",
    "        return args\n",
    "\n",
    "    for limit_field in (\"per_page\", \"limit\", \"max_results\"):\n",
    "        if limit_field in field_names:\n",
    "            args[limit_field] = limit\n",
    "            break\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def _decode_readme_payload(payload: Any) -> str:\n",
    "    \"\"\"Decode common GitHub content payload shapes deterministically.\"\"\"\n",
    "    if isinstance(payload, str):\n",
    "        return payload\n",
    "\n",
    "    if isinstance(payload, dict):\n",
    "        # Common REST-style content response shape.\n",
    "        content = payload.get(\"content\")\n",
    "        encoding = payload.get(\"encoding\")\n",
    "        if isinstance(content, str) and encoding == \"base64\":\n",
    "            raw = base64.b64decode(content)\n",
    "            return raw.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "        # Some servers inline plaintext\n",
    "        if isinstance(content, str) and encoding in (None, \"utf-8\", \"text\"):\n",
    "            return content\n",
    "\n",
    "        # Fallback: pretty print structured payload\n",
    "        return json.dumps(payload, indent=2)[:2000]\n",
    "\n",
    "    return str(payload)\n",
    "\n",
    "\n",
    "if GITHUB_SERVER_CONFIG is None:\n",
    "    print(\"[SKIP] GitHub MCP repo showcase (set GITHUB_TOKEN to enable)\")\n",
    "else:\n",
    "    github_tools = await get_tools_for_server(\"github\", GITHUB_SERVER_CONFIG)\n",
    "    tool_names = {t.name for t in github_tools}\n",
    "\n",
    "    required_tools = {\"get_file_contents\", \"list_branches\", \"list_commits\"}\n",
    "    missing_tools = sorted(required_tools - tool_names)\n",
    "    assert not missing_tools, f\"Missing required GitHub MCP tools: {missing_tools}\"\n",
    "\n",
    "    # 1) README.md\n",
    "    readme_call = await call_tool(\n",
    "        github_tools,\n",
    "        \"get_file_contents\",\n",
    "        _file_args_for(\"get_file_contents\", path=\"README.md\", ref=DEFAULT_REF),\n",
    "        timeout_sec=30,\n",
    "    )\n",
    "    if \"error\" in readme_call:\n",
    "        raise RuntimeError(readme_call[\"error\"])\n",
    "\n",
    "    readme_payload = _parse_maybe_json(readme_call.get(\"result\"))\n",
    "    readme_text = _decode_readme_payload(readme_payload)\n",
    "    print(f\"[INFO] {REPO_FULL_NAME} README.md (first 400 chars):\")\n",
    "    print(readme_text[:400])\n",
    "\n",
    "    # 2) Branches\n",
    "    branches_call = await call_tool(\n",
    "        github_tools,\n",
    "        \"list_branches\",\n",
    "        _repo_args_for(\"list_branches\"),\n",
    "        timeout_sec=30,\n",
    "    )\n",
    "    if \"error\" in branches_call:\n",
    "        raise RuntimeError(branches_call[\"error\"])\n",
    "\n",
    "    branches_payload = _parse_maybe_json(branches_call.get(\"result\"))\n",
    "    if isinstance(branches_payload, list):\n",
    "        branch_names = []\n",
    "        for b in branches_payload:\n",
    "            if isinstance(b, dict) and isinstance(b.get(\"name\"), str):\n",
    "                branch_names.append(b[\"name\"])\n",
    "            elif isinstance(b, str):\n",
    "                branch_names.append(b)\n",
    "        print(\"[INFO] Branches (sample):\", branch_names[:10])\n",
    "    else:\n",
    "        print(\"[INFO] Branches payload (truncated):\", str(branches_payload)[:400])\n",
    "\n",
    "    # 3) Recent commits (small sample)\n",
    "    commits_call = await call_tool(\n",
    "        github_tools,\n",
    "        \"list_commits\",\n",
    "        _limit_args_for(\"list_commits\", limit=5),\n",
    "        timeout_sec=30,\n",
    "    )\n",
    "    if \"error\" in commits_call:\n",
    "        raise RuntimeError(commits_call[\"error\"])\n",
    "\n",
    "    commits_payload = _parse_maybe_json(commits_call.get(\"result\"))\n",
    "    if isinstance(commits_payload, list) and commits_payload:\n",
    "        first = commits_payload[0]\n",
    "        if isinstance(first, dict):\n",
    "            sha = first.get(\"sha\") or first.get(\"id\")\n",
    "            msg = None\n",
    "            commit_obj = first.get(\"commit\")\n",
    "            if isinstance(commit_obj, dict):\n",
    "                message_obj = commit_obj.get(\"message\")\n",
    "                if isinstance(message_obj, str):\n",
    "                    msg = message_obj\n",
    "            if msg is None and isinstance(first.get(\"message\"), str):\n",
    "                msg = first.get(\"message\")\n",
    "\n",
    "            print(\"[INFO] Latest commit (best-effort):\")\n",
    "            print(\"  sha:\", sha)\n",
    "            if msg is not None:\n",
    "                print(\"  message:\", msg.splitlines()[0][:200])\n",
    "        else:\n",
    "            print(\"[INFO] Commits payload (first item):\", str(first)[:400])\n",
    "    else:\n",
    "        print(\"[INFO] Commits payload (truncated):\", str(commits_payload)[:400])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ac561",
   "metadata": {},
   "source": [
    "### GitHub MCP: `search_code` example\n",
    "\n",
    "This cell demonstrates `search_code` against `gitwalter/ai-dev-agent`.\n",
    "\n",
    "Important: GitHub code search is scoped to a single repo via the query qualifier `repo:OWNER/REPO`.\n",
    "This example always includes `repo:gitwalter/ai-dev-agent` in the search query.\n",
    "\n",
    "- Uses **read-only** GitHub MCP tooling\n",
    "- Uses schema introspection to choose the right argument names\n",
    "- Prints a small, stable sample (first few matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80215c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "# Keep the query small and stable. This is intended as a reproducible example.\n",
    "# IMPORTANT: GitHub code search is scoped via the query string qualifier:\n",
    "#   repo:OWNER/REPO\n",
    "# Without that qualifier, GitHub searches across all repos you have access to.\n",
    "REPO_FULL_NAME = \"gitwalter/ai-dev-agent\"\n",
    "SEARCH_TEXT = \"MultiServerMCPClient\"\n",
    "SEARCH_QUERY = f\"repo:{REPO_FULL_NAME} {SEARCH_TEXT}\"\n",
    "MAX_MATCHES_TO_PRINT = 10\n",
    "\n",
    "\n",
    "def _parse_maybe_json(value: Any) -> Any:\n",
    "    \"\"\"Parse JSON if the MCP tool returned a JSON string; otherwise return as-is.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            return json.loads(value)\n",
    "        except Exception:\n",
    "            return value\n",
    "    return value\n",
    "\n",
    "\n",
    "def _get_tool(tools: list, tool_name: str):\n",
    "    \"\"\"Find a tool object by name or raise with a clear error.\"\"\"\n",
    "    tool = next((t for t in tools if t.name == tool_name), None)\n",
    "    if tool is None:\n",
    "        raise KeyError(f\"Tool not found: {tool_name}. Available: {[t.name for t in tools]}\")\n",
    "    return tool\n",
    "\n",
    "\n",
    "def _schema_field_names(tool: Any) -> set[str] | None:\n",
    "    \"\"\"Best-effort extraction of input field names from a LangChain tool args schema.\"\"\"\n",
    "    schema = getattr(tool, \"args_schema\", None)\n",
    "    if schema is None:\n",
    "        return None\n",
    "\n",
    "    model_fields = getattr(schema, \"model_fields\", None)\n",
    "    if isinstance(model_fields, dict):\n",
    "        return set(model_fields.keys())\n",
    "\n",
    "    fields = getattr(schema, \"__fields__\", None)\n",
    "    if isinstance(fields, dict):\n",
    "        return set(fields.keys())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _build_search_code_args(tool: Any) -> dict[str, Any]:\n",
    "    \"\"\"Build args for `search_code` based on schema fields.\"\"\"\n",
    "    fields = _schema_field_names(tool)\n",
    "\n",
    "    # Fallback to common naming; MCP server should raise a clear validation error if wrong.\n",
    "    if fields is None:\n",
    "        return {\n",
    "            \"query\": SEARCH_QUERY,\n",
    "            \"owner\": \"gitwalter\",\n",
    "            \"repo\": \"ai-dev-agent\",\n",
    "        }\n",
    "\n",
    "    args: dict[str, Any] = {}\n",
    "\n",
    "    # Query field\n",
    "    if \"query\" in fields:\n",
    "        args[\"query\"] = SEARCH_QUERY\n",
    "    elif \"search_query\" in fields:\n",
    "        args[\"search_query\"] = SEARCH_QUERY\n",
    "    else:\n",
    "        raise RuntimeError(f\"Cannot infer query field for search_code. Fields={sorted(fields)}\")\n",
    "\n",
    "    # Repo identification\n",
    "    if \"owner\" in fields and \"repo\" in fields:\n",
    "        args[\"owner\"] = \"gitwalter\"\n",
    "        args[\"repo\"] = \"ai-dev-agent\"\n",
    "    else:\n",
    "        for full_name_field in (\"repository\", \"repo_full_name\", \"full_name\"):\n",
    "            if full_name_field in fields:\n",
    "                args[full_name_field] = \"gitwalter/ai-dev-agent\"\n",
    "                break\n",
    "\n",
    "    # Optional limits\n",
    "    for limit_field in (\"per_page\", \"limit\", \"max_results\"):\n",
    "        if limit_field in fields:\n",
    "            args[limit_field] = 20\n",
    "            break\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "if GITHUB_SERVER_CONFIG is None:\n",
    "    print(\"[SKIP] GitHub MCP search_code example (set GITHUB_TOKEN to enable)\")\n",
    "else:\n",
    "    github_tools = await get_tools_for_server(\"github\", GITHUB_SERVER_CONFIG)\n",
    "\n",
    "    search_tool = _get_tool(github_tools, \"search_code\")\n",
    "    args = _build_search_code_args(search_tool)\n",
    "\n",
    "    res = await call_tool(github_tools, \"search_code\", args, timeout_sec=30)\n",
    "    if \"error\" in res:\n",
    "        raise RuntimeError(res[\"error\"])\n",
    "\n",
    "    payload = _parse_maybe_json(res.get(\"result\"))\n",
    "\n",
    "    # Print a small best-effort view of the results.\n",
    "    if isinstance(payload, dict):\n",
    "        items = payload.get(\"items\")\n",
    "        if isinstance(items, list):\n",
    "            print(f\"[INFO] search_code matches (showing up to {MAX_MATCHES_TO_PRINT}):\")\n",
    "            for item in items[:MAX_MATCHES_TO_PRINT]:\n",
    "                if isinstance(item, dict):\n",
    "                    path = item.get(\"path\") or item.get(\"file\") or item.get(\"name\")\n",
    "                    repo = item.get(\"repository\")\n",
    "                    score = item.get(\"score\")\n",
    "                    print(\"-\", {\"path\": path, \"repository\": repo, \"score\": score})\n",
    "                else:\n",
    "                    print(\"-\", str(item)[:200])\n",
    "        else:\n",
    "            print(\"[INFO] search_code result keys:\", sorted(payload.keys())[:50])\n",
    "            print(\"[INFO] search_code payload (truncated):\", json.dumps(payload, indent=2)[:1000])\n",
    "    elif isinstance(payload, list):\n",
    "        print(f\"[INFO] search_code returned a list (showing up to {MAX_MATCHES_TO_PRINT}):\")\n",
    "        for item in payload[:MAX_MATCHES_TO_PRINT]:\n",
    "            print(\"-\", str(item)[:200])\n",
    "    else:\n",
    "        print(\"[INFO] search_code payload (truncated):\", str(payload)[:1000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b2f874",
   "metadata": {},
   "source": [
    "## LLM + DeepAgents examples (Gemini 2.5 Flash)\n",
    "\n",
    "These examples show how to use the MCP tools through a **tool-calling LLM** and **DeepAgents**.\n",
    "\n",
    "Notes:\n",
    "- These examples are designed to be deterministic: **temperature=0**.\n",
    "- All examples are **read-only by default**. The only write-capable tool here is `link.heal`, which is demonstrated as a template and is not executed automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0184908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Gemini configuration (must match project standards)\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\n",
    "        \"Missing Gemini API key. Set GEMINI_API_KEY or GOOGLE_API_KEY in the environment.\"\n",
    "    )\n",
    "\n",
    "# IMPORTANT: temperature=0 and convert_system_message_to_human=True for Gemini compatibility.\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True,\n",
    ")\n",
    "\n",
    "print(\"[INFO] LLM ready:\", getattr(llm, \"model\", \"gemini-2.5-flash\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f55b80b",
   "metadata": {},
   "source": [
    "### Load MCP tools (for agents)\n",
    "\n",
    "This loads tools from our dev servers and groups them per server, so we can build focused subagents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5bee2",
   "metadata": {},
   "source": [
    "### Build DeepAgents subagents (focused toolsets)\n",
    "\n",
    "We create small, focused subagents instead of one giant agent:\n",
    "\n",
    "- **repo_agent**: local repo inspection (`dev_repo` tools)\n",
    "- **search_agent**: fast local search (`dev_search` tools)\n",
    "- **docs_agent**: link validation/reporting (`dev_docs` tools)\n",
    "- **github_agent**: GitHub repo inspection and code search (GitHub MCP tools)\n",
    "\n",
    "Then we create a **coordinator** that can delegate to the right specialist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "try:\n",
    "    from deepagents import create_deep_agent\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"deepagents is required for this section. Install it in your environment. \"\n",
    "        f\"Import error: {type(e).__name__}: {e}\"\n",
    "    )\n",
    "\n",
    "print(\"[INFO] DeepAgents imported successfully\")\n",
    "print(\"[INFO] LLM and server configs already set up in previous cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-test-intro",
   "metadata": {},
   "source": [
    "### Simple LLM Tool Test with ACTUAL MCP Tools\n",
    "\n",
    "This test demonstrates using the **actual MCP tools** (`file.exists`) directly with an LLM agent.\n",
    "\n",
    "**Key insight**: MCP tools must be loaded fresh just before agent invocation to avoid stale HTTP connection state.\n",
    "\n",
    "The pattern is:\n",
    "1. Load tools fresh from MCP server\n",
    "2. Create agent with those tools immediately\n",
    "3. Invoke agent immediately while connections are active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-mcp-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMPLE LLM TOOL TEST - Using ACTUAL MCP tools via STDIO transport\n",
    "# =============================================================================\n",
    "# STDIO transport spawns servers as subprocesses - much more reliable!\n",
    "# No HTTP connection issues, no stale state.\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Create client with STDIO transport\n",
    "print(\"[1] Creating MCP client with STDIO transport...\")\n",
    "client = MultiServerMCPClient({\n",
    "    \"dev_repo\": LOCAL_SERVER_CONFIGS[\"dev_repo\"]\n",
    "})\n",
    "\n",
    "# Get the actual MCP tools\n",
    "print(\"[2] Loading MCP tools...\")\n",
    "repo_tools = await client.get_tools()\n",
    "print(f\"    Loaded {len(repo_tools)} tools: {[t.name for t in repo_tools]}\")\n",
    "\n",
    "# Create agent with ACTUAL MCP tools (no wrappers!)\n",
    "print(\"[3] Creating agent with actual MCP tools...\")\n",
    "simple_agent = create_deep_agent(\n",
    "    model=llm,\n",
    "    tools=repo_tools,  # These are the ACTUAL MCP tools!\n",
    "    system_prompt=\"You are a file inspector. Use the file.exists tool when asked about files.\"\n",
    ")\n",
    "\n",
    "# Test with a known file\n",
    "test_file = str(PROJECT_ROOT / \"README.md\")\n",
    "print(f\"[4] Testing with file: {test_file}\")\n",
    "\n",
    "# Invoke the agent - tools should work!\n",
    "print(\"[5] Invoking agent...\")\n",
    "response = await simple_agent.ainvoke({\n",
    "    \"messages\": [HumanMessage(content=f\"Check if this file exists: {test_file}\")]\n",
    "})\n",
    "\n",
    "# Display result\n",
    "print(\"[6] Agent response:\")\n",
    "if isinstance(response, dict) and 'messages' in response:\n",
    "    for msg in response['messages']:\n",
    "        if hasattr(msg, 'content'):\n",
    "            print(f\"  {msg.type}: {msg.content[:500] if len(msg.content) > 500 else msg.content}\")\n",
    "else:\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7747b310",
   "metadata": {},
   "source": [
    "### LLM examples: `dev_repo` tools (one example per tool)\n",
    "\n",
    "These prompts are written to encourage the agent to call a specific tool.\n",
    "\n",
    "Tools covered:\n",
    "- `file.exists`\n",
    "- `file.get_info`\n",
    "- `file.list_directory`\n",
    "- `file.read`\n",
    "- `file.search_content`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222cf95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def _as_text(value: object) -> str:\n",
    "    \"\"\"Best-effort conversion of a model message content to text.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return value\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "async def ask(agent_obj: Any, prompt: str) -> str:\n",
    "    \"\"\"Ask a DeepAgent and return the last message content as text.\"\"\"\n",
    "    res = await agent_obj.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": prompt}]})\n",
    "    return _as_text(res[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "async def run_examples(title: str, prompts: Iterable[str], agent_obj: Any) -> None:\n",
    "    \"\"\"Run a list of prompts against an agent and print compact results.\"\"\"\n",
    "    print(f\"\\n=== {title} ===\\n\")\n",
    "    for i, p in enumerate(prompts, 1):\n",
    "        print(f\"[{i}] Prompt:\\n{p}\\n\")\n",
    "        out = await ask(agent_obj, p)\n",
    "        # Keep output readable in notebooks\n",
    "        print(f\"[{i}] Response (truncated):\\n{out[:1200]}\\n\")\n",
    "\n",
    "\n",
    "current_sprint_path = str(PROJECT_ROOT / \"docs/agile/sprints/current_sprint.md\")\n",
    "\n",
    "repo_prompts = [\n",
    "    # file.exists\n",
    "    \"Call tool file.exists with file_path='\" + current_sprint_path + \"'. Return only the tool result.\",\n",
    "    # file.get_info\n",
    "    \"Call tool file.get_info with file_path='README.md'. Return only the tool result.\",\n",
    "    # file.list_directory\n",
    "    \"Call tool file.list_directory with directory='docs/agile/sprints', pattern='*.md', recursive=False. Return only the tool result.\",\n",
    "    # file.read\n",
    "    \"Call tool file.read with file_path='docs/agile/sprints/current_sprint.md', start_line=1, end_line=20. Return only the tool result.\",\n",
    "    # file.search_content\n",
    "    \"Call tool file.search_content with query='US-MCP-002', directory='docs/agile/sprints', file_pattern='*.md', max_results=10. Return only the tool result.\",\n",
    "]\n",
    "\n",
    "await run_examples(\"dev_repo examples\", repo_prompts, repo_agent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff05ae1c",
   "metadata": {},
   "source": [
    "### LLM examples: `dev_search` tool\n",
    "\n",
    "Tool covered:\n",
    "- `file.search_content`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9f6f0",
   "metadata": {},
   "source": [
    "### LLM examples: `dev_docs` link tools (one example per tool)\n",
    "\n",
    "Tools covered:\n",
    "- `link.validate`\n",
    "- `link.scan_all` (expensive; disabled by default in the example)\n",
    "- `link.generate_report`\n",
    "- `link.heal` (write operation; shown as a template only)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91644c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_search: file.search_content via LLM\n",
    "search_prompts = [\n",
    "    \"Call tool file.search_content with query='search_content_mcp', directory='utils/mcp', file_pattern='*.py', max_results=10. Return only the tool result.\",\n",
    "]\n",
    "await run_examples(\"dev_search examples\", search_prompts, search_agent)\n",
    "\n",
    "\n",
    "# dev_docs: link tools via LLM\n",
    "sprint8_dir = str(PROJECT_ROOT / \"docs/agile/sprints/sprint_8\")\n",
    "\n",
    "docs_prompts = [\n",
    "    \"Call tool link.validate with target_directory='\" + sprint8_dir + \"'. Return only the tool result.\",\n",
    "    \"Call tool link.generate_report with target_directory='\" + sprint8_dir + \"', output_path=None. Return only the tool result.\",\n",
    "]\n",
    "await run_examples(\"dev_docs examples\", docs_prompts, docs_agent)\n",
    "\n",
    "# link.scan_all can be expensive on large dirs. Keep it opt-in.\n",
    "RUN_EXPENSIVE_LINK_SCAN = False\n",
    "if RUN_EXPENSIVE_LINK_SCAN:\n",
    "    scan_prompt = (\n",
    "        \"Call tool link.scan_all with target_directory='\" + sprint8_dir + \"'. \"\n",
    "        \"Return only the tool result.\"\n",
    "    )\n",
    "    await run_examples(\"dev_docs scan_all example\", [scan_prompt], docs_agent)\n",
    "\n",
    "# link.heal is a write operation. This notebook does not execute it automatically.\n",
    "# Template only:\n",
    "#   rename_mapping = {\"docs/old.md\": \"docs/new.md\"}\n",
    "#   call link.heal(rename_mapping=rename_mapping, target_directory=\"docs\")\n",
    "print(\"[INFO] link.heal example: template only (write operation)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2df918",
   "metadata": {},
   "source": [
    "### LLM examples: GitHub MCP tools (including repo-scoped code search)\n",
    "\n",
    "Tools covered (read-only):\n",
    "- `get_me`\n",
    "- `list_branches`\n",
    "- `list_commits`\n",
    "- `get_file_contents`\n",
    "- `search_code` (always include `repo:gitwalter/ai-dev-agent` in the query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72342397",
   "metadata": {},
   "outputs": [],
   "source": [
    "if github_agent is None:\n",
    "    print(\"[SKIP] GitHubAgent not available (set GITHUB_TOKEN to enable GitHub MCP)\")\n",
    "else:\n",
    "    github_prompts = [\n",
    "        \"Call tool get_me with no args. Return only the tool result.\",\n",
    "        \"Call tool list_branches for repository gitwalter/ai-dev-agent. Return only the tool result.\",\n",
    "        \"Call tool list_commits for repository gitwalter/ai-dev-agent with a small limit (e.g., 5). Return only the tool result.\",\n",
    "        \"Call tool get_file_contents for repository gitwalter/ai-dev-agent path='README.md' ref='main'. Return only the tool result.\",\n",
    "        \"Call tool search_code with query='repo:gitwalter/ai-dev-agent MultiServerMCPClient'. Return only the tool result.\",\n",
    "    ]\n",
    "\n",
    "    await run_examples(\"github examples\", github_prompts, github_agent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e3e2e",
   "metadata": {},
   "source": [
    "### DeepAgents coordinator demo: realistic dev workflow\n",
    "\n",
    "This demonstrates a typical \"dev agent\" workflow:\n",
    "\n",
    "- Local repo inspection (dev_repo)\n",
    "- Local search (dev_search)\n",
    "- Remote code search (GitHub MCP)\n",
    "- Docs link validation (dev_docs)\n",
    "\n",
    "The prompt asks the coordinator to delegate to specialists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinator_prompt = \"\"\"We previously hit a server-side error where a tool wrapper passed the wrong argument name to an underlying function.\n",
    "\n",
    "Tasks:\n",
    "1) Using local tools, find the definition of search_content_mcp and summarize its parameter names.\n",
    "2) Using GitHub code search (repo-scoped), find occurrences of 'search_content_mcp(' in gitwalter/ai-dev-agent.\n",
    "3) Using dev_docs, validate links under docs/agile/sprints/sprint_8 and report whether there are broken links.\n",
    "\n",
    "Keep the answer structured as:\n",
    "- Local signature\n",
    "- GitHub occurrences (paths)\n",
    "- Link validation summary\n",
    "\"\"\"\n",
    "\n",
    "result = await coordinator.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": coordinator_prompt}]})\n",
    "print(_as_text(result[\"messages\"][-1].content)[:2000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4aa25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3659eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stop local FastMCP servers started by this notebook ---\n",
    "# This will NOT stop servers that were already running before the start cell.\n",
    "from utils.mcp.fastmcp.start_notebook_mcp_servers import stop_servers\n",
    "\n",
    "if \"_MCP_NOTEBOOK_SERVER_PROCS\" not in globals() or not _MCP_NOTEBOOK_SERVER_PROCS:\n",
    "    print(\"[INFO] No notebook-started MCP servers to stop\")\n",
    "else:\n",
    "    stop_servers(_MCP_NOTEBOOK_SERVER_PROCS)\n",
    "    _MCP_NOTEBOOK_SERVER_PROCS = None\n",
    "    print(\"[OK] Notebook-started MCP servers stopped\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
