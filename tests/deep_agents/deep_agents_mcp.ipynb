{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# LangGraph Deep Agent with MCP Tools, Memory, and RAG\n",
    "\n",
    "This notebook demonstrates how to build a powerful LangGraph agent with:\n",
    "- **MCP Tools**: Integration with Model Context Protocol servers for external tools\n",
    "- **Memory**: Thread-scoped conversation persistence using checkpointers\n",
    "- **RAG**: Retrieval Augmented Generation for knowledge base queries\n",
    "- **Gemini 2.5 Flash**: Using Google's latest LLM model\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "+------------------+     +------------------+     +------------------+\n",
    "|    MCP Tools     |---->|   Deep Agent     |---->|   RAG Retriever  |\n",
    "|  (SAP Docs, etc) |     | (Gemini 2.5)     |     |  (Vector Store)  |\n",
    "+------------------+     +------------------+     +------------------+\n",
    "                               |\n",
    "                               v\n",
    "                    +------------------+\n",
    "                    |   Checkpointer   |\n",
    "                    |    (Memory)      |\n",
    "                    +------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-deps-md",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "First, ensure you have the required packages installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if packages are not installed\n",
    "# !pip install langchain langgraph langchain-google-genai langchain-mcp-adapters deepagents\n",
    "# !pip install langchain-core langchain-community langchain-text-splitters\n",
    "# !pip install langchain-huggingface sentence-transformers  # HuggingFace embeddings (local, no API key)\n",
    "# !pip install beautifulsoup4 lxml  # Required for WebBaseLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-md",
   "metadata": {},
   "source": [
    "## 2. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain core\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.tools import tool\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# MCP Adapters\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Gemini LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# HuggingFace Embeddings (local, no API key needed)\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Deep Agents\n",
    "from deepagents import create_deep_agent\n",
    "\n",
    "print('[OK] All imports successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-md",
   "metadata": {},
   "source": [
    "## 3. Environment Configuration\n",
    "\n",
    "Set up API keys for Gemini and any other services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "env-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] API key found\n"
     ]
    }
   ],
   "source": [
    "# Verify API key is set\n",
    "# You can set it here or use environment variables\n",
    "# os.environ['GOOGLE_API_KEY'] = 'your-api-key-here'\n",
    "\n",
    "api_key = os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY')\n",
    "if not api_key:\n",
    "    print('[WARNING] GEMINI_API_KEY or GOOGLE_API_KEY not found in environment')\n",
    "    print('Please set your API key using: os.environ[\"GOOGLE_API_KEY\"] = \"your-key\"')\n",
    "else:\n",
    "    print('[OK] API key found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-md",
   "metadata": {},
   "source": [
    "## 4. Initialize Gemini 2.5 Flash LLM\n",
    "\n",
    "Create the LLM instance with temperature=0 for deterministic responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "llm-init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] LLM initialized: models/gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini 2.5 Flash with temperature=0 for deterministic responses\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,  # CRITICAL: Always 0 for consistency\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    convert_system_message_to_human=True  # Required for Gemini compatibility\n",
    ")\n",
    "\n",
    "print(f'[OK] LLM initialized: {llm.model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-docs-md",
   "metadata": {},
   "source": [
    "## 5. RAG Setup - Fetch Knowledge Base from Websites\n",
    "\n",
    "We'll fetch real documentation about MCP and Deep Agents from their official sources to build a knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rag-docs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[...] Fetching documentation from websites using LangChain WebBaseLoader...\n",
      "[OK] Loaded 5 raw documents from web\n",
      "[OK] Split into 57 chunks\n",
      "\n",
      "[OK] Created knowledge base with 61 documents:\n",
      "  - 57 fetched from websites\n",
      "  - 4 curated documents\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# URLs to fetch documentation from\n",
    "doc_urls = [\n",
    "    \"https://modelcontextprotocol.io/docs/getting-started/intro\",\n",
    "    \"https://modelcontextprotocol.io/docs/learn/architecture\",\n",
    "    \"https://docs.langchain.com/oss/python/deepagents/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/deepagents/middleware\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/mcp\",\n",
    "]\n",
    "\n",
    "print(\"[...] Fetching documentation from websites using LangChain WebBaseLoader...\")\n",
    "\n",
    "# Fetch documents using LangChain's WebBaseLoader\n",
    "try:\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=doc_urls,\n",
    "        bs_kwargs={\"parse_only\": None},  # Parse full page\n",
    "    )\n",
    "    raw_docs = loader.load()\n",
    "    print(f\"[OK] Loaded {len(raw_docs)} raw documents from web\")\n",
    "    \n",
    "    # Split documents into smaller chunks for better retrieval\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    documents = text_splitter.split_documents(raw_docs)\n",
    "    print(f\"[OK] Split into {len(documents)} chunks\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[WARNING] Failed to fetch some URLs: {e}\")\n",
    "    documents = []\n",
    "\n",
    "# Add some curated content as backup/supplement\n",
    "curated_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"MCP (Model Context Protocol) is an open-source standard for connecting AI applications to external systems. \n",
    "        Using MCP, AI applications like Claude or ChatGPT can connect to data sources (e.g. local files, databases), \n",
    "        tools (e.g. search engines, calculators) and workflows (e.g. specialized prompts)—enabling them to access key \n",
    "        information and perform tasks. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a \n",
    "        standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications \n",
    "        to external systems. MCP follows a client-server architecture where an MCP host establishes connections to \n",
    "        one or more MCP servers. The key participants are: MCP Host (the AI application), MCP Client (maintains \n",
    "        connection to server), and MCP Server (provides context to clients).\"\"\",\n",
    "        metadata={\"source\": \"mcp_curated\", \"topic\": \"overview\", \"url\": \"https://modelcontextprotocol.io\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Deep agents are built with a modular middleware architecture. Deep agents have access to:\n",
    "        A planning tool (write_todos) - enables agents to break down complex tasks into discrete steps, track progress\n",
    "        A filesystem for storing context and long-term memories (ls, read_file, write_file, edit_file)\n",
    "        The ability to spawn subagents (task tool) - creates ephemeral agents for isolated multi-step tasks\n",
    "        Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent, \n",
    "        TodoListMiddleware, FilesystemMiddleware, and SubAgentMiddleware are automatically attached.\n",
    "        Middleware is composable—you can add as many or as few middleware to an agent as needed.\"\"\",\n",
    "        metadata={\"source\": \"deepagents_curated\", \"topic\": \"middleware\", \"url\": \"https://docs.langchain.com/oss/python/deepagents/middleware\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"deepagents is a standalone library for building agents that can tackle complex, multi-step tasks.\n",
    "        Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come \n",
    "        with planning capabilities, file systems for context management, and the ability to spawn subagents.\n",
    "        Core capabilities include: Planning and task decomposition with write_todos tool, Context management with \n",
    "        file system tools preventing context window overflow, Subagent spawning for context isolation and parallel \n",
    "        execution, and Long-term memory using LangGraph's Store for persistent memory across threads.\"\"\",\n",
    "        metadata={\"source\": \"deepagents_curated\", \"topic\": \"overview\", \"url\": \"https://docs.langchain.com/oss/python/deepagents/overview\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Model Context Protocol (MCP) is an open protocol that standardizes how applications provide \n",
    "        tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the \n",
    "        langchain-mcp-adapters library. MultiServerMCPClient enables agents to use tools defined across one or \n",
    "        more MCP servers. MCP servers can use different transports: stdio for local Python scripts, \n",
    "        streamable_http for remote HTTP servers. The MCP client is stateless by default - each tool invocation \n",
    "        creates a fresh MCP ClientSession, executes the tool, and then cleans up.\"\"\",\n",
    "        metadata={\"source\": \"langchain_mcp_curated\", \"topic\": \"integration\", \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Combine fetched and curated documents\n",
    "all_documents = documents + curated_documents\n",
    "\n",
    "print(f'\\n[OK] Created knowledge base with {len(all_documents)} documents:')\n",
    "print(f'  - {len(documents)} fetched from websites')\n",
    "print(f'  - {len(curated_documents)} curated documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectorstore-md",
   "metadata": {},
   "source": [
    "## 6. Create Vector Store and Retriever\n",
    "\n",
    "Initialize the vector store with HuggingFace's `all-MiniLM-L6-v2` embedding model (local, no API key needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vectorstore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] HuggingFace embeddings initialized (all-MiniLM-L6-v2)\n",
      "[OK] Vector store created with 61 documents\n",
      "[OK] Retriever configured to return top 3 results\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace embeddings (local, no API key required)\n",
    "# Using all-MiniLM-L6-v2: Fast, lightweight, 384 dimensions\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\n",
    "        'device': 'cpu',           # Works on any machine\n",
    "        'trust_remote_code': False  # Security\n",
    "    },\n",
    "    encode_kwargs={\n",
    "        'normalize_embeddings': True,  # Better similarity scores\n",
    "        'batch_size': 32               # Optimize for speed\n",
    "    }\n",
    ")\n",
    "print('[OK] HuggingFace embeddings initialized (all-MiniLM-L6-v2)')\n",
    "\n",
    "# Create in-memory vector store from fetched documents\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=all_documents,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f'[OK] Vector store created with {len(all_documents)} documents')\n",
    "print('[OK] Retriever configured to return top 3 results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-tool-md",
   "metadata": {},
   "source": [
    "## 7. Create RAG Retriever Tool\n",
    "\n",
    "Wrap the retriever as a tool that the agent can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rag-tool",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] RAG tool created and tested\n",
      "\n",
      "Test query result:\n",
      "[1] Source: https://modelcontextprotocol.io/docs/learn/architecture | Topic: general\n",
      "For specific implementation details, please refer to the documentation for your language-specific SDK.\n",
      "​Scope\n",
      "The Model Context Protocol includes the following projects:\n",
      "\n",
      "[2] Source: deepagents_curated | Topic: overview\n",
      "deepagents is a standalone library for building agents that can tackle complex, multi-step tasks.\n",
      "        Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manu...\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the internal knowledge base for information about LangGraph, \n",
    "    Deep Agents, MCP, memory, and RAG concepts.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to find relevant documents\n",
    "        \n",
    "    Returns:\n",
    "        Relevant document excerpts from the knowledge base\n",
    "    \"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    if not docs:\n",
    "        return \"No relevant documents found in the knowledge base.\"\n",
    "    \n",
    "    # Format results with source metadata\n",
    "    results = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        topic = doc.metadata.get('topic', 'general')\n",
    "        results.append(f\"[{i}] Source: {source} | Topic: {topic}\\n{doc.page_content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "\n",
    "# Test the RAG tool\n",
    "test_result = search_knowledge_base.invoke({\"query\": \"What is LangGraph?\"})\n",
    "print('[OK] RAG tool created and tested')\n",
    "print('\\nTest query result:')\n",
    "print(test_result[:500] + '...' if len(test_result) > 500 else test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mcp-md",
   "metadata": {},
   "source": [
    "## 8. MCP Tools Integration\n",
    "\n",
    "Set up the MCP client to connect to external tool servers. The MCP client can connect to multiple servers simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "mcp-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded 1 MCP tools:\n",
      "  - get_weather\n"
     ]
    }
   ],
   "source": [
    "async def get_mcp_tools():\n",
    "    \"\"\"\n",
    "    Initialize MCP client and get tools from configured servers.\n",
    "    \n",
    "    MCP Servers can be:\n",
    "    - stdio: Local Python scripts\n",
    "    - streamable_http: Remote HTTP servers\n",
    "    \n",
    "    Returns:\n",
    "        List of LangChain tools from MCP servers\n",
    "    \"\"\"\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            # SAP Documentation MCP Server\n",
    "            # \"sap_docs\": {\n",
    "            #     \"transport\": \"streamable_http\",\n",
    "            #     \"url\": \"https://mcp-sap-docs.marianzeis.de/mcp\",\n",
    "            # },\n",
    "            # You can add more MCP servers here:\n",
    "            # \"math\": {\n",
    "            #     \"transport\": \"stdio\",\n",
    "            #     \"command\": r\"C:\\App\\Anaconda\\python.exe\",\n",
    "            #     \"args\": [r\"path\\to\\math_server.py\"],\n",
    "            # },\n",
    "            \"weather\": {\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"url\": \"http://localhost:8000/mcp\",\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    tools = await client.get_tools()\n",
    "    return tools\n",
    "\n",
    "\n",
    "# Get MCP tools\n",
    "mcp_tools = await get_mcp_tools()\n",
    "print(f'[OK] Loaded {len(mcp_tools)} MCP tools:')\n",
    "for t in mcp_tools:\n",
    "    print(f'  - {t.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-md",
   "metadata": {},
   "source": [
    "## 9. Memory/Checkpointer Setup\n",
    "\n",
    "Set up the checkpointer for conversation memory. This enables:\n",
    "- Short-term memory within a conversation thread\n",
    "- Ability to resume conversations\n",
    "- Multi-turn context retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "checkpointer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Checkpointer initialized (InMemorySaver)\n",
      "\n",
      "Memory types available:\n",
      "  - Short-term: Conversation history within a thread\n",
      "  - Long-term: Use StoreBackend for cross-thread persistence\n"
     ]
    }
   ],
   "source": [
    "# Create checkpointer for memory persistence\n",
    "# InMemorySaver: For development/testing (data lost on restart)\n",
    "# PostgresSaver: For production (persistent across restarts)\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "print('[OK] Checkpointer initialized (InMemorySaver)')\n",
    "print('\\nMemory types available:')\n",
    "print('  - Short-term: Conversation history within a thread')\n",
    "print('  - Long-term: Use StoreBackend for cross-thread persistence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-md",
   "metadata": {},
   "source": [
    "## 10. Create the Deep Agent\n",
    "\n",
    "Combine all components into a powerful Deep Agent:\n",
    "- **LLM**: Gemini 2.5 Flash\n",
    "- **Tools**: RAG retriever + MCP tools\n",
    "- **Memory**: Checkpointer for conversation persistence\n",
    "- **Middleware**: TodoList, Filesystem, SubAgent (included by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "create-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Deep Agent created with:\n",
      "  - LLM: Gemini 2.5 Flash\n",
      "  - Tools: 2 (1 MCP + 1 RAG)\n",
      "  - Memory: MemorySaver checkpointer\n",
      "  - Middleware: TodoList, Filesystem, SubAgent\n"
     ]
    }
   ],
   "source": [
    "# Combine all tools: RAG + MCP\n",
    "all_tools = [search_knowledge_base] + mcp_tools\n",
    "\n",
    "# System prompt for the agent\n",
    "system_prompt = \"\"\"\\\n",
    "You are an intelligent research assistant with access to multiple knowledge sources.\n",
    "\n",
    "Your capabilities:\n",
    "1. **Internal Knowledge Base (RAG)**: Use 'search_knowledge_base' to query information about \n",
    "   LangGraph, Deep Agents, MCP, memory systems, and RAG concepts.\n",
    "\n",
    "2. **SAP Documentation (MCP)**: Use SAP documentation tools to search for ABAP, UI5, CAP, \n",
    "   and other SAP-related information.\n",
    "\n",
    "3. **Planning**: Break down complex tasks into manageable steps using the todo list.\n",
    "\n",
    "4. **File System**: Store important findings and notes for later reference.\n",
    "\n",
    "Guidelines:\n",
    "- Always search relevant knowledge sources before answering technical questions\n",
    "- Cite your sources when providing information\n",
    "- For complex questions, create a plan first\n",
    "- Be concise but thorough in your responses\n",
    "\"\"\"\n",
    "\n",
    "# Create the deep agent with all components\n",
    "agent = create_deep_agent(\n",
    "    model=llm,\n",
    "    tools=all_tools,\n",
    "    system_prompt=system_prompt,\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "print('[OK] Deep Agent created with:')\n",
    "print(f'  - LLM: Gemini 2.5 Flash')\n",
    "print(f'  - Tools: {len(all_tools)} ({len(mcp_tools)} MCP + 1 RAG)')\n",
    "print(f'  - Memory: MemorySaver checkpointer')\n",
    "print(f'  - Middleware: TodoList, Filesystem, SubAgent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper-md",
   "metadata": {},
   "source": [
    "## 11. Helper Function for Agent Interaction\n",
    "\n",
    "Create a helper to easily chat with the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "helper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def extract_text_content(content) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from various content formats.\n",
    "    \n",
    "    Handles:\n",
    "    - Plain strings\n",
    "    - List of content blocks (e.g., [{\"type\": \"text\", \"text\": \"...\"}])\n",
    "    - AIMessage with content attribute\n",
    "    \"\"\"\n",
    "    if content is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # If it's already a string, return as-is\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    \n",
    "    # If it's a list of content blocks (common with Gemini/Claude)\n",
    "    if isinstance(content, list):\n",
    "        text_parts = []\n",
    "        for block in content:\n",
    "            if isinstance(block, str):\n",
    "                text_parts.append(block)\n",
    "            elif isinstance(block, dict):\n",
    "                # Handle {\"type\": \"text\", \"text\": \"...\"} format\n",
    "                if block.get(\"type\") == \"text\":\n",
    "                    text_parts.append(block.get(\"text\", \"\"))\n",
    "                # Handle other dict formats\n",
    "                elif \"text\" in block:\n",
    "                    text_parts.append(block[\"text\"])\n",
    "                elif \"content\" in block:\n",
    "                    text_parts.append(str(block[\"content\"]))\n",
    "            else:\n",
    "                # Try to convert to string\n",
    "                text_parts.append(str(block))\n",
    "        return \"\\n\".join(text_parts)\n",
    "    \n",
    "    # Fallback: convert to string\n",
    "    return str(content)\n",
    "\n",
    "\n",
    "async def chat(message: str, thread_id: str = \"default\") -> str:\n",
    "    \"\"\"\n",
    "    Send a message to the agent and get a response.\n",
    "    \n",
    "    Args:\n",
    "        message: The user's message\n",
    "        thread_id: Conversation thread ID for memory continuity\n",
    "        \n",
    "    Returns:\n",
    "        The agent's response text\n",
    "    \"\"\"\n",
    "    response = await agent.ainvoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": message}]},\n",
    "        {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "    \n",
    "    # Extract the last message content\n",
    "    last_message = response[\"messages\"][-1]\n",
    "    \n",
    "    # Handle various content formats\n",
    "    return extract_text_content(last_message.content)\n",
    "\n",
    "\n",
    "def display_response(response):\n",
    "    \"\"\"Display the response as formatted markdown.\"\"\"\n",
    "    # Ensure response is a string\n",
    "    text = extract_text_content(response) if not isinstance(response, str) else response\n",
    "    \n",
    "    if text:\n",
    "        display(Markdown(text))\n",
    "    else:\n",
    "        print(\"[No response content]\")\n",
    "\n",
    "\n",
    "print('[OK] Helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1-md",
   "metadata": {},
   "source": [
    "## 12. Example Usage\n",
    "\n",
    "### Example 1: RAG Query (Internal Knowledge Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "example1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In LangGraph, particularly within the context of Deep Agents, the distinction between short-term and long-term memory is as follows:\n",
       "\n",
       "*   **Short-term memory** refers to the default filesystem used by tools like `ls`, `read_file`, `write_file`, and `edit_file`. This memory is local to the current graph state and is not persistent across different threads or conversations. Information stored here is lost once the agent's current execution or thread concludes. (Source: [2])\n",
       "\n",
       "*   **Long-term memory** enables agents to retain information persistently across multiple conversations and threads. This is achieved by configuring a `CompositeBackend` to route specific file paths (e.g., `/memories/`) to a `StoreBackend`, which then utilizes a `Store` (such as `InMemoryStore`). This setup allows agents to save and retrieve information from past interactions, ensuring data availability beyond the current session. (Source: [1], [2])"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query the internal knowledge base using RAG\n",
    "response1 = await chat(\n",
    "    \"What is the difference between short-term and long-term memory in LangGraph?\",\n",
    "    # \"How is the weather in Sao Paulo today?\",\n",
    "    thread_id=\"rag-demo\"\n",
    ")\n",
    "display_response(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2-md",
   "metadata": {},
   "source": [
    "### Example 2: MCP Query (SAP Documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query SAP documentation using MCP tools\n",
    "response2 = await chat(\n",
    "    \"How do I handle internal tables in modern ABAP? Give me a brief overview.\",\n",
    "    thread_id=\"mcp-demo\"\n",
    ")\n",
    "display_response(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example3-md",
   "metadata": {},
   "source": [
    "### Example 3: Memory Continuity Demo\n",
    "\n",
    "Demonstrate that the agent remembers previous messages within a thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "example3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello Alice! It's great to meet you. I can help you learn about AI agents. What specifically about AI agents are you interested in? Are there any particular concepts or aspects you'd like to explore first?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First message in a new thread\n",
    "response3a = await chat(\n",
    "    \"My name is Alice and I'm learning about AI agents.\",\n",
    "    thread_id=\"memory-demo\"\n",
    ")\n",
    "print(\"First response:\")\n",
    "display_response(response3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "example3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow-up response (should remember context):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Alice, and you were learning about AI agents."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Follow-up message in the same thread - agent should remember the name\n",
    "response3b = await chat(\n",
    "    \"What's my name? And what was I learning about?\",\n",
    "    thread_id=\"memory-demo\"  # Same thread ID!\n",
    ")\n",
    "print(\"Follow-up response (should remember context):\")\n",
    "display_response(response3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example4-md",
   "metadata": {},
   "source": [
    "### Example 4: Combined Query (RAG + MCP)\n",
    "\n",
    "The agent can use multiple tools in one query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined query that might use both RAG and MCP\n",
    "response4 = await chat(\n",
    "    \"First, explain what Deep Agents middleware provides (use internal knowledge), \"\n",
    "    \"then give me a quick tip about clean ABAP coding (use SAP docs).\",\n",
    "    thread_id=\"combined-demo\"\n",
    ")\n",
    "display_response(response4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-md",
   "metadata": {},
   "source": [
    "## 13. Direct Tool Testing\n",
    "\n",
    "Test tools directly without the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "tools-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG Tool Test ===\n",
      "[1] Source: langchain_mcp_curated | Topic: integration\n",
      "Model Context Protocol (MCP) is an open protocol that standardizes how applications provide \n",
      "        tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the \n",
      "        langchain-mcp-adapters library. MultiServerMCPClient enables agents to use tools defined across one or \n",
      "        more MCP servers. MCP servers can use different transports: stdio for local Python scripts, \n",
      "        streamable_http for remote HTTP servers. The MCP client is stateless by default - each tool invocation \n",
      "        creates a fresh MCP ClientSession, executes the tool, and then cleans up.\n",
      "\n",
      "[2] Source: mcp_curated | Topic: overview\n",
      "MCP (Model Context Protocol) is an open-source standard for connecting AI applications to external systems. \n",
      "        Using MCP, AI applications like Claude or ChatGPT can connect to data sources (e.g. local files, databases), \n",
      "        tools (e.g. search engines, calculators) and workflows (e.g. specialized prompts)—enabling them to access key \n",
      "        information and perform tasks. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a \n",
      "        standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications \n",
      "        to external systems. MCP follows a client-server architecture where an MCP host establishes connections to \n",
      "        one or more MCP servers. The key participants are: MCP Host (the AI application), MCP Client (maintains \n",
      "        connection to server), and MCP Server (provides context to clients).\n",
      "\n",
      "[3] Source: https://modelcontextprotocol.io/docs/getting-started/intro | Topic: general\n",
      "What is the Model Context Protocol (MCP)? - Model Context ProtocolSkip to main contentModel Context Protocol home pageSearch...⌘KBlogGitHubSearch...NavigationGet startedWhat is the Model Context Protocol (MCP)?DocumentationSpecificationCommunityGet startedWhat is MCP?About MCPArchitectureServersClientsVersioningDevelop with MCPConnect to local MCP serversConnect to remote MCP ServersBuild an MCP serverBuild an MCP clientSDKsSecurityDeveloper toolsMCP InspectorOn this pageWhat can MCP enable?Why does MCP matter?Start BuildingLearn moreGet startedWhat is the Model Context Protocol (MCP)?Copy pageCopy pageMCP (Model Context Protocol) is an open-source standard for connecting AI applications to external systems.\n",
      "Using MCP, AI applications like Claude or ChatGPT can connect to data sources (e.g. local files, databases), tools (e.g. search engines, calculators) and workflows (e.g. specialized prompts)—enabling them to access key information and perform tasks.\n",
      "Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications to external systems.\n",
      "\n",
      "​What can MCP enable?\n",
      "\n",
      "=== MCP Tool Test (search) ===\n",
      "{\"results\":[{\"id\":\"/sap-styleguides/clean-abap/CleanABAP#prefer-inline-to-up-front-declarations\",\"title\":\"Prefer inline to up-front declarations\",\"url\":\"https://github.com/SAP/styleguides/blob/main/\",\"snippet\":\"Prefer inline to up-front declarations\\n\\n> [Clean ABAP](#clean-abap) > [Content](#content) > [Variables](#variables) > [This section](#prefer-inline-to-up-front-declarations) If you follow these guidelines, your methods will become so short (3-5 statements) that declaring variables inline at first occurrence will look more natural METHOD d...\\n\\n/sap-styleguides/clean-abap/CleanABAP#prefer-inline-to-...\",\"score\":23.04421360706857,\"metadata\":{\"source\":\"sap-styleguides\",\"library\":\"/sap-styleguides\",\"bm25Score\":-21.739824157611856,\"rank\":1}},{\"id\":\"/abap-docs-latest/abenselect_inline_decl_abexa\",\"title\":\"SELECT, Inline Declarations\",\"url\":\"https://help.sap.com/doc/abapdocu_cp_index_htm/CLOUD/en-US/.html\",\"snippet\":\"SELECT, Inline Declarations\\n\\nThis example demonstrates inline de...\n"
     ]
    }
   ],
   "source": [
    "# Test RAG tool directly\n",
    "print(\"=== RAG Tool Test ===\")\n",
    "rag_result = search_knowledge_base.invoke({\"query\": \"What is MCP protocol?\"})\n",
    "print(rag_result)\n",
    "print()\n",
    "\n",
    "# Test MCP tool directly (if available)\n",
    "if mcp_tools:\n",
    "    print(\"=== MCP Tool Test (search) ===\")\n",
    "    # Find the search tool\n",
    "    search_tool = next((t for t in mcp_tools if t.name == 'search'), None)\n",
    "    if search_tool:\n",
    "        mcp_result = await search_tool.ainvoke({\"query\": \"ABAP inline declarations\"})\n",
    "        # Print first 1000 chars of result\n",
    "        print(str(mcp_result)[:1000] + '...' if len(str(mcp_result)) > 1000 else mcp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-md",
   "metadata": {},
   "source": [
    "## 14. Configuration Options\n",
    "\n",
    "### Additional MCP Servers\n",
    "\n",
    "You can add more MCP servers to expand the agent's capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adding more MCP servers\n",
    "example_mcp_config = \"\"\"\n",
    "# MCP Server Configuration Examples:\n",
    "\n",
    "mcp_servers = {\n",
    "    # Local stdio server (Python script)\n",
    "    \"math\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": r\"C:\\\\App\\\\Anaconda\\\\python.exe\",\n",
    "        \"args\": [\"math_server.py\"],\n",
    "    },\n",
    "    \n",
    "    # Remote HTTP server\n",
    "    \"weather\": {\n",
    "        \"transport\": \"streamable_http\",\n",
    "        \"url\": \"https://weather-mcp.example.com/mcp\",\n",
    "    },\n",
    "    \n",
    "    # Server with authentication\n",
    "    \"github\": {\n",
    "        \"transport\": \"streamable_http\",\n",
    "        \"url\": \"https://github-mcp.example.com/mcp\",\n",
    "        \"headers\": {\n",
    "            \"Authorization\": \"Bearer YOUR_TOKEN\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # FastMCP server\n",
    "    \"fastmcp\": {\n",
    "        \"transport\": \"streamable_http\",\n",
    "        \"url\": \"https://gofastmcp.com/mcp\",\n",
    "    },\n",
    "}\n",
    "\"\"\"\n",
    "print(example_mcp_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prod-md",
   "metadata": {},
   "source": [
    "### Production Memory Configuration\n",
    "\n",
    "For production, use a persistent checkpointer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "prod-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# For production, use PostgresSaver instead of MemorySaver:\n",
      "\n",
      "from langgraph.checkpoint.postgres import PostgresSaver\n",
      "\n",
      "# Connection string\n",
      "connection_string = \"postgresql://user:password@localhost:5432/langgraph\"\n",
      "\n",
      "# Create production checkpointer\n",
      "checkpointer = PostgresSaver.from_conn_string(connection_string)\n",
      "\n",
      "# Or use async version\n",
      "from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n",
      "checkpointer = AsyncPostgresSaver.from_conn_string(connection_string)\n",
      "\n",
      "# SQLite for local development\n",
      "from langgraph.checkpoint.sqlite import SqliteSaver\n",
      "checkpointer = SqliteSaver.from_conn_string(\"checkpoints.db\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Production checkpointer example\n",
    "production_config = \"\"\"\n",
    "# For production, use PostgresSaver instead of MemorySaver:\n",
    "\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "# Connection string\n",
    "connection_string = \"postgresql://user:password@localhost:5432/langgraph\"\n",
    "\n",
    "# Create production checkpointer\n",
    "checkpointer = PostgresSaver.from_conn_string(connection_string)\n",
    "\n",
    "# Or use async version\n",
    "from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n",
    "checkpointer = AsyncPostgresSaver.from_conn_string(connection_string)\n",
    "\n",
    "# SQLite for local development\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "checkpointer = SqliteSaver.from_conn_string(\"checkpoints.db\")\n",
    "\"\"\"\n",
    "print(production_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-md",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to build a powerful LangGraph Deep Agent with:\n",
    "\n",
    "1. **MCP Tools Integration**: Connected to external SAP documentation via Model Context Protocol\n",
    "2. **Memory/Checkpointing**: Thread-scoped conversation persistence using MemorySaver\n",
    "3. **RAG Capabilities**: Knowledge base fetched from official documentation websites\n",
    "4. **HuggingFace Embeddings**: Local embeddings with `all-MiniLM-L6-v2` (no API key needed)\n",
    "5. **Gemini 2.5 Flash**: Fast, capable LLM for agent reasoning\n",
    "\n",
    "### Key Components Used:\n",
    "\n",
    "| Component | Library | Purpose |\n",
    "|-----------|---------|--------|\n",
    "| Deep Agent | `deepagents` | Agent with planning, filesystem, subagents |\n",
    "| MCP Client | `langchain-mcp-adapters` | External tool integration |\n",
    "| Embeddings | `langchain-huggingface` | Local embeddings (all-MiniLM-L6-v2) |\n",
    "| Vector Store | `langchain_core` | RAG knowledge base |\n",
    "| Web Loader | `langchain-community` | Fetch docs from websites |\n",
    "| Checkpointer | `langgraph` | Conversation memory |\n",
    "| LLM | `langchain-google-genai` | Gemini 2.5 Flash |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Add more MCP tool servers for expanded capabilities\n",
    "- Use `FilesystemBackend` for persistent agent workspace\n",
    "- Implement `StoreBackend` for cross-thread long-term memory\n",
    "- Deploy with PostgresSaver for production memory persistence\n",
    "- Add human-in-the-loop with `interrupt_on` parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
