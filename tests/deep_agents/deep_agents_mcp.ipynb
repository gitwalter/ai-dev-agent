{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# LangGraph Deep Agent with MCP Tools, Memory, and RAG\n",
    "\n",
    "This notebook demonstrates how to build a powerful LangGraph agent with:\n",
    "- **MCP Tools**: Integration with Model Context Protocol servers for external tools\n",
    "- **Memory**: Thread-scoped conversation persistence using checkpointers\n",
    "- **RAG**: Retrieval Augmented Generation for knowledge base queries\n",
    "- **Gemini 2.5 Flash**: Using Google's latest LLM model\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "+------------------+     +------------------+     +------------------+\n",
    "|    MCP Tools     |---->|   Deep Agent     |---->|   RAG Retriever  |\n",
    "|  (SAP Docs, etc) |     | (Gemini 2.5)     |     |  (Vector Store)  |\n",
    "+------------------+     +------------------+     +------------------+\n",
    "                               |\n",
    "                               v\n",
    "                    +------------------+\n",
    "                    |   Checkpointer   |\n",
    "                    |    (Memory)      |\n",
    "                    +------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-deps-md",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "First, ensure you have the required packages installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if packages are not installed\n",
    "# !pip install langchain langgraph langchain-google-genai langchain-mcp-adapters deepagents\n",
    "# !pip install langchain-core langchain-community langchain-text-splitters\n",
    "# !pip install langchain-huggingface sentence-transformers  # HuggingFace embeddings (local, no API key)\n",
    "# !pip install beautifulsoup4 lxml  # Required for WebBaseLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-md",
   "metadata": {},
   "source": [
    "## 2. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain core\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.tools import tool\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# MCP Adapters\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Gemini LLM\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# HuggingFace Embeddings (local, no API key needed)\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Deep Agents\n",
    "from deepagents import create_deep_agent\n",
    "\n",
    "print('[OK] All imports successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-md",
   "metadata": {},
   "source": [
    "## 3. Environment Configuration\n",
    "\n",
    "Set up API keys for Gemini and any other services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] API key found\n"
     ]
    }
   ],
   "source": [
    "# Verify API key is set\n",
    "# You can set it here or use environment variables\n",
    "# os.environ['GOOGLE_API_KEY'] = 'your-api-key-here'\n",
    "\n",
    "api_key = os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY')\n",
    "if not api_key:\n",
    "    print('[WARNING] GEMINI_API_KEY or GOOGLE_API_KEY not found in environment')\n",
    "    print('Please set your API key using: os.environ[\"GOOGLE_API_KEY\"] = \"your-key\"')\n",
    "else:\n",
    "    print('[OK] API key found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-md",
   "metadata": {},
   "source": [
    "## 4. Initialize Gemini 2.5 Flash LLM\n",
    "\n",
    "Create the LLM instance with temperature=0 for deterministic responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "llm-init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] LLM initialized: models/gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini 2.5 Flash with temperature=0 for deterministic responses\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,  # CRITICAL: Always 0 for consistency\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    convert_system_message_to_human=True  # Required for Gemini compatibility\n",
    ")\n",
    "\n",
    "print(f'[OK] LLM initialized: {llm.model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-docs-md",
   "metadata": {},
   "source": [
    "## 5. RAG Setup - Fetch Knowledge Base from Websites\n",
    "\n",
    "We'll fetch real documentation about MCP and Deep Agents from their official sources to build a knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rag-docs",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[...] Fetching documentation from websites using LangChain WebBaseLoader...\n",
      "[OK] Loaded 5 raw documents from web\n",
      "[OK] Split into 57 chunks\n",
      "\n",
      "[OK] Created knowledge base with 61 documents:\n",
      "  - 57 fetched from websites\n",
      "  - 4 curated documents\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# URLs to fetch documentation from\n",
    "doc_urls = [\n",
    "    \"https://modelcontextprotocol.io/docs/getting-started/intro\",\n",
    "    \"https://modelcontextprotocol.io/docs/learn/architecture\",\n",
    "    \"https://docs.langchain.com/oss/python/deepagents/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/deepagents/middleware\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/mcp\",\n",
    "]\n",
    "\n",
    "print(\"[...] Fetching documentation from websites using LangChain WebBaseLoader...\")\n",
    "\n",
    "# Fetch documents using LangChain's WebBaseLoader\n",
    "try:\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=doc_urls,\n",
    "        bs_kwargs={\"parse_only\": None},  # Parse full page\n",
    "    )\n",
    "    raw_docs = loader.load()\n",
    "    print(f\"[OK] Loaded {len(raw_docs)} raw documents from web\")\n",
    "    \n",
    "    # Split documents into smaller chunks for better retrieval\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    documents = text_splitter.split_documents(raw_docs)\n",
    "    print(f\"[OK] Split into {len(documents)} chunks\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[WARNING] Failed to fetch some URLs: {e}\")\n",
    "    documents = []\n",
    "\n",
    "# Add some curated content as backup/supplement\n",
    "curated_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"MCP (Model Context Protocol) is an open-source standard for connecting AI applications to external systems. \n",
    "        Using MCP, AI applications like Claude or ChatGPT can connect to data sources (e.g. local files, databases), \n",
    "        tools (e.g. search engines, calculators) and workflows (e.g. specialized prompts)—enabling them to access key \n",
    "        information and perform tasks. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a \n",
    "        standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications \n",
    "        to external systems. MCP follows a client-server architecture where an MCP host establishes connections to \n",
    "        one or more MCP servers. The key participants are: MCP Host (the AI application), MCP Client (maintains \n",
    "        connection to server), and MCP Server (provides context to clients).\"\"\",\n",
    "        metadata={\"source\": \"mcp_curated\", \"topic\": \"overview\", \"url\": \"https://modelcontextprotocol.io\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Deep agents are built with a modular middleware architecture. Deep agents have access to:\n",
    "        A planning tool (write_todos) - enables agents to break down complex tasks into discrete steps, track progress\n",
    "        A filesystem for storing context and long-term memories (ls, read_file, write_file, edit_file)\n",
    "        The ability to spawn subagents (task tool) - creates ephemeral agents for isolated multi-step tasks\n",
    "        Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent, \n",
    "        TodoListMiddleware, FilesystemMiddleware, and SubAgentMiddleware are automatically attached.\n",
    "        Middleware is composable—you can add as many or as few middleware to an agent as needed.\"\"\",\n",
    "        metadata={\"source\": \"deepagents_curated\", \"topic\": \"middleware\", \"url\": \"https://docs.langchain.com/oss/python/deepagents/middleware\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"deepagents is a standalone library for building agents that can tackle complex, multi-step tasks.\n",
    "        Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come \n",
    "        with planning capabilities, file systems for context management, and the ability to spawn subagents.\n",
    "        Core capabilities include: Planning and task decomposition with write_todos tool, Context management with \n",
    "        file system tools preventing context window overflow, Subagent spawning for context isolation and parallel \n",
    "        execution, and Long-term memory using LangGraph's Store for persistent memory across threads.\"\"\",\n",
    "        metadata={\"source\": \"deepagents_curated\", \"topic\": \"overview\", \"url\": \"https://docs.langchain.com/oss/python/deepagents/overview\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Model Context Protocol (MCP) is an open protocol that standardizes how applications provide \n",
    "        tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the \n",
    "        langchain-mcp-adapters library. MultiServerMCPClient enables agents to use tools defined across one or \n",
    "        more MCP servers. MCP servers can use different transports: stdio for local Python scripts, \n",
    "        streamable_http for remote HTTP servers. The MCP client is stateless by default - each tool invocation \n",
    "        creates a fresh MCP ClientSession, executes the tool, and then cleans up.\"\"\",\n",
    "        metadata={\"source\": \"langchain_mcp_curated\", \"topic\": \"integration\", \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Combine fetched and curated documents\n",
    "all_documents = documents + curated_documents\n",
    "\n",
    "print(f'\\n[OK] Created knowledge base with {len(all_documents)} documents:')\n",
    "print(f'  - {len(documents)} fetched from websites')\n",
    "print(f'  - {len(curated_documents)} curated documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectorstore-md",
   "metadata": {},
   "source": [
    "## 6. Create Vector Store and Retriever\n",
    "\n",
    "Initialize the vector store with HuggingFace's `all-MiniLM-L6-v2` embedding model (local, no API key needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vectorstore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] HuggingFace embeddings initialized (all-MiniLM-L6-v2)\n",
      "[OK] Vector store created with 61 documents\n",
      "[OK] Retriever configured to return top 3 results\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace embeddings (local, no API key required)\n",
    "# Using all-MiniLM-L6-v2: Fast, lightweight, 384 dimensions\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\n",
    "        'device': 'cpu',           # Works on any machine\n",
    "        'trust_remote_code': False  # Security\n",
    "    },\n",
    "    encode_kwargs={\n",
    "        'normalize_embeddings': True,  # Better similarity scores\n",
    "        'batch_size': 32               # Optimize for speed\n",
    "    }\n",
    ")\n",
    "print('[OK] HuggingFace embeddings initialized (all-MiniLM-L6-v2)')\n",
    "\n",
    "# Create in-memory vector store from fetched documents\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=all_documents,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f'[OK] Vector store created with {len(all_documents)} documents')\n",
    "print('[OK] Retriever configured to return top 3 results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-tool-md",
   "metadata": {},
   "source": [
    "## 7. Create RAG Retriever Tool\n",
    "\n",
    "Wrap the retriever as a tool that the agent can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rag-tool",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] RAG tool created and tested\n",
      "\n",
      "Test query result:\n",
      "[1] Source: https://modelcontextprotocol.io/docs/learn/architecture | Topic: general\n",
      "For specific implementation details, please refer to the documentation for your language-specific SDK.\n",
      "​Scope\n",
      "The Model Context Protocol includes the following projects:\n",
      "\n",
      "[2] Source: deepagents_curated | Topic: overview\n",
      "deepagents is a standalone library for building agents that can tackle complex, multi-step tasks.\n",
      "        Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manu...\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the internal knowledge base for information about LangGraph, \n",
    "    Deep Agents, MCP, memory, and RAG concepts.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to find relevant documents\n",
    "        \n",
    "    Returns:\n",
    "        Relevant document excerpts from the knowledge base\n",
    "    \"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    if not docs:\n",
    "        return \"No relevant documents found in the knowledge base.\"\n",
    "    \n",
    "    # Format results with source metadata\n",
    "    results = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        topic = doc.metadata.get('topic', 'general')\n",
    "        results.append(f\"[{i}] Source: {source} | Topic: {topic}\\n{doc.page_content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "\n",
    "# Test the RAG tool\n",
    "test_result = search_knowledge_base.invoke({\"query\": \"What is LangGraph?\"})\n",
    "print('[OK] RAG tool created and tested')\n",
    "print('\\nTest query result:')\n",
    "print(test_result[:500] + '...' if len(test_result) > 500 else test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mcp-md",
   "metadata": {},
   "source": [
    "## 8. MCP Tools Integration\n",
    "\n",
    "Set up the MCP client to connect to external tool servers. The MCP client can connect to multiple servers simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mcp-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded 1 MCP tools:\n",
      "  - get_weather\n"
     ]
    }
   ],
   "source": [
    "async def get_mcp_tools():\n",
    "    \"\"\"\n",
    "    Initialize MCP client and get tools from configured servers.\n",
    "    \n",
    "    MCP Servers can be:\n",
    "    - stdio: Local Python scripts\n",
    "    - streamable_http: Remote HTTP servers\n",
    "    \n",
    "    Returns:\n",
    "        List of LangChain tools from MCP servers\n",
    "    \"\"\"\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            # SAP Documentation MCP Server\n",
    "            # \"sap_docs\": {\n",
    "            #     \"transport\": \"streamable_http\",\n",
    "            #     \"url\": \"https://mcp-sap-docs.marianzeis.de/mcp\",\n",
    "            # },\n",
    "            # You can add more MCP servers here:\n",
    "            # \"math\": {\n",
    "            #     \"transport\": \"stdio\",\n",
    "            #     \"command\": r\"C:\\App\\Anaconda\\python.exe\",\n",
    "            #     \"args\": [r\"path\\to\\math_server.py\"],\n",
    "            # },\n",
    "            \"weather\": {\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"url\": \"http://localhost:8000/mcp\",\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    tools = await client.get_tools()\n",
    "    return tools\n",
    "\n",
    "\n",
    "# Get MCP tools\n",
    "mcp_tools = await get_mcp_tools()\n",
    "print(f'[OK] Loaded {len(mcp_tools)} MCP tools:')\n",
    "for t in mcp_tools:\n",
    "    print(f'  - {t.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-md",
   "metadata": {},
   "source": [
    "## 9. Memory/Checkpointer Setup\n",
    "\n",
    "Set up the checkpointer for conversation memory. This enables:\n",
    "- Short-term memory within a conversation thread\n",
    "- Ability to resume conversations\n",
    "- Multi-turn context retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "checkpointer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Checkpointer initialized (InMemorySaver)\n",
      "\n",
      "Memory types available:\n",
      "  - Short-term: Conversation history within a thread\n",
      "  - Long-term: Use StoreBackend for cross-thread persistence\n"
     ]
    }
   ],
   "source": [
    "# Create checkpointer for memory persistence\n",
    "# InMemorySaver: For development/testing (data lost on restart)\n",
    "# PostgresSaver: For production (persistent across restarts)\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "print('[OK] Checkpointer initialized (InMemorySaver)')\n",
    "print('\\nMemory types available:')\n",
    "print('  - Short-term: Conversation history within a thread')\n",
    "print('  - Long-term: Use StoreBackend for cross-thread persistence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-md",
   "metadata": {},
   "source": [
    "## 10. Create the Deep Agent\n",
    "\n",
    "Combine all components into a powerful Deep Agent:\n",
    "- **LLM**: Gemini 2.5 Flash\n",
    "- **Tools**: RAG retriever + MCP tools\n",
    "- **Memory**: Checkpointer for conversation persistence\n",
    "- **Middleware**: TodoList, Filesystem, SubAgent (included by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "create-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Deep Agent created with:\n",
      "  - LLM: Gemini 2.5 Flash\n",
      "  - Tools: 2 (1 MCP + 1 RAG)\n",
      "  - Memory: MemorySaver checkpointer\n",
      "  - Middleware: TodoList, Filesystem, SubAgent\n"
     ]
    }
   ],
   "source": [
    "# Combine all tools: RAG + MCP\n",
    "all_tools = [search_knowledge_base] + mcp_tools\n",
    "\n",
    "# System prompt for the agent\n",
    "system_prompt = \"\"\"\\\n",
    "You are an intelligent research assistant with access to multiple knowledge sources.\n",
    "\n",
    "Your capabilities:\n",
    "1. **Internal Knowledge Base (RAG)**: Use 'search_knowledge_base' to query information about \n",
    "   LangGraph, Deep Agents, MCP, memory systems, and RAG concepts.\n",
    "\n",
    "2. **SAP Documentation (MCP)**: Use SAP documentation tools to search for ABAP, UI5, CAP, \n",
    "   and other SAP-related information.\n",
    "\n",
    "3. **Planning**: Break down complex tasks into manageable steps using the todo list.\n",
    "\n",
    "4. **File System**: Store important findings and notes for later reference.\n",
    "\n",
    "Guidelines:\n",
    "- Always search relevant knowledge sources before answering technical questions\n",
    "- Cite your sources when providing information\n",
    "- For complex questions, create a plan first\n",
    "- Be concise but thorough in your responses\n",
    "\"\"\"\n",
    "\n",
    "# Create the deep agent with all components\n",
    "agent = create_deep_agent(\n",
    "    model=llm,\n",
    "    tools=all_tools,\n",
    "    system_prompt=system_prompt,\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "print('[OK] Deep Agent created with:')\n",
    "print(f'  - LLM: Gemini 2.5 Flash')\n",
    "print(f'  - Tools: {len(all_tools)} ({len(mcp_tools)} MCP + 1 RAG)')\n",
    "print(f'  - Memory: MemorySaver checkpointer')\n",
    "print(f'  - Middleware: TodoList, Filesystem, SubAgent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper-md",
   "metadata": {},
   "source": [
    "## 11. Helper Function for Agent Interaction\n",
    "\n",
    "Create a helper to easily chat with the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "helper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def extract_text_content(content) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from various content formats.\n",
    "    \n",
    "    Handles:\n",
    "    - Plain strings\n",
    "    - List of content blocks (e.g., [{\"type\": \"text\", \"text\": \"...\"}])\n",
    "    - AIMessage with content attribute\n",
    "    \"\"\"\n",
    "    if content is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # If it's already a string, return as-is\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    \n",
    "    # If it's a list of content blocks (common with Gemini/Claude)\n",
    "    if isinstance(content, list):\n",
    "        text_parts = []\n",
    "        for block in content:\n",
    "            if isinstance(block, str):\n",
    "                text_parts.append(block)\n",
    "            elif isinstance(block, dict):\n",
    "                # Handle {\"type\": \"text\", \"text\": \"...\"} format\n",
    "                if block.get(\"type\") == \"text\":\n",
    "                    text_parts.append(block.get(\"text\", \"\"))\n",
    "                # Handle other dict formats\n",
    "                elif \"text\" in block:\n",
    "                    text_parts.append(block[\"text\"])\n",
    "                elif \"content\" in block:\n",
    "                    text_parts.append(str(block[\"content\"]))\n",
    "            else:\n",
    "                # Try to convert to string\n",
    "                text_parts.append(str(block))\n",
    "        return \"\\n\".join(text_parts)\n",
    "    \n",
    "    # Fallback: convert to string\n",
    "    return str(content)\n",
    "\n",
    "\n",
    "async def chat(message: str, thread_id: str = \"default\") -> str:\n",
    "    \"\"\"\n",
    "    Send a message to the agent and get a response.\n",
    "    \n",
    "    Args:\n",
    "        message: The user's message\n",
    "        thread_id: Conversation thread ID for memory continuity\n",
    "        \n",
    "    Returns:\n",
    "        The agent's response text\n",
    "    \"\"\"\n",
    "    response = await agent.ainvoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": message}]},\n",
    "        {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "    \n",
    "    # Extract the last message content\n",
    "    last_message = response[\"messages\"][-1]\n",
    "    \n",
    "    # Handle various content formats\n",
    "    return extract_text_content(last_message.content)\n",
    "\n",
    "\n",
    "def display_response(response):\n",
    "    \"\"\"Display the response as formatted markdown.\"\"\"\n",
    "    # Ensure response is a string\n",
    "    text = extract_text_content(response) if not isinstance(response, str) else response\n",
    "    \n",
    "    if text:\n",
    "        display(Markdown(text))\n",
    "    else:\n",
    "        print(\"[No response content]\")\n",
    "\n",
    "\n",
    "print('[OK] Helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1-md",
   "metadata": {},
   "source": [
    "## 12. Example Usage\n",
    "\n",
    "### Example 1: RAG Query (Internal Knowledge Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "example1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In LangGraph, particularly within the context of Deep Agents, the distinction between short-term and long-term memory is as follows:\n",
       "\n",
       "*   **Short-term memory** refers to the default filesystem used by tools like `ls`, `read_file`, `write_file`, and `edit_file`. This memory is local to the current graph state and is not persistent across different threads or conversations. Information stored here is lost once the agent's current execution or thread concludes. (Source: [2])\n",
       "\n",
       "*   **Long-term memory** enables agents to retain information persistently across multiple conversations and threads. This is achieved by configuring a `CompositeBackend` to route specific file paths (e.g., `/memories/`) to a `StoreBackend`, which then utilizes a `Store` (such as `InMemoryStore`). This setup allows agents to save and retrieve information from past interactions, ensuring data availability beyond the current session. (Source: [1], [2])"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query the internal knowledge base using RAG\n",
    "response1 = await chat(\n",
    "    \"What is the difference between short-term and long-term memory in LangGraph?\",\n",
    "    # \"How is the weather in Sao Paulo today?\",\n",
    "    thread_id=\"rag-demo\"\n",
    ")\n",
    "display_response(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2-md",
   "metadata": {},
   "source": [
    "### Example 2: MCP Query (SAP Documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query SAP documentation using MCP tools\n",
    "response2 = await chat(\n",
    "    \"How do I handle internal tables in modern ABAP? Give me a brief overview.\",\n",
    "    thread_id=\"mcp-demo\"\n",
    ")\n",
    "display_response(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example3-md",
   "metadata": {},
   "source": [
    "### Example 3: Memory Continuity Demo\n",
    "\n",
    "Demonstrate that the agent remembers previous messages within a thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "example3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello Alice! It's great to meet you. I can help you learn about AI agents. What specifically about AI agents are you interested in? Are there any particular concepts or aspects you'd like to explore first?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First message in a new thread\n",
    "response3a = await chat(\n",
    "    \"My name is Alice and I'm learning about AI agents.\",\n",
    "    thread_id=\"memory-demo\"\n",
    ")\n",
    "print(\"First response:\")\n",
    "display_response(response3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "example3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow-up response (should remember context):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Alice, and you were learning about AI agents."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Follow-up message in the same thread - agent should remember the name\n",
    "response3b = await chat(\n",
    "    \"What's my name? And what was I learning about?\",\n",
    "    thread_id=\"memory-demo\"  # Same thread ID!\n",
    ")\n",
    "print(\"Follow-up response (should remember context):\")\n",
    "display_response(response3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example4-md",
   "metadata": {},
   "source": [
    "### Example 4: Combined Query (RAG + MCP)\n",
    "\n",
    "The agent can use multiple tools in one query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined query that might use both RAG and MCP\n",
    "response4 = await chat(\n",
    "    \"First, explain what Deep Agents middleware provides (use internal knowledge), \"\n",
    "    \"then give me a quick tip about clean ABAP coding (use SAP docs).\",\n",
    "    thread_id=\"combined-demo\"\n",
    ")\n",
    "display_response(response4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-md",
   "metadata": {},
   "source": [
    "## 13. Direct Tool Testing\n",
    "\n",
    "Test tools directly without the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "tools-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG Tool Test ===\n",
      "[1] Source: langchain_mcp_curated | Topic: integration\n",
      "Model Context Protocol (MCP) is an open protocol that standardizes how applications provide \n",
      "        tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the \n",
      "        langchain-mcp-adapters library. MultiServerMCPClient enables agents to use tools defined across one or \n",
      "        more MCP servers. MCP servers can use different transports: stdio for local Python scripts, \n",
      "        streamable_http for remote HTTP servers. The MCP client is stateless by default - each tool invocation \n",
      "        creates a fresh MCP ClientSession, executes the tool, and then cleans up.\n",
      "\n",
      "[2] Source: mcp_curated | Topic: overview\n",
      "MCP (Model Context Protocol) is an open-source standard for connecting AI applications to external systems. \n",
      "        Using MCP, AI applications like Claude or ChatGPT can connect to data sources (e.g. local files, databases), \n",
      "        tools (e.g. search engines, calculators) and workflows (e.g. specialized prompts)—enabling them to access key \n",
      "        information and perform tasks. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a \n",
      "        standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications \n",
      "        to external systems. MCP follows a client-server architecture where an MCP host establishes connections to \n",
      "        one or more MCP servers. The key participants are: MCP Host (the AI application), MCP Client (maintains \n",
      "        connection to server), and MCP Server (provides context to clients).\n",
      "\n",
      "[3] Source: https://modelcontextprotocol.io/docs/getting-started/intro | Topic: general\n",
      "What is the Model Context Protocol (MCP)? - Model Context ProtocolSkip to main contentModel Context Protocol home pageSearch...⌘KBlogGitHubSearch...NavigationGet startedWhat is the Model Context Protocol (MCP)?DocumentationSpecificationCommunityGet startedWhat is MCP?About MCPArchitectureServersClientsVersioningDevelop with MCPConnect to local MCP serversConnect to remote MCP ServersBuild an MCP serverBuild an MCP clientSDKsSecurityDeveloper toolsMCP InspectorOn this pageWhat can MCP enable?Why does MCP matter?Start BuildingLearn moreGet startedWhat is the Model Context Protocol (MCP)?Copy pageCopy pageMCP (Model Context Protocol) is an open-source standard for connecting AI applications to external systems.\n",
      "Using MCP, AI applications like Claude or ChatGPT can connect to data sources (e.g. local files, databases), tools (e.g. search engines, calculators) and workflows (e.g. specialized prompts)—enabling them to access key information and perform tasks.\n",
      "Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications to external systems.\n",
      "\n",
      "​What can MCP enable?\n",
      "\n",
      "=== MCP Tool Test (search) ===\n",
      "{\"results\":[{\"id\":\"/sap-styleguides/clean-abap/CleanABAP#prefer-inline-to-up-front-declarations\",\"title\":\"Prefer inline to up-front declarations\",\"url\":\"https://github.com/SAP/styleguides/blob/main/\",\"snippet\":\"Prefer inline to up-front declarations\\n\\n> [Clean ABAP](#clean-abap) > [Content](#content) > [Variables](#variables) > [This section](#prefer-inline-to-up-front-declarations) If you follow these guidelines, your methods will become so short (3-5 statements) that declaring variables inline at first occurrence will look more natural METHOD d...\\n\\n/sap-styleguides/clean-abap/CleanABAP#prefer-inline-to-...\",\"score\":23.04421360706857,\"metadata\":{\"source\":\"sap-styleguides\",\"library\":\"/sap-styleguides\",\"bm25Score\":-21.739824157611856,\"rank\":1}},{\"id\":\"/abap-docs-latest/abenselect_inline_decl_abexa\",\"title\":\"SELECT, Inline Declarations\",\"url\":\"https://help.sap.com/doc/abapdocu_cp_index_htm/CLOUD/en-US/.html\",\"snippet\":\"SELECT, Inline Declarations\\n\\nThis example demonstrates inline de...\n"
     ]
    }
   ],
   "source": [
    "# Test RAG tool directly\n",
    "print(\"=== RAG Tool Test ===\")\n",
    "rag_result = search_knowledge_base.invoke({\"query\": \"What is MCP protocol?\"})\n",
    "print(rag_result)\n",
    "print()\n",
    "\n",
    "# Test MCP tool directly (if available)\n",
    "if mcp_tools:\n",
    "    print(\"=== MCP Tool Test (search) ===\")\n",
    "    # Find the search tool\n",
    "    search_tool = next((t for t in mcp_tools if t.name == 'search'), None)\n",
    "    if search_tool:\n",
    "        mcp_result = await search_tool.ainvoke({\"query\": \"ABAP inline declarations\"})\n",
    "        # Print first 1000 chars of result\n",
    "        print(str(mcp_result)[:1000] + '...' if len(str(mcp_result)) > 1000 else mcp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-md",
   "metadata": {},
   "source": [
    "## 14. Configuration Options\n",
    "\n",
    "### Additional MCP Servers\n",
    "\n",
    "You can add more MCP servers to expand the agent's capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adding more MCP servers\n",
    "example_mcp_config = \"\"\"\n",
    "# MCP Server Configuration Examples:\n",
    "\n",
    "mcp_servers = {\n",
    "    # Local stdio server (Python script)\n",
    "    \"math\": {\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": r\"C:\\\\App\\\\Anaconda\\\\python.exe\",\n",
    "        \"args\": [\"math_server.py\"],\n",
    "    },\n",
    "    \n",
    "    # Remote HTTP server\n",
    "    \"weather\": {\n",
    "        \"transport\": \"streamable_http\",\n",
    "        \"url\": \"https://weather-mcp.example.com/mcp\",\n",
    "    },\n",
    "    \n",
    "    # Server with authentication\n",
    "    \"github\": {\n",
    "        \"transport\": \"streamable_http\",\n",
    "        \"url\": \"https://github-mcp.example.com/mcp\",\n",
    "        \"headers\": {\n",
    "            \"Authorization\": \"Bearer YOUR_TOKEN\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # FastMCP server\n",
    "    \"fastmcp\": {\n",
    "        \"transport\": \"streamable_http\",\n",
    "        \"url\": \"https://gofastmcp.com/mcp\",\n",
    "    },\n",
    "}\n",
    "\"\"\"\n",
    "print(example_mcp_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27970016",
   "metadata": {},
   "source": [
    "## 15. Enhanced Agent with Long-Term Memory\n",
    "\n",
    "Now let's upgrade the agent with:\n",
    "1. **CompositeBackend**: Hybrid storage with ephemeral + persistent paths\n",
    "2. **StoreBackend**: Cross-thread persistent memory for `/memories/` path\n",
    "3. **StateBackend**: Ephemeral storage for temporary files\n",
    "\n",
    "This enables the agent to:\n",
    "- Save important learnings to `/memories/` that persist forever\n",
    "- Use temporary workspace files that are cleaned up per-thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb2f56ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Enhanced Deep Agent created with long-term memory:\n",
      "  - /memories/* -> Persistent (survives across threads)\n",
      "  - /workspace/* -> Ephemeral (per-thread only)\n",
      "  - Store: InMemoryStore (use PostgresStore for production)\n"
     ]
    }
   ],
   "source": [
    "# Import backends for long-term memory\n",
    "from deepagents.backends import CompositeBackend, StateBackend, StoreBackend\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Create the persistent store for cross-thread memory\n",
    "persistent_store = InMemoryStore()\n",
    "\n",
    "# Define the composite backend factory\n",
    "def make_hybrid_backend(runtime):\n",
    "    \"\"\"\n",
    "    Create a hybrid backend that:\n",
    "    - Routes /memories/* to persistent StoreBackend\n",
    "    - Routes everything else to ephemeral StateBackend\n",
    "    \"\"\"\n",
    "    return CompositeBackend(\n",
    "        default=StateBackend(runtime),  # Ephemeral storage (per-thread)\n",
    "        routes={\n",
    "            \"/memories/\": StoreBackend(runtime)  # Persistent storage (cross-thread)\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Enhanced system prompt that teaches the agent about memory management\n",
    "enhanced_system_prompt = \"\"\"\\\n",
    "You are an intelligent research assistant with access to multiple knowledge sources and persistent memory.\n",
    "\n",
    "Your capabilities:\n",
    "1. **Internal Knowledge Base (RAG)**: Use 'search_knowledge_base' to query information about \n",
    "   LangGraph, Deep Agents, MCP, memory systems, and RAG concepts.\n",
    "\n",
    "2. **External Tools (MCP)**: Use available MCP tools to search external documentation.\n",
    "\n",
    "3. **Planning**: Break down complex tasks into manageable steps using the todo list.\n",
    "\n",
    "4. **File System with Memory**:\n",
    "   - **Persistent memories** (/memories/): Write important learnings, user preferences, \n",
    "     and knowledge you want to remember across ALL conversations here.\n",
    "     Example: write_file(\"/memories/user_preferences.md\", \"User prefers concise answers...\")\n",
    "   \n",
    "   - **Temporary workspace** (/workspace/): Use for scratch files and temporary notes.\n",
    "     These are cleared when the conversation ends.\n",
    "\n",
    "Guidelines:\n",
    "- Always search relevant knowledge sources before answering technical questions\n",
    "- Cite your sources when providing information\n",
    "- For complex questions, create a plan first\n",
    "- **Important**: Save valuable learnings to /memories/ so you remember them in future conversations\n",
    "- Be concise but thorough in your responses\n",
    "\"\"\"\n",
    "\n",
    "# Create the enhanced deep agent with long-term memory\n",
    "enhanced_agent = create_deep_agent(\n",
    "    model=llm,\n",
    "    tools=all_tools,\n",
    "    system_prompt=enhanced_system_prompt,\n",
    "    checkpointer=checkpointer,  # For short-term thread memory\n",
    "    store=persistent_store,     # For long-term cross-thread memory\n",
    "    backend=make_hybrid_backend # Hybrid filesystem backend\n",
    ")\n",
    "\n",
    "print('[OK] Enhanced Deep Agent created with long-term memory:')\n",
    "print('  - /memories/* -> Persistent (survives across threads)')\n",
    "print('  - /workspace/* -> Ephemeral (per-thread only)')\n",
    "print('  - Store: InMemoryStore (use PostgresStore for production)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288760e",
   "metadata": {},
   "source": [
    "### Demo: Long-Term Memory Persistence\n",
    "\n",
    "Let's demonstrate how memories persist across different conversation threads:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5657195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Thread 1: Teaching the agent ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello Alice! I've saved your preferences to my memory. I'll do my best to provide concise, technical answers with code examples in our future conversations. How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper function for the enhanced agent\n",
    "async def chat_enhanced(message: str, thread_id: str = \"default\") -> str:\n",
    "    \"\"\"Chat with the enhanced agent that has long-term memory.\"\"\"\n",
    "    response = await enhanced_agent.ainvoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": message}]},\n",
    "        {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "    last_message = response[\"messages\"][-1]\n",
    "    return extract_text_content(last_message.content)\n",
    "\n",
    "# Step 1: First conversation - teach the agent something\n",
    "print(\"=== Thread 1: Teaching the agent ===\")\n",
    "response1 = await chat_enhanced(\n",
    "    \"My name is Alice and I'm a senior Python developer. \"\n",
    "    \"I prefer concise, technical answers with code examples. \"\n",
    "    \"Please remember this for our future conversations by saving it to your memories.\",\n",
    "    thread_id=\"thread-1\"\n",
    ")\n",
    "display_response(response1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5717e1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Thread 2: Testing memory recall (NEW conversation) ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I remember that your name is Alice, you are a Senior Python developer, and you prefer concise, technical answers with code examples."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Memory persistence verified if the agent recalled Alice's preferences ---\n"
     ]
    }
   ],
   "source": [
    "# Step 2: NEW thread - test if memory persists\n",
    "print(\"=== Thread 2: Testing memory recall (NEW conversation) ===\")\n",
    "response2 = await chat_enhanced(\n",
    "    \"What do you remember about me from your memories? \"\n",
    "    \"Check your /memories/ folder.\",\n",
    "    thread_id=\"thread-2\"  # Different thread!\n",
    ")\n",
    "display_response(response2)\n",
    "\n",
    "print(\"\\n--- Memory persistence verified if the agent recalled Alice's preferences ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44018205",
   "metadata": {},
   "source": [
    "## 16. Human-in-the-Loop (HITL) for Sensitive Operations\n",
    "\n",
    "Add human oversight for sensitive tool operations. The agent will pause and wait for approval before executing configured tools.\n",
    "\n",
    "**Decision Types:**\n",
    "- `approve`: Execute the tool as proposed\n",
    "- `edit`: Modify the tool arguments before execution  \n",
    "- `reject`: Cancel the operation with feedback\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Agent → Check Policy → [Interrupt] → Human Decision → Execute/Cancel\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f08a70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] HITL Agent created with approval requirements:\n",
      "  - write_file: Requires approval (approve/edit/reject)\n",
      "  - edit_file: Requires approval (approve/edit/reject)\n",
      "  - task (subagent): Requires approval (approve/reject only)\n",
      "  - All other tools: Auto-approved\n"
     ]
    }
   ],
   "source": [
    "# Import Command for resuming after interrupts\n",
    "from langgraph.types import Command\n",
    "\n",
    "# Create agent with HITL for file write operations\n",
    "# NOTE: create_deep_agent has a built-in `interrupt_on` parameter - no middleware needed!\n",
    "hitl_agent = create_deep_agent(\n",
    "    model=llm,\n",
    "    tools=all_tools,\n",
    "    system_prompt=enhanced_system_prompt,\n",
    "    checkpointer=checkpointer,  # REQUIRED for HITL to work!\n",
    "    store=persistent_store,\n",
    "    backend=make_hybrid_backend,\n",
    "    # Use the built-in interrupt_on parameter (NOT middleware!)\n",
    "    interrupt_on={\n",
    "        # Require approval for file writes (all decision types allowed)\n",
    "        \"write_file\": True,\n",
    "        \"edit_file\": True,\n",
    "        # Allow approve/reject only (no editing) for task tool\n",
    "        \"task\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n",
    "        # NOTE: Tools not listed here are auto-approved (ls, read_file, etc.)\n",
    "    }\n",
    ")\n",
    "\n",
    "print('[OK] HITL Agent created with approval requirements:')\n",
    "print('  - write_file: Requires approval (approve/edit/reject)')\n",
    "print('  - edit_file: Requires approval (approve/edit/reject)')\n",
    "print('  - task (subagent): Requires approval (approve/reject only)')\n",
    "print('  - All other tools: Auto-approved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9188cd8d",
   "metadata": {},
   "source": [
    "### Demo: Human-in-the-Loop Workflow\n",
    "\n",
    "When the agent tries to write a file, execution pauses for human approval:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c87874f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Requesting file write (will pause for approval) ===\n",
      "\n",
      "Agent: Write a file at /memories/demo.md with the content: HITL demo successful...\n",
      "Agent: Write a file at /memories/demo.md with the content: HITL demo successful...\n",
      "Agent: ...\n",
      "Agent: ...\n",
      "\n",
      "============================================================\n",
      "[INTERRUPT DETECTED]\n",
      "============================================================\n",
      "[INTERRUPT] ID: 802244a332e0dcf32049c19736722573\n",
      "[INTERRUPT] Tool execution requires approval\n",
      "\n",
      "Tool: write_file\n",
      "Args: {'file_path': '/memories/demo.md', 'content': 'HITL demo successful'}\n",
      "[INTERRUPT] Awaiting human decision...\n",
      "============================================================\n",
      "\n",
      "[OK] Found 1 interrupt(s) requiring approval\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Request an action that requires approval\n",
    "config = {\"configurable\": {\"thread_id\": \"hitl-demo-v2\"}}  # Fresh thread for new demo\n",
    "\n",
    "# Collect interrupt events\n",
    "interrupts = []\n",
    "print(\"=== Requesting file write (will pause for approval) ===\\n\")\n",
    "\n",
    "# CRITICAL: Use stream_mode=\"values\" to properly receive interrupt state\n",
    "# This is required by LangGraph to surface __interrupt__ in the step dictionary\n",
    "for step in hitl_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Write a file at /memories/demo.md with the content: HITL demo successful\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",  # REQUIRED for interrupt detection!\n",
    "):\n",
    "    # Check for messages in the step\n",
    "    if \"messages\" in step:\n",
    "        last_msg = step[\"messages\"][-1]\n",
    "        if hasattr(last_msg, 'content'):\n",
    "            content_preview = extract_text_content(last_msg.content)[:200]\n",
    "            print(f\"Agent: {content_preview}...\")\n",
    "    \n",
    "    # Check for interrupts - this is how LangGraph surfaces them\n",
    "    if \"__interrupt__\" in step:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"[INTERRUPT DETECTED]\")\n",
    "        print(\"=\"*60)\n",
    "        for interrupt in step[\"__interrupt__\"]:\n",
    "            interrupts.append(interrupt)\n",
    "            print(f\"[INTERRUPT] ID: {interrupt.id}\")\n",
    "            # Extract action requests from the interrupt\n",
    "            if hasattr(interrupt, 'value') and isinstance(interrupt.value, dict):\n",
    "                action_requests = interrupt.value.get(\"action_requests\", [])\n",
    "                for request in action_requests:\n",
    "                    description = request.get(\"description\", \"No description\")\n",
    "                    print(f\"[INTERRUPT] {description}\")\n",
    "            print(\"[INTERRUPT] Awaiting human decision...\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "if interrupts:\n",
    "    print(f\"\\n[OK] Found {len(interrupts)} interrupt(s) requiring approval\")\n",
    "else:\n",
    "    print(\"\\n[INFO] No interrupts - operation completed without requiring approval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51ac95a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Approving the pending operation ===\n",
      "\n",
      "Agent: ...\n",
      "Agent: ...\n",
      "Agent: Updated file /demo.md...\n",
      "Agent: I have successfully written the file `/memories/demo.md` with the content \"HITL demo successful\"....\n",
      "\n",
      "[OK] Operation completed after approval\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Resume with approval (run this after the interrupt above)\n",
    "if interrupts:\n",
    "    print(\"=== Approving the pending operation ===\\n\")\n",
    "    \n",
    "    # Create approval decisions for each interrupt\n",
    "    decisions = [{\"type\": \"approve\"} for _ in interrupts]\n",
    "    \n",
    "    # Resume execution with the approval decisions\n",
    "    # CRITICAL: Use Command with resume parameter to continue interrupted graph\n",
    "    for step in hitl_agent.stream(\n",
    "        Command(resume={\"decisions\": decisions}),\n",
    "        config,  # Same thread_id to resume the interrupted execution\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        if \"messages\" in step:\n",
    "            last_msg = step[\"messages\"][-1]\n",
    "            if hasattr(last_msg, 'content'):\n",
    "                content_preview = extract_text_content(last_msg.content)[:200]\n",
    "                print(f\"Agent: {content_preview}...\")\n",
    "        \n",
    "        # Check for any new interrupts during resume\n",
    "        if \"__interrupt__\" in step:\n",
    "            print(\"[INFO] Additional interrupt detected during resume\")\n",
    "            for interrupt in step[\"__interrupt__\"]:\n",
    "                print(f\"  ID: {interrupt.id}\")\n",
    "    \n",
    "    print(\"\\n[OK] Operation completed after approval\")\n",
    "else:\n",
    "    print(\"[INFO] No pending interrupts to approve\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a76dcd",
   "metadata": {},
   "source": [
    "## 17. FilesystemBackend: Local Disk Access (Optional)\n",
    "\n",
    "Give the agent direct access to your local filesystem. This is powerful but requires caution.\n",
    "\n",
    "**Warning**: Only use this in controlled environments. The agent will have read/write access to the specified root directory!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e77ae007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Created sandbox directory: C:\\Users\\pogawal\\AppData\\Local\\Temp\\deepagent_sandbox_x7pdmnlv\n",
      "[OK] Filesystem Agent created:\n",
      "  - /local/* -> C:\\Users\\pogawal\\AppData\\Local\\Temp\\deepagent_sandbox_x7pdmnlv\n",
      "  - /memories/* -> Persistent store\n",
      "  - All writes require HITL approval\n"
     ]
    }
   ],
   "source": [
    "from deepagents.backends import FilesystemBackend\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create a safe sandbox directory for demo\n",
    "sandbox_dir = tempfile.mkdtemp(prefix=\"deepagent_sandbox_\")\n",
    "print(f\"[OK] Created sandbox directory: {sandbox_dir}\")\n",
    "\n",
    "# Agent with local filesystem access (sandboxed)\n",
    "def make_local_backend(runtime):\n",
    "    \"\"\"\n",
    "    Backend with local disk access.\n",
    "    Combines:\n",
    "    - /local/ -> Real filesystem (sandboxed with virtual_mode)\n",
    "    - /memories/ -> Persistent store\n",
    "    - Default -> Ephemeral state\n",
    "    \"\"\"\n",
    "    return CompositeBackend(\n",
    "        default=StateBackend(runtime),\n",
    "        routes={\n",
    "            \"/memories/\": StoreBackend(runtime),\n",
    "            # CRITICAL: virtual_mode=True ensures paths are resolved relative to root_dir\n",
    "            # Without this, paths like /hello_world.txt would resolve to C:\\hello_world.txt on Windows\n",
    "            \"/local/\": FilesystemBackend(root_dir=sandbox_dir, virtual_mode=True)\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create filesystem-enabled agent\n",
    "fs_agent = create_deep_agent(\n",
    "    model=llm,\n",
    "    tools=all_tools,\n",
    "    system_prompt=\"\"\"\\\n",
    "You are a research assistant with access to:\n",
    "1. RAG knowledge base (search_knowledge_base)\n",
    "2. MCP external tools\n",
    "3. File system with three areas:\n",
    "   - /local/ - Real files on the local machine (sandboxed)\n",
    "   - /memories/ - Persistent cross-thread storage\n",
    "   - /workspace/ - Temporary per-thread storage\n",
    "\n",
    "Use /local/ for creating real files that persist on disk.\n",
    "\"\"\",\n",
    "    checkpointer=checkpointer,\n",
    "    store=persistent_store,\n",
    "    backend=make_local_backend,\n",
    "    # Use built-in interrupt_on parameter (NOT middleware!)\n",
    "    interrupt_on={\n",
    "        \"write_file\": True,  # Requires approval\n",
    "        \"edit_file\": True,   # Requires approval\n",
    "    }\n",
    ")\n",
    "\n",
    "print('[OK] Filesystem Agent created:')\n",
    "print(f'  - /local/* -> {sandbox_dir}')\n",
    "print('  - /memories/* -> Persistent store')\n",
    "print('  - All writes require HITL approval')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70914643",
   "metadata": {},
   "source": [
    "### Demo: Filesystem Backend in Action\n",
    "\n",
    "Let's test the filesystem-enabled agent by asking it to create a real file on disk.\n",
    "The file will be created in the sandboxed directory and will persist after the notebook closes.\n",
    "\n",
    "**IMPORTANT**: Before running the cells below:\n",
    "1. **Re-run Cell 47 above** to create a fresh `fs_agent` with the `virtual_mode=True` fix\n",
    "2. This will create a new sandbox directory and configure the agent correctly\n",
    "\n",
    "**Note**: This demo uses HITL, so you'll need to approve the write operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9833888d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Filesystem Backend Demo ===\n",
      "\n",
      "Sandbox directory: C:\\Users\\pogawal\\AppData\\Local\\Temp\\deepagent_sandbox_x7pdmnlv\n",
      "Current timestamp: 2025-12-18 13:49:19\n",
      "\n",
      "Agent: Create a file at /local/hello_world.txt with the following exact content:\n",
      "\n",
      "# Hello from Deep Agent!\n",
      "\n",
      "This file was created by a LangGraph Deep Agent w...\n",
      "Agent: Create a file at /local/hello_world.txt with the following exact content:\n",
      "\n",
      "# Hello from Deep Agent!\n",
      "\n",
      "This file was created by a LangGraph Deep Agent w...\n",
      "Agent: ...\n",
      "Agent: ...\n",
      "\n",
      "============================================================\n",
      "[FILESYSTEM WRITE INTERRUPT]\n",
      "============================================================\n",
      "[INTERRUPT] ID: 5877d2f77fc6abac622dc231b8af92e8\n",
      "[INTERRUPT] Tool execution requires approval\n",
      "\n",
      "Tool: write_file\n",
      "Args: {'file_path': '/local/hello_world.txt', 'content': '# Hello from Deep Agent!\\n\\nThis file was created by a LangGraph Deep Agent with FilesystemBackend.\\nIt demonstrates direct access to the local filesystem (sandboxed for safety).\\n\\nCreated at: 2025-12-18 13:49:19\\nAgent: Deep Agent with Gemini 2.5 Flash'}\n",
      "[INTERRUPT] Awaiting approval to write to local disk...\n",
      "============================================================\n",
      "\n",
      "[OK] Found 1 interrupt(s) requiring approval\n"
     ]
    }
   ],
   "source": [
    "# Demo: Create a real file using the filesystem backend\n",
    "# This file will persist on the actual disk in the sandbox directory\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# IMPORTANT: Verify the sandbox directory exists (Cell 47 must be run first!)\n",
    "if 'sandbox_dir' not in dir() or not os.path.exists(sandbox_dir):\n",
    "    print(\"[ERROR] sandbox_dir not defined or doesn't exist!\")\n",
    "    print(\"[ACTION REQUIRED] Please re-run Cell 47 (FilesystemBackend setup) first!\")\n",
    "    raise RuntimeError(\"Re-run Cell 47 first to create fs_agent with virtual_mode=True\")\n",
    "\n",
    "# Verify fs_agent exists\n",
    "if 'fs_agent' not in dir():\n",
    "    print(\"[ERROR] fs_agent not defined!\")\n",
    "    print(\"[ACTION REQUIRED] Please re-run Cell 47 (FilesystemBackend setup) first!\")\n",
    "    raise RuntimeError(\"Re-run Cell 47 first\")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": f\"filesystem-demo-{datetime.now().strftime('%H%M%S')}\"}}  # Fresh thread with timestamp\n",
    "\n",
    "# Get the actual current timestamp to include in the file\n",
    "current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Collect interrupt events\n",
    "fs_interrupts = []\n",
    "print(\"=== Filesystem Backend Demo ===\\n\")\n",
    "print(f\"Sandbox directory: {sandbox_dir}\")\n",
    "print(f\"Current timestamp: {current_timestamp}\\n\")\n",
    "\n",
    "# Ask the agent to create a real file with the actual timestamp\n",
    "file_content = f\"\"\"# Hello from Deep Agent!\n",
    "\n",
    "This file was created by a LangGraph Deep Agent with FilesystemBackend.\n",
    "It demonstrates direct access to the local filesystem (sandboxed for safety).\n",
    "\n",
    "Created at: {current_timestamp}\n",
    "Agent: Deep Agent with Gemini 2.5 Flash\n",
    "\"\"\"\n",
    "\n",
    "for step in fs_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": f\"Create a file at /local/hello_world.txt with the following exact content:\\n\\n{file_content}\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    # Check for messages\n",
    "    if \"messages\" in step:\n",
    "        last_msg = step[\"messages\"][-1]\n",
    "        if hasattr(last_msg, 'content'):\n",
    "            content_preview = extract_text_content(last_msg.content)[:150]\n",
    "            print(f\"Agent: {content_preview}...\")\n",
    "    \n",
    "    # Check for interrupts\n",
    "    if \"__interrupt__\" in step:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"[FILESYSTEM WRITE INTERRUPT]\")\n",
    "        print(\"=\"*60)\n",
    "        for interrupt in step[\"__interrupt__\"]:\n",
    "            fs_interrupts.append(interrupt)\n",
    "            print(f\"[INTERRUPT] ID: {interrupt.id}\")\n",
    "            if hasattr(interrupt, 'value') and isinstance(interrupt.value, dict):\n",
    "                action_requests = interrupt.value.get(\"action_requests\", [])\n",
    "                for request in action_requests:\n",
    "                    description = request.get(\"description\", \"Write operation pending\")\n",
    "                    print(f\"[INTERRUPT] {description}\")\n",
    "            print(\"[INTERRUPT] Awaiting approval to write to local disk...\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "if fs_interrupts:\n",
    "    print(f\"\\n[OK] Found {len(fs_interrupts)} interrupt(s) requiring approval\")\n",
    "else:\n",
    "    print(\"\\n[INFO] No interrupts - file may have been created automatically\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d31a7e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Approving filesystem write ===\n",
      "\n",
      "Agent: ...\n",
      "Agent: ...\n",
      "Agent: Updated file /hello_world.txt...\n",
      "Agent: The file `/local/hello_world.txt` has been successfully created with the specified content....\n",
      "\n",
      "[OK] Filesystem write approved\n",
      "\n",
      "=== Verifying file on disk ===\n",
      "Expected path: C:\\Users\\pogawal\\AppData\\Local\\Temp\\deepagent_sandbox_x7pdmnlv\\hello_world.txt\n",
      "[OK] File exists on disk!\n",
      "\n",
      "File contents:\n",
      "----------------------------------------\n",
      "# Hello from Deep Agent!\n",
      "\n",
      "This file was created by a LangGraph Deep Agent with FilesystemBackend.\n",
      "It demonstrates direct access to the local filesystem (sandboxed for safety).\n",
      "\n",
      "Created at: 2025-12-18 13:49:19\n",
      "Agent: Deep Agent with Gemini 2.5 Flash\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Approve the filesystem write and verify the file was created\n",
    "if fs_interrupts:\n",
    "    print(\"=== Approving filesystem write ===\\n\")\n",
    "    \n",
    "    decisions = [{\"type\": \"approve\"} for _ in fs_interrupts]\n",
    "    \n",
    "    for step in fs_agent.stream(\n",
    "        Command(resume={\"decisions\": decisions}),\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        if \"messages\" in step:\n",
    "            last_msg = step[\"messages\"][-1]\n",
    "            if hasattr(last_msg, 'content'):\n",
    "                content_preview = extract_text_content(last_msg.content)[:200]\n",
    "                print(f\"Agent: {content_preview}...\")\n",
    "    \n",
    "    print(\"\\n[OK] Filesystem write approved\")\n",
    "    \n",
    "    # Verify the file exists on disk\n",
    "    import os\n",
    "    expected_file = os.path.join(sandbox_dir, \"hello_world.txt\")\n",
    "    \n",
    "    print(f\"\\n=== Verifying file on disk ===\")\n",
    "    print(f\"Expected path: {expected_file}\")\n",
    "    \n",
    "    if os.path.exists(expected_file):\n",
    "        print(\"[OK] File exists on disk!\")\n",
    "        print(\"\\nFile contents:\")\n",
    "        print(\"-\" * 40)\n",
    "        with open(expected_file, 'r') as f:\n",
    "            print(f.read())\n",
    "        print(\"-\" * 40)\n",
    "    else:\n",
    "        print(\"[X] File not found - checking sandbox directory contents:\")\n",
    "        for item in os.listdir(sandbox_dir):\n",
    "            print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"[INFO] No pending filesystem interrupts to approve\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the sandbox directory using the agent\n",
    "# This demonstrates read operations (which don't require HITL approval)\n",
    "\n",
    "config_read = {\"configurable\": {\"thread_id\": \"filesystem-read-demo-v3\"}}\n",
    "\n",
    "print(\"=== Listing files via agent (no approval needed for reads) ===\\n\")\n",
    "\n",
    "for step in fs_agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"List all files in /local/ and show me what's in the hello_world.txt file if it exists.\"}]},\n",
    "    config_read,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    if \"messages\" in step:\n",
    "        last_msg = step[\"messages\"][-1]\n",
    "        if hasattr(last_msg, 'content'):\n",
    "            content = extract_text_content(last_msg.content)\n",
    "            # Show more content for this read demo\n",
    "            if len(content) > 50:  # Only show substantial responses\n",
    "                print(f\"Agent:\\n{content}\\n\")\n",
    "\n",
    "print(\"\\n=== Filesystem demo complete ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bfcfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Cleanup the sandbox directory\n",
    "# Uncomment to remove the sandbox and its contents after the demo\n",
    "\n",
    "# import shutil\n",
    "# if os.path.exists(sandbox_dir):\n",
    "#     shutil.rmtree(sandbox_dir)\n",
    "#     print(f\"[OK] Sandbox directory cleaned up: {sandbox_dir}\")\n",
    "# else:\n",
    "#     print(f\"[INFO] Sandbox directory already removed\")\n",
    "\n",
    "# For now, just show where the files are persisted\n",
    "print(f\"[INFO] Files persist at: {sandbox_dir}\")\n",
    "print(\"[INFO] To clean up manually, delete the sandbox directory or restart your computer.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d1c494",
   "metadata": {},
   "source": [
    "## 18. Subagents: Specialized Multi-Agent Architecture\n",
    "\n",
    "Deep Agents can spawn **subagents** to handle specialized tasks. This enables:\n",
    "- **Task Isolation**: Each subagent works in its own context\n",
    "- **Specialized Tools**: Different agents use different tool sets\n",
    "- **Parallel Execution**: Multiple subagents can work simultaneously\n",
    "- **Context Management**: Prevents context window overflow\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "                    +-------------------+\n",
    "                    |  Coordinator      |\n",
    "                    |  (Main Agent)     |\n",
    "                    +-------------------+\n",
    "                           |\n",
    "          +----------------+----------------+\n",
    "          |                |                |\n",
    "+---------v-----+  +-------v------+  +------v-------+\n",
    "| Research      |  | Finance      |  | Weather      |\n",
    "| Subagent      |  | Subagent     |  | Subagent     |\n",
    "| (RAG + News)  |  | (Stocks)     |  | (Weather)    |\n",
    "+---------------+  +--------------+  +--------------+\n",
    "```\n",
    "\n",
    "**Prerequisites**: Start the MCP servers before running this section:\n",
    "```bash\n",
    "# Terminal 1: Finance server\n",
    "cd utils/mcp/fastmcp && python finance_server.py\n",
    "\n",
    "# Terminal 2: News server  \n",
    "cd utils/mcp/fastmcp && python news_server.py\n",
    "\n",
    "# Terminal 3: Calculator server\n",
    "cd utils/mcp/fastmcp && python calculator_server.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aee6ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Connecting to Specialized MCP Servers ===\n",
      "\n",
      "[OK] Finance: 4 tools\n",
      "     - get_stock_quote\n",
      "     - get_company_info\n",
      "     - get_stock_history\n",
      "     - calculate_investment_return\n",
      "[OK] News: 4 tools\n",
      "     - get_top_headlines\n",
      "     - search_news\n",
      "     - search_wikipedia\n",
      "     - get_current_datetime\n",
      "[OK] Calculator: 3 tools\n",
      "     - calculate_expression\n",
      "     - convert_units\n",
      "     - calculate_statistics\n",
      "[OK] Weather: 1 tools\n",
      "     - get_weather\n",
      "\n",
      "[OK] Connected to available servers\n"
     ]
    }
   ],
   "source": [
    "# Connect to specialized MCP servers for subagent tools\n",
    "# IMPORTANT: Start the servers first (see instructions above)\n",
    "\n",
    "async def get_specialized_mcp_tools():\n",
    "    \"\"\"Connect to specialized MCP servers and organize tools by category.\"\"\"\n",
    "    tools_by_category = {}\n",
    "    \n",
    "    server_configs = {\n",
    "        \"finance\": {\"transport\": \"streamable_http\", \"url\": \"http://localhost:8001/mcp\"},\n",
    "        \"news\": {\"transport\": \"streamable_http\", \"url\": \"http://localhost:8002/mcp\"},\n",
    "        \"calculator\": {\"transport\": \"streamable_http\", \"url\": \"http://localhost:8003/mcp\"},\n",
    "        \"weather\": {\"transport\": \"streamable_http\", \"url\": \"http://localhost:8000/mcp\"},\n",
    "    }\n",
    "    \n",
    "    for category, config in server_configs.items():\n",
    "        try:\n",
    "            client = MultiServerMCPClient({category: config})\n",
    "            tools = await client.get_tools()\n",
    "            if tools:\n",
    "                tools_by_category[category] = tools\n",
    "                print(f\"[OK] {category.title()}: {len(tools)} tools\")\n",
    "                for t in tools:\n",
    "                    print(f\"     - {t.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[X] {category.title()}: Not available\")\n",
    "            tools_by_category[category] = []\n",
    "    \n",
    "    return tools_by_category\n",
    "\n",
    "print(\"=== Connecting to Specialized MCP Servers ===\\n\")\n",
    "specialized_tools = await get_specialized_mcp_tools()\n",
    "print(f\"\\n[OK] Connected to available servers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b9a30",
   "metadata": {},
   "source": [
    "### Create Specialized Subagents\n",
    "\n",
    "Each subagent has:\n",
    "1. **Specialized tools** - Only the tools relevant to its domain\n",
    "2. **Focused system prompt** - Clear instructions for its specific role\n",
    "3. **Independent context** - Isolated execution environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "670d7af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Subagent prompts defined\n"
     ]
    }
   ],
   "source": [
    "# Define specialized system prompts for each subagent\n",
    "\n",
    "RESEARCH_AGENT_PROMPT = \"\"\"You are a Research Specialist subagent.\n",
    "\n",
    "Your tools:\n",
    "- search_knowledge_base: Query internal documentation\n",
    "- search_wikipedia: Get encyclopedia information\n",
    "- get_top_headlines: Get current news headlines\n",
    "\n",
    "Your task: Research topics thoroughly using your tools.\n",
    "Be comprehensive but concise. Cite your sources.\n",
    "Return findings in a structured format.\"\"\"\n",
    "\n",
    "FINANCE_AGENT_PROMPT = \"\"\"You are a Finance Analyst subagent.\n",
    "\n",
    "Your tools:\n",
    "- get_stock_quote: Get current stock prices\n",
    "- get_company_info: Get company information\n",
    "- calculate_investment_return: Calculate investment returns\n",
    "\n",
    "Your task: Analyze financial data and provide insights.\n",
    "Be precise with numbers. Always include source data.\n",
    "Warn about investment risks when appropriate.\"\"\"\n",
    "\n",
    "WEATHER_AGENT_PROMPT = \"\"\"You are a Weather & Time Specialist subagent.\n",
    "\n",
    "Your tools:\n",
    "- get_weather: Get weather for any location\n",
    "- get_current_datetime: Get current date/time in any timezone\n",
    "- convert_units: Convert temperature and other units\n",
    "\n",
    "Your task: Provide weather information and time data.\n",
    "Include both metric and imperial units when relevant.\n",
    "Suggest appropriate clothing or activities based on weather.\"\"\"\n",
    "\n",
    "print(\"[OK] Subagent prompts defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bf6b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Subagents created:\n",
      "  - Research Agent: 5 tools\n",
      "  - Finance Agent: 4 tools\n",
      "  - Weather Agent: 4 tools\n"
     ]
    }
   ],
   "source": [
    "# Create specialized subagents with their respective tools\n",
    "\n",
    "# Collect tools for each subagent\n",
    "research_tools = [search_knowledge_base]  # RAG tool\n",
    "if specialized_tools.get(\"news\"):\n",
    "    research_tools.extend(specialized_tools[\"news\"])\n",
    "\n",
    "finance_tools = []\n",
    "if specialized_tools.get(\"finance\"):\n",
    "    finance_tools.extend(specialized_tools[\"finance\"])\n",
    "\n",
    "weather_tools = []\n",
    "if specialized_tools.get(\"weather\"):\n",
    "    weather_tools.extend(specialized_tools[\"weather\"])\n",
    "if specialized_tools.get(\"calculator\"):\n",
    "    weather_tools.extend(specialized_tools[\"calculator\"])\n",
    "\n",
    "# Create subagents\n",
    "subagent_checkpointer = MemorySaver()\n",
    "\n",
    "research_agent = create_deep_agent(\n",
    "    model=llm,\n",
    "    tools=research_tools,\n",
    "    system_prompt=RESEARCH_AGENT_PROMPT,\n",
    "    checkpointer=subagent_checkpointer,\n",
    ") if research_tools else None\n",
    "\n",
    "finance_agent = create_deep_agent(\n",
    "    model=llm,\n",
    "    tools=finance_tools,\n",
    "    system_prompt=FINANCE_AGENT_PROMPT,\n",
    "    checkpointer=subagent_checkpointer,\n",
    ") if finance_tools else None\n",
    "\n",
    "weather_agent = create_deep_agent(\n",
    "    model=llm,\n",
    "    tools=weather_tools,\n",
    "    system_prompt=WEATHER_AGENT_PROMPT,\n",
    "    checkpointer=subagent_checkpointer,\n",
    ") if weather_tools else None\n",
    "\n",
    "print(\"[OK] Subagents created:\")\n",
    "print(f\"  - Research Agent: {len(research_tools)} tools\")\n",
    "print(f\"  - Finance Agent: {len(finance_tools)} tools\")\n",
    "print(f\"  - Weather Agent: {len(weather_tools)} tools\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63621353",
   "metadata": {},
   "source": [
    "### Create Coordinator Agent\n",
    "\n",
    "The coordinator uses custom tools to delegate work to subagents.\n",
    "It analyzes user requests and routes them to the appropriate specialist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e67a915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Delegation tools created\n"
     ]
    }
   ],
   "source": [
    "# Create delegation tools for the coordinator agent\n",
    "\n",
    "@tool\n",
    "async def delegate_to_research(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Delegate a research task to the Research Specialist subagent.\n",
    "    Use for: documentation queries, news, wikipedia lookups.\n",
    "    \n",
    "    Args:\n",
    "        query: The research question or topic to investigate\n",
    "    \"\"\"\n",
    "    if not research_agent:\n",
    "        return \"Research agent not available (missing tools)\"\n",
    "    \n",
    "    response = await research_agent.ainvoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        {\"configurable\": {\"thread_id\": f\"research-{hash(query) % 10000}\"}}\n",
    "    )\n",
    "    return extract_text_content(response[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "@tool\n",
    "async def delegate_to_finance(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Delegate a financial analysis task to the Finance Analyst subagent.\n",
    "    Use for: stock prices, company info, investment calculations.\n",
    "    \n",
    "    Args:\n",
    "        query: The financial question or analysis request\n",
    "    \"\"\"\n",
    "    if not finance_agent:\n",
    "        return \"Finance agent not available (start finance_server.py first)\"\n",
    "    \n",
    "    response = await finance_agent.ainvoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        {\"configurable\": {\"thread_id\": f\"finance-{hash(query) % 10000}\"}}\n",
    "    )\n",
    "    return extract_text_content(response[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "@tool\n",
    "async def delegate_to_weather(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Delegate a weather/time task to the Weather Specialist subagent.\n",
    "    Use for: weather forecasts, time zones, unit conversions.\n",
    "    \n",
    "    Args:\n",
    "        query: The weather or time-related question\n",
    "    \"\"\"\n",
    "    if not weather_agent:\n",
    "        return \"Weather agent not available (start weather_server.py first)\"\n",
    "    \n",
    "    response = await weather_agent.ainvoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        {\"configurable\": {\"thread_id\": f\"weather-{hash(query) % 10000}\"}}\n",
    "    )\n",
    "    return extract_text_content(response[\"messages\"][-1].content)\n",
    "\n",
    "print(\"[OK] Delegation tools created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88a03be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Coordinator Agent created with 3 delegation tools\n"
     ]
    }
   ],
   "source": [
    "# Create the Coordinator Agent\n",
    "\n",
    "COORDINATOR_PROMPT = \"\"\"You are a Coordinator Agent managing a team of specialized subagents.\n",
    "\n",
    "Your team:\n",
    "1. **Research Specialist** (delegate_to_research): Documentation, news, Wikipedia\n",
    "2. **Finance Analyst** (delegate_to_finance): Stock prices, company info, investments\n",
    "3. **Weather Specialist** (delegate_to_weather): Weather, time zones, conversions\n",
    "\n",
    "Your role:\n",
    "- Analyze user requests and determine which specialist(s) to involve\n",
    "- Delegate tasks using the appropriate delegation tool\n",
    "- Synthesize results from multiple specialists if needed\n",
    "- Present a unified, coherent response to the user\n",
    "\n",
    "Guidelines:\n",
    "- For complex requests, break them down and delegate to multiple specialists\n",
    "- Always explain which specialist handled each part of the request\n",
    "- If a specialist is unavailable, inform the user gracefully\"\"\"\n",
    "\n",
    "coordinator_tools = [delegate_to_research, delegate_to_finance, delegate_to_weather]\n",
    "\n",
    "coordinator_agent = create_deep_agent(\n",
    "    model=llm,\n",
    "    tools=coordinator_tools,\n",
    "    system_prompt=COORDINATOR_PROMPT,\n",
    "    checkpointer=MemorySaver(),\n",
    ")\n",
    "\n",
    "print(\"[OK] Coordinator Agent created with 3 delegation tools\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e30c0",
   "metadata": {},
   "source": [
    "### Demo: Multi-Agent Coordination\n",
    "\n",
    "Let's see the coordinator agent delegate tasks to specialists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0a6a132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Coordinator helper ready\n"
     ]
    }
   ],
   "source": [
    "# Helper function for coordinator agent\n",
    "async def ask_coordinator(question: str, thread_id: str = \"coordinator-demo\") -> str:\n",
    "    \"\"\"Send a question to the coordinator agent.\"\"\"\n",
    "    response = await coordinator_agent.ainvoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
    "        {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "    return extract_text_content(response[\"messages\"][-1].content)\n",
    "\n",
    "print(\"[OK] Coordinator helper ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467eda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: Single specialist delegation (Research)\n",
    "print(\"=== Demo 1: Research Specialist ===\\n\")\n",
    "\n",
    "response1 = await ask_coordinator(\n",
    "    \"What is the Model Context Protocol (MCP)? Search the knowledge base.\",\n",
    "    thread_id=\"demo-research\"\n",
    ")\n",
    "display_response(response1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d062f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Single specialist delegation (Finance)\n",
    "# NOTE: Requires finance_server.py running on port 8001\n",
    "print(\"=== Demo 2: Finance Specialist ===\\n\")\n",
    "\n",
    "response2 = await ask_coordinator(\n",
    "    \"Get me the current stock price for NVIDIA (NVDA) and Apple (AAPL).\",\n",
    "    thread_id=\"demo-finance\"\n",
    ")\n",
    "display_response(response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adda337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: Single specialist delegation (Weather)\n",
    "# NOTE: Requires weather_server.py running on port 8000\n",
    "print(\"=== Demo 3: Weather Specialist ===\\n\")\n",
    "\n",
    "response3 = await ask_coordinator(\n",
    "    \"What's the weather in Tokyo and what time is it there?\",\n",
    "    thread_id=\"demo-weather\"\n",
    ")\n",
    "display_response(response3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ff09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 4: Multi-specialist coordination\n",
    "# The coordinator should delegate to multiple specialists\n",
    "print(\"=== Demo 4: Multi-Specialist Coordination ===\\n\")\n",
    "\n",
    "response4 = await ask_coordinator(\n",
    "    \"\"\"I'm planning a business trip to London next week. Please help me with:\n",
    "    1. What's the current weather like in London?\n",
    "    2. Get me the stock price for British Airways (IAG.L)\n",
    "    3. What do you know about deep agents from the knowledge base?\n",
    "    \n",
    "    Summarize all findings in a trip planning report.\"\"\",\n",
    "    thread_id=\"demo-multi\"\n",
    ")\n",
    "display_response(response4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464ab83",
   "metadata": {},
   "source": [
    "### Subagent Architecture Summary\n",
    "\n",
    "**How it works:**\n",
    "1. User sends request to **Coordinator Agent**\n",
    "2. Coordinator analyzes request and identifies required specialists\n",
    "3. Coordinator calls delegation tools (`delegate_to_research`, etc.)\n",
    "4. Each delegation tool invokes the corresponding **Subagent**\n",
    "5. Subagent uses its specialized MCP tools to gather information\n",
    "6. Results flow back to Coordinator\n",
    "7. Coordinator synthesizes and presents unified response\n",
    "\n",
    "**Benefits:**\n",
    "- **Specialization**: Each agent is optimized for its domain\n",
    "- **Isolation**: Subagents work in separate contexts\n",
    "- **Scalability**: Easy to add new specialists\n",
    "- **Maintainability**: Clear separation of concerns\n",
    "\n",
    "**Production considerations:**\n",
    "- Use persistent checkpointers (PostgresSaver) for subagent memory\n",
    "- Add HITL for sensitive subagent operations\n",
    "- Implement retry logic and error handling\n",
    "- Consider parallel execution for independent subagent tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54284407",
   "metadata": {},
   "source": [
    "### Optional: Test Subagents Directly\n",
    "\n",
    "You can also interact with individual subagents directly for debugging or specialized tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d222ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct subagent test - Research Agent\n",
    "async def test_subagent(agent, name: str, query: str):\n",
    "    \"\"\"Test a subagent directly.\"\"\"\n",
    "    if not agent:\n",
    "        print(f\"[X] {name} not available\")\n",
    "        return\n",
    "    \n",
    "    print(f\"=== Testing {name} ===\\n\")\n",
    "    response = await agent.ainvoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        {\"configurable\": {\"thread_id\": f\"test-{name.lower()}\"}}\n",
    "    )\n",
    "    result = extract_text_content(response[\"messages\"][-1].content)\n",
    "    display_response(result)\n",
    "\n",
    "# Test the research agent directly\n",
    "await test_subagent(\n",
    "    research_agent, \n",
    "    \"Research Agent\", \n",
    "    \"Search the knowledge base for information about MCP middleware\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Finance Agent directly (requires finance_server.py on port 8001)\n",
    "await test_subagent(\n",
    "    finance_agent,\n",
    "    \"Finance Agent\", \n",
    "    \"Get the current stock price for Microsoft (MSFT)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ad89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Weather Agent directly (requires weather_server.py on port 8000)\n",
    "await test_subagent(\n",
    "    weather_agent,\n",
    "    \"Weather Agent\", \n",
    "    \"What's the weather in New York and convert 72 Fahrenheit to Celsius\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfbc658",
   "metadata": {},
   "source": [
    "### Starting the MCP Servers\n",
    "\n",
    "Run these commands in separate terminals to start all MCP servers:\n",
    "\n",
    "```powershell\n",
    "# Terminal 1: Weather Server (port 8000)\n",
    "cd C:\\Users\\pogawal\\WorkFolder\\Documents\\Python\\ai-dev-agent\\utils\\mcp\\fastmcp\n",
    "C:\\App\\Anaconda\\python.exe weather_server.py\n",
    "\n",
    "# Terminal 2: Finance Server (port 8001)\n",
    "cd C:\\Users\\pogawal\\WorkFolder\\Documents\\Python\\ai-dev-agent\\utils\\mcp\\fastmcp\n",
    "C:\\App\\Anaconda\\python.exe finance_server.py\n",
    "\n",
    "# Terminal 3: News Server (port 8002)\n",
    "cd C:\\Users\\pogawal\\WorkFolder\\Documents\\Python\\ai-dev-agent\\utils\\mcp\\fastmcp\n",
    "C:\\App\\Anaconda\\python.exe news_server.py\n",
    "\n",
    "# Terminal 4: Calculator Server (port 8003)\n",
    "cd C:\\Users\\pogawal\\WorkFolder\\Documents\\Python\\ai-dev-agent\\utils\\mcp\\fastmcp\n",
    "C:\\App\\Anaconda\\python.exe calculator_server.py\n",
    "```\n",
    "\n",
    "After starting the servers, re-run Cell 54 to connect to them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2304b5e3",
   "metadata": {},
   "source": [
    "## 19. Direct Tool Testing: News Server\n",
    "\n",
    "Test the news server tools directly without LLM usage.\n",
    "This imports the tool functions directly and calls them to verify the APIs work.\n",
    "\n",
    "**Requirements:**\n",
    "```bash\n",
    "pip install newsapi-python wikipedia\n",
    "```\n",
    "\n",
    "**Environment:**\n",
    "- `NEWS_API_KEY` must be set (get free key at https://newsapi.org)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c368d5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] News server module imported successfully\n",
      "\n",
      "Available tools:\n",
      "  - get_top_headlines(category, country, query, page_size)\n",
      "  - search_news(query, page_size, sort_by, language)\n",
      "  - search_wikipedia(query, sentences)\n",
      "  - get_wikipedia_page(title)\n",
      "  - get_current_datetime(timezone)\n",
      "  - get_news_sources(category, country)\n"
     ]
    }
   ],
   "source": [
    "# Direct import test - verify the news server module works without MCP\n",
    "# This tests the underlying functions, not the MCP wrapper\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, r\"C:\\Users\\pogawal\\WorkFolder\\Documents\\Python\\ai-dev-agent\")\n",
    "\n",
    "# Import the tools directly from the news server module\n",
    "from utils.mcp.fastmcp.news_server import (\n",
    "    get_top_headlines,\n",
    "    search_news,\n",
    "    search_wikipedia,\n",
    "    get_wikipedia_page,\n",
    "    get_current_datetime,\n",
    "    get_news_sources\n",
    ")\n",
    "\n",
    "print(\"[OK] News server module imported successfully\")\n",
    "print(\"\\nAvailable tools:\")\n",
    "print(\"  - get_top_headlines(category, country, query, page_size)\")\n",
    "print(\"  - search_news(query, page_size, sort_by, language)\")\n",
    "print(\"  - search_wikipedia(query, sentences)\")\n",
    "print(\"  - get_wikipedia_page(title)\")\n",
    "print(\"  - get_current_datetime(timezone)\")\n",
    "print(\"  - get_news_sources(category, country)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29395576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test: get_current_datetime ===\n",
      "\n",
      "Current Date/Time (UTC):\n",
      "Date: Thursday, December 18, 2025\n",
      "Time: 03:00:23 PM\n",
      "ISO: 2025-12-18T15:00:23\n",
      "\n",
      "Current Date/Time (CET):\n",
      "Date: Thursday, December 18, 2025\n",
      "Time: 04:00:23 PM\n",
      "ISO: 2025-12-18T16:00:23\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Date/Time (no API key required)\n",
    "print(\"=== Test: get_current_datetime ===\\n\")\n",
    "\n",
    "result = get_current_datetime(\"UTC\")\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "result = get_current_datetime(\"CET\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3180c70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test: search_wikipedia ===\n",
      "\n",
      "Wikipedia: Python (programming language)\n",
      "\n",
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-collected.\n",
      "\n",
      "Read more: https://en.wikipedia.org/wiki/Python_(programming_language)\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Wikipedia (no API key required)\n",
    "print(\"=== Test: search_wikipedia ===\\n\")\n",
    "\n",
    "result = search_wikipedia(\"Python programming language\", sentences=3)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb50f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test: get_wikipedia_page ===\n",
      "\n",
      "Wikipedia: Artificial intelligence\n",
      "\n",
      "Summary:\n",
      "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\n",
      "High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\n",
      "Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics.\n",
      "\n",
      "Sections: N/A\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/Artificial_intelligence\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Wikipedia page (no API key required)\n",
    "print(\"=== Test: get_wikipedia_page ===\\n\")\n",
    "\n",
    "result = get_wikipedia_page(\"Artificial intelligence\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "febaedcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test: get_top_headlines ===\n",
      "\n",
      "Top Technology Headlines (US):\n",
      "\n",
      "1. [IGN] Video Game Physical Software and Hardware Sales Just Had the Worst November in the U.S. Since 1995 - IGN\n",
      "   Last month, November, was a shockingly terrible month for video game sales in the U.S. While we traditionally think of N...\n",
      "   Published: 2025-12-17 14:11\n",
      "   https://www.ign.com/articles/video-game-physical-software-and-hardware-sales-just-had-the-worst-november-in-the-us-since-1995\n",
      "\n",
      "2. [Pinkbike.com] Product of the Year Nominees - Pinkbike\n",
      "   The products that raised the bar the highest in their own categories.\n",
      "   Published: 2025-12-17 12:00\n",
      "   https://www.pinkbike.com/news/2025-pinkbike-awards-product-of-the-year-nominees.html\n",
      "\n",
      "3. [IGN] Elden Ring: Nightreign Update 1.03.1 Is a Big One, Makes Key Balance Changes and Adds New Content for the Forsaken Hollows DLC - IGN\n",
      "   None\n",
      "   Published: 2025-12-17 11:44\n",
      "   https://www.ign.com/articles/elden-ring-nightreign-update-1031-is-a-big-one-makes-key-balance-changes-and-adds-new-content-for-the-forsaken-hollows-dlc\n"
     ]
    }
   ],
   "source": [
    "# Test 4: NewsAPI - Top Headlines (requires NEWS_API_KEY)\n",
    "import os\n",
    "print(\"=== Test: get_top_headlines ===\\n\")\n",
    "\n",
    "if os.environ.get(\"NEWS_API_KEY\"):\n",
    "    result = get_top_headlines(category=\"technology\", country=\"us\", page_size=3)\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"[X] NEWS_API_KEY not set in environment\")\n",
    "    print(\"    Set it with: os.environ['NEWS_API_KEY'] = 'your-key'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "527b77aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test: search_news ===\n",
      "\n",
      "News Search: 'artificial intelligence' (15326 total results, showing 3)\n",
      "\n",
      "1. [National Institutes of Health] Request for Information: Call for Input into NIH's Best Pharmaceuticals for Children Act (BPCA) Priorities for 2026 and Beyond\n",
      "   NIH Funding Opportunities and Notices in the NIH Guide for Grants and Contracts: Request for Information: Call for Input into NIH's Best Pharmaceutica...\n",
      "   Published: 2025-12-17 15:10\n",
      "   https://grants.nih.gov/grants/guide/notice-files/NOT-HD-25-004.html\n",
      "\n",
      "2. [Business Insider] How AI is inspiring companies to adopt skills-based hiring\n",
      "   AI is accelerating the shift toward skills-based hiring, and in the process reshaping how companies go about recruiting.\n",
      "   Published: 2025-12-17 15:10\n",
      "   https://www.businessinsider.com/ai-accelerating-trend-job-hires-college-degrees-matter-less-2025-12\n",
      "\n",
      "3. [MetroWest Daily News] Hudson MA High School placed in lockdown. What authorities are saying\n",
      "   Hudson High School was placed on lockdown on Dec. 17 after what authorities said was later deemed a non-credible threat.\n",
      "   Published: 2025-12-17 15:08\n",
      "   https://www.metrowestdailynews.com/story/news/crime/2025/12/17/threat-deemed-not-credible-after-hudson-ma-high-school-lockdown/87808786007/\n"
     ]
    }
   ],
   "source": [
    "# Test 5: NewsAPI - Search News (requires NEWS_API_KEY)\n",
    "print(\"=== Test: search_news ===\\n\")\n",
    "\n",
    "if os.environ.get(\"NEWS_API_KEY\"):\n",
    "    result = search_news(query=\"artificial intelligence\", page_size=3, sort_by=\"publishedAt\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"[X] NEWS_API_KEY not set - skipping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3713e759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test: get_news_sources ===\n",
      "\n",
      "News Sources:\n",
      "\n",
      "- Ars Technica (technology)\n",
      "  The PC enthusiast's resource. Power users and the tools they love, without compu...\n",
      "  https://arstechnica.com\n",
      "\n",
      "- Crypto Coins News (technology)\n",
      "  Providing breaking cryptocurrency news - focusing on Bitcoin, Ethereum, ICOs, bl...\n",
      "  https://www.ccn.com\n",
      "\n",
      "- Engadget (technology)\n",
      "  Engadget is a web magazine with obsessive daily coverage of everything new in ga...\n",
      "  https://www.engadget.com\n",
      "\n",
      "- Gruenderszene (technology)\n",
      "  Online-Magazin für Startups und die digitale Wirtschaft. News und Hintergründe z...\n",
      "  http://www.gruenderszene.de\n",
      "\n",
      "- Hacker News (technology)\n",
      "  Hacker News is a social news website focusing on computer science and entreprene...\n",
      "  https://news.ycombinator.com\n",
      "\n",
      "- Recode (technology)\n",
      "  Get the latest independent tech news, reviews and analysis from Recode with the ...\n",
      "  http://www.recode.net\n",
      "\n",
      "- T3n (technology)\n",
      "  Das Online-Magazin bietet Artikel zu den Themen E-Business, Social Media, Startu...\n",
      "  https://t3n.de\n",
      "\n",
      "- TechCrunch (technology)\n",
      "  TechCrunch is a leading technology media property, dedicated to obsessively prof...\n",
      "  https://techcrunch.com\n",
      "\n",
      "- TechCrunch (CN) (technology)\n",
      "  TechCrunch is a leading technology media property, dedicated to obsessively prof...\n",
      "  https://techcrunch.cn\n",
      "\n",
      "- TechRadar (technology)\n",
      "  The latest technology news and reviews, covering computing, home entertainment s...\n",
      "  https://www.techradar.com\n",
      "\n",
      "- The Next Web (technology)\n",
      "  The Next Web is one of the world’s largest online publications that delivers an ...\n",
      "  http://thenextweb.com\n",
      "\n",
      "- The Verge (technology)\n",
      "  The Verge covers the intersection of technology, science, art, and culture.\n",
      "  http://www.theverge.com\n",
      "\n",
      "- Wired (technology)\n",
      "  Wired is a monthly American magazine, published in print and online editions, th...\n",
      "  https://www.wired.com\n",
      "\n",
      "- Wired.de (technology)\n",
      "  Wired reports on how emerging technologies affect culture, the economy and polit...\n",
      "  https://www.wired.de\n",
      "\n",
      "==================================================\n",
      "[OK] News server testing complete!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 6: NewsAPI - Get Sources (requires NEWS_API_KEY)\n",
    "print(\"=== Test: get_news_sources ===\\n\")\n",
    "\n",
    "if os.environ.get(\"NEWS_API_KEY\"):\n",
    "    result = get_news_sources(category=\"technology\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"[X] NEWS_API_KEY not set - skipping\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"[OK] News server testing complete!\")\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
