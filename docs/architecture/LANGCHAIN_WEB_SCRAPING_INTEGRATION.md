# LangChain Web Scraping Integration

**Created:** 2025-10-10  
**Status:** âœ… Integrated into Research Swarm  
**Purpose:** Document LangChain web scraping and parsing capabilities

---

## ğŸ¯ Overview

Our **Web Research Swarm** uses LangChain's powerful document loaders and transformers for robust web scraping and content extraction.

---

## ğŸ“¦ LangChain Components Used

### **1. Document Loaders**

#### **AsyncHtmlLoader** âœ… (Currently Integrated)
```python
from langchain_community.document_loaders import AsyncHtmlLoader

# Load multiple URLs asynchronously
loader = AsyncHtmlLoader(["https://example.com/page1", "https://example.com/page2"])
docs = loader.load()
```

**Benefits:**
- âœ… Async loading for multiple URLs
- âœ… Efficient batch processing
- âœ… Built-in error handling
- âœ… Returns structured Document objects

#### **WebBaseLoader** (Alternative)
```python
from langchain_community.document_loaders import WebBaseLoader

# Simple single URL loading
loader = WebBaseLoader("https://example.com")
docs = loader.load()
```

**Benefits:**
- âœ… Simple API for single URLs
- âœ… BeautifulSoup integration
- âœ… CSS selector support

### **2. Document Transformers**

#### **Html2TextTransformer** âœ… (Currently Integrated)
```python
from langchain_community.document_transformers import Html2TextTransformer

# Transform HTML to clean markdown/text
html2text = Html2TextTransformer()
cleaned_docs = html2text.transform_documents(html_docs)
```

**Benefits:**
- âœ… Converts HTML to clean text
- âœ… Preserves structure (headers, lists)
- âœ… Removes scripts, styles, navigation
- âœ… Markdown output option

#### **BeautifulSoupTransformer** (Available)
```python
from langchain_community.document_transformers import BeautifulSoupTransformer

# Advanced HTML parsing with selectors
bs_transformer = BeautifulSoupTransformer()
transformed = bs_transformer.transform_documents(
    docs, 
    tags_to_extract=["article", "main", "div.content"]
)
```

**Benefits:**
- âœ… CSS selector-based extraction
- âœ… Flexible tag filtering
- âœ… Preserves semantic structure

### **3. Text Splitters**

#### **RecursiveCharacterTextSplitter** (Available for Integration)
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Smart text chunking
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]
)
chunks = splitter.split_documents(docs)
```

**Benefits:**
- âœ… Semantic-aware splitting
- âœ… Configurable chunk size
- âœ… Overlap for context preservation

---

## ğŸ—ï¸ Current Implementation

### **ContentParserAgent** (agents/research/content_parser_agent.py)

```python
async def _extract_raw_content(self, result: Dict) -> str:
    """Extract content using LangChain loaders."""
    
    if LANGCHAIN_PARSING_AVAILABLE:
        # Use LangChain's async HTML loader
        loader = AsyncHtmlLoader([url])
        docs = loader.load()
        
        # Transform HTML to clean text
        html2text = Html2TextTransformer()
        transformed_docs = html2text.transform_documents(docs)
        
        # Extract cleaned content
        content = transformed_docs[0].page_content
        return self._clean_content(content)
```

**Fallback Hierarchy:**
1. âœ… **LangChain** (AsyncHtmlLoader + Html2TextTransformer)
2. âš ï¸ **BeautifulSoup** (Direct HTML parsing)
3. ğŸ”„ **Simulated** (For testing without network)

---

## ğŸš€ Advanced Features (Available for Future Integration)

### **1. JavaScript-Heavy Sites**

#### **AsyncChromiumLoader** (Requires Playwright)
```python
from langchain_community.document_loaders import AsyncChromiumLoader

# Load JavaScript-rendered pages
loader = AsyncChromiumLoader(["https://spa-website.com"])
docs = loader.load()
```

**Setup:**
```bash
pip install playwright
playwright install
```

### **2. Sitemap Scraping**

#### **SitemapLoader**
```python
from langchain_community.document_loaders import SitemapLoader

# Scrape entire sitemap
loader = SitemapLoader("https://example.com/sitemap.xml")
docs = loader.load()
```

### **3. Recursive Crawling**

#### **RecursiveUrlLoader**
```python
from langchain_community.document_loaders import RecursiveUrlLoader

# Recursively crawl website
loader = RecursiveUrlLoader(
    "https://example.com",
    max_depth=2,
    extractor=lambda html: BeautifulSoup(html).get_text()
)
docs = loader.load()
```

---

## ğŸ“Š Integration Status

| Component | Status | File | Notes |
|-----------|--------|------|-------|
| AsyncHtmlLoader | âœ… Integrated | content_parser_agent.py | Async URL loading |
| Html2TextTransformer | âœ… Integrated | content_parser_agent.py | HTML to clean text |
| BeautifulSoupTransformer | ğŸ“‹ Available | - | For advanced parsing |
| RecursiveTextSplitter | ğŸ“‹ Available | - | For chunking long content |
| AsyncChromiumLoader | ğŸ“‹ Available | - | For JS-heavy sites |
| SitemapLoader | ğŸ“‹ Available | - | For sitemap scraping |
| RecursiveUrlLoader | ğŸ“‹ Available | - | For site crawling |

---

## ğŸ”§ Configuration

### **Install Full LangChain Web Tools**
```bash
# Core LangChain web scraping
pip install langchain-community

# HTML parsing
pip install beautifulsoup4 html2text lxml

# JavaScript rendering (optional)
pip install playwright
playwright install chromium

# Advanced scraping (optional)
pip install selenium
```

### **Environment Variables**
```bash
# Optional: API keys for commercial scrapers
OXYLABS_API_KEY=your_key_here
BRIGHTDATA_API_KEY=your_key_here

# User agent for polite scraping
USER_AGENT="Mozilla/5.0 (Research Bot)"
```

---

## ğŸ¯ Best Practices

### **1. Respectful Scraping**
- âœ… Check `robots.txt` before scraping
- âœ… Implement rate limiting (1-2 seconds between requests)
- âœ… Use meaningful User-Agent strings
- âœ… Cache results to avoid repeated requests

### **2. Error Handling**
```python
try:
    docs = loader.load()
except Exception as e:
    logger.error(f"Scraping failed: {e}")
    # Fallback to cached or simulated content
```

### **3. Content Quality**
- âœ… Remove navigation, ads, footers
- âœ… Extract main content only
- âœ… Preserve semantic structure
- âœ… Handle multiple encodings

### **4. Performance**
- âœ… Use async loaders for multiple URLs
- âœ… Batch process when possible
- âœ… Cache frequently accessed content
- âœ… Set reasonable timeouts

---

## ğŸ“š Resources

- [LangChain Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)
- [LangChain Web Scraping Guide](https://python.langchain.com/docs/use_cases/web_scraping/)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Playwright for Python](https://playwright.dev/python/)

---

## ğŸ”„ Next Steps

1. **Enhance WebSearchAgent** - Integrate real search APIs (Google, Bing, DuckDuckGo)
2. **Add JavaScript Support** - Integrate AsyncChromiumLoader for SPA sites
3. **Implement Caching** - Cache scraped content to reduce network calls
4. **Add Sitemap Support** - Enable full site scraping via sitemaps
5. **Quality Filters** - Implement content quality scoring and filtering

---

**Status:** âœ… LangChain web scraping successfully integrated into Research Swarm  
**Next:** Real search API integration and JavaScript rendering support

