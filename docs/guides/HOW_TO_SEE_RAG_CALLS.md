# How to See RAG Calls in Action

**Created:** 2025-01-08  
**Purpose:** Guide to visualizing multi-stage RAG retrieval  
**Audience:** Developers testing the RAG system

---

## ğŸ¯ **Quick Answer**

There are **3 ways** to see the RAG system's multi-stage retrieval:

1. âœ… **RAG UI Testing Page** (BEST for transparency)
2. âœ… **Agent Chat with Debug Mode** (Real-time visualization)
3. âœ… **LangSmith Dashboard** (Complete traces)

---

## ğŸ“Š **Method 1: RAG UI Testing Page** (RECOMMENDED)

### **Where:** `ğŸ§ª Testing & Evaluation` tab in RAG Management App

### **What You'll See:**

```
ğŸ” Transparency Report
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â±ï¸ Retrieval Time    ğŸ“„ Results Found    ğŸ” Search Type    âœ… Success
   425ms                   12              Multi-Stage       Yes

ğŸ’¬ Generated Response
[Your LLM-generated answer appears here]

ğŸ” Retrieved Context Details (Expanded)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Retrieved 12 context chunks:

Result #1
Content Preview: [First 300 chars of retrieved text...]
Score: 0.892
  Semantic: 0.85
  Keyword: 0.92
  Quality: 0.90
  Diversity: 0.88

Result #2
[... and so on ...]

âš™ï¸ Processing Pipeline (Expanded)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Searches Performed: 4

[Query Rewriting]
[Multi-Stage Retrieval]
[Deduplication]
[Multi-Signal Re-ranking]
[Position Optimization]
[LLM Generation]
```

### **How to Use:**

1. Start the RAG UI:
   ```bash
   streamlit run apps/rag_management_app.py --server.port 8510
   ```

2. Navigate to **"ğŸ§ª Testing & Evaluation"** tab

3. Enter a test query (e.g., "What is context engineering?")

4. Click **"ğŸš€ Run Test"**

5. Expand **"ğŸ” Retrieved Context Details"** to see all search results with scores

6. Expand **"âš™ï¸ Processing Pipeline"** to see which stages were executed

---

## ğŸ’¬ **Method 2: Agent Chat with Debug Mode**

### **Where:** `ğŸ’¬ Agent Chat` tab in RAG Management App

### **What You'll See:**

```
[User Message]
What is context engineering?

[Agent Response]
Context engineering is the art of...

ğŸ“Š Context Statistics
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸ Retrieval Time: 412ms
ğŸ“„ Results Found: 12
ğŸ” Search Type: Multi-Stage

ğŸ” Context Debug Panel (If Debug Mode ON)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ˆ Quality Metrics
  Average Relevance: 0.78
  Context Diversity: 0.85

ğŸ” Retrieval Details
  Total Searches: 4
  Total Results: 18 (deduplicated to 12)
  Query Variants: 3

ğŸ“Š Agent Statistics
  Searches Performed: 4
  Total Context Time: 0.412s
```

### **How to Use:**

1. Navigate to **"ğŸ’¬ Agent Chat"** tab

2. **Enable Debug Mode** (checkbox at top)

3. Adjust **Context Detail Level** slider to "Debug"

4. Ask your question

5. After response, you'll see:
   - Context statistics
   - Debug panel (if enabled)
   - Quality metrics
   - Full retrieval breakdown

---

## ğŸŒ **Method 3: LangSmith Dashboard**

### **Where:** [https://smith.langchain.com/](https://smith.langchain.com/)

### **Setup:**

Add to `.streamlit/secrets.toml`:
```toml
LANGCHAIN_TRACING_V2 = "true"
LANGCHAIN_API_KEY = "your-api-key"
LANGCHAIN_PROJECT = "ai-dev-agent-rag"
```

### **What You'll See in LangSmith:**

```
Trace: "ContextAwareAgent Query"
â”œâ”€ Input: "What is context engineering?"
â”œâ”€ Context Retrieval
â”‚  â”œâ”€ Query Rewriting (3 variants)
â”‚  â”œâ”€ Search #1: original query â†’ 5 results
â”‚  â”œâ”€ Search #2: variant 1 â†’ 5 results  
â”‚  â”œâ”€ Search #3: variant 2 â†’ 5 results
â”‚  â”œâ”€ Search #4: variant 3 â†’ 5 results
â”‚  â””â”€ Deduplication: 18 â†’ 12 unique results
â”œâ”€ LLM Call: ChatGoogleGenerativeAI
â”‚  â”œâ”€ Prompt: [Full prompt with context]
â”‚  â”œâ”€ Tokens: 1,245 input / 387 output
â”‚  â”œâ”€ Latency: 2,134ms
â”‚  â””â”€ Response: [Full response text]
â””â”€ Output: [Final answer]
```

### **How to Use:**

1. Verify LangSmith is enabled in **"âš™ï¸ System Settings"** page

2. Run any query in RAG UI

3. Go to [smith.langchain.com](https://smith.langchain.com/)

4. Click on your project: **"ai-dev-agent-rag"**

5. Click on the latest trace

6. Expand the execution tree to see:
   - All context retrieval calls
   - Full prompts with retrieved context
   - LLM responses
   - Token usage
   - Timing for each stage

---

## ğŸ” **What the Multi-Stage RAG Does**

When you ask a question, the system performs:

### **Stage 1: Query Rewriting** âœï¸
```python
Original: "What is context engineering?"
Variant 1: "Define context engineering in AI"
Variant 2: "Context engineering techniques and practices"
Variant 3: "How does context engineering work"
```

### **Stage 2: Multi-Search Retrieval** ğŸ”
```
Search with original query    â†’ 5 results
Search with variant 1         â†’ 5 results
Search with variant 2         â†’ 5 results
Search with variant 3         â†’ 5 results
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total retrieved: 20 results
```

### **Stage 3: Deduplication** ğŸ”€
```
20 results â†’ Check similarity
â†’ Remove duplicates
â†’ 12 unique results
```

### **Stage 4: Multi-Signal Re-ranking** ğŸ“Š
For each result, calculate:
- **Semantic Score** (0.0-1.0): Vector similarity
- **Keyword Score** (0.0-1.0): Query term matches
- **Quality Score** (0.0-1.0): Content completeness
- **Diversity Score** (0.0-1.0): Uniqueness from others

```python
Combined Score = (
    0.40 Ã— semantic_score +
    0.25 Ã— keyword_score +
    0.20 Ã— quality_score +
    0.15 Ã— diversity_score
)
```

### **Stage 5: Position Optimization** ğŸ¯
```
Top results â†’ Front of context
Medium results â†’ Middle
Lower results â†’ End or excluded
```

### **Stage 6: LLM Generation** ğŸ¤–
```
Prompt = System Message + Query + Ranked Context
â†’ Send to Gemini
â†’ Generate response
```

---

## ğŸ“ **Logs You'll See in Console**

When running the RAG system, you'll see these logs:

```bash
INFO - ğŸ”„ Query rewriting: 3 variants generated
INFO - ğŸ” ContextAwareAgent: Found 5 results for 'What is context engineering?'
INFO - ğŸ” ContextAwareAgent: Found 5 results for 'Define context engineering in AI'
INFO - ğŸ” ContextAwareAgent: Found 5 results for 'Context engineering techniques'
INFO - ğŸ” ContextAwareAgent: Found 5 results for 'How does context engineering work'
INFO - ğŸ”€ Deduplication: 20 â†’ 12 unique results
INFO - ğŸ“Š Multi-signal re-ranking: Top score 0.892, Avg 0.714
INFO - ğŸ¯ Final context prepared: 12 results, avg score 0.714
INFO - ğŸ¤– ContextAwareAgent: Executing task with LLM...
INFO - ğŸ¤– ContextAwareAgent: Task completed successfully
```

---

## ğŸ¯ **Quick Test Commands**

### **Test RAG UI:**
```bash
# Activate environment
C:\App\Anaconda\Scripts\activate.bat base

# Start RAG UI
streamlit run apps/rag_management_app.py --server.port 8510

# Navigate to: http://localhost:8510
# Go to "ğŸ§ª Testing & Evaluation" tab
# Enter query and click "Run Test"
```

### **Check Logs in Real-Time:**
```bash
# Windows PowerShell
Get-Content logs/agent.log -Wait -Tail 50

# Or view specific RAG logs
Get-Content logs/agent.log | Select-String "ContextAwareAgent"
```

---

## ğŸ¨ **Understanding the Scores**

### **High Relevance (0.7-1.0)** ğŸŸ¢
- Strong semantic match to query
- High keyword overlap
- Quality content
- Unique information

### **Medium Relevance (0.4-0.7)** ğŸŸ¡
- Moderate semantic match
- Some keyword overlap
- Acceptable quality
- May have some overlap with other results

### **Low Relevance (0.0-0.4)** ğŸ”´
- Weak semantic match
- Low keyword overlap
- Lower quality or incomplete
- Highly similar to other results

---

## âœ… **Verification Checklist**

To verify multi-stage RAG is working:

- [ ] See multiple searches in logs (usually 3-4)
- [ ] See "Query rewriting: X variants generated"
- [ ] See "Deduplication: X â†’ Y unique results"
- [ ] See "Multi-signal re-ranking" log
- [ ] See scoring breakdown in UI (Semantic, Keyword, Quality, Diversity)
- [ ] See "Searches Performed: 4" in context stats
- [ ] See varied content in retrieved results (not duplicates)

---

## ğŸš€ **Next Steps**

1. **Test with different queries** to see how retrieval adapts
2. **Enable Debug Mode** in Agent Chat for detailed stats
3. **Check LangSmith** for complete execution traces
4. **Build Golden Dataset** in Testing page for systematic evaluation

---

**Pro Tip:** Use the **Testing & Evaluation** page for initial development and debugging, then rely on **LangSmith** for production monitoring and optimization.

