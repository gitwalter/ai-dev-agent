# Meaningful Testing Rule

**CRITICAL**: All tests must validate real working productive code and meaningful functionality. No false positives, no superficial assertions, no tests that pass without validating actual working code.

## Core Requirements

### 1. Real Output Validation
**MANDATORY**: Tests must validate actual agent outputs, not just check for existence
```python
# CORRECT: Validates actual content and structure
def validate_requirements_analyst_output(agent_result):
    assert 'functional_requirements' in agent_result.output
    assert isinstance(agent_result.output['functional_requirements'], list)
    assert len(agent_result.output['functional_requirements']) > 0
    # Validate each requirement has meaningful content
    for req in agent_result.output['functional_requirements']:
        assert isinstance(req, dict)
        assert 'description' in req
        assert len(req['description'].strip()) > 10  # Meaningful description

# INCORRECT: Only checks existence
def validate_requirements_analyst_output(agent_result):
    assert hasattr(agent_result, 'output')  # Too superficial
    assert agent_result.output is not None  # No content validation
```

### 2. Content Quality Validation
**MANDATORY**: Tests must validate content quality, not just presence
```python
# CORRECT: Validates meaningful content
def validate_code_generator_output(agent_result):
    assert 'code_files' in agent_result.output
    assert len(agent_result.output['code_files']) > 0
    
    # Validate each code file has meaningful content
    for file_path, content in agent_result.output['code_files'].items():
        assert len(content.strip()) > 50  # Minimum meaningful size
        assert 'def ' in content or 'class ' in content  # Contains actual code
        assert 'import ' in content or 'from ' in content  # Has dependencies
        
        # Validate file structure
        if 'main' in file_path.lower() or 'app' in file_path.lower():
            assert 'if __name__' in content or 'def main' in content

# INCORRECT: Only checks file count
def validate_code_generator_output(agent_result):
    assert len(agent_result.output['code_files']) > 0  # No content validation
```

### 3. Agent-Specific Validation
**MANDATORY**: Each agent must have specific validation for its expected outputs

#### Requirements Analyst Validation
```python
def validate_requirements_analyst_output(agent_result):
    """Validate requirements analyst produces meaningful requirements."""
    output = agent_result.output
    
    # Validate functional requirements
    assert 'functional_requirements' in output
    requirements = output['functional_requirements']
    assert isinstance(requirements, list)
    assert len(requirements) >= 3  # Should have multiple requirements
    
    # Validate each requirement
    for req in requirements:
        assert isinstance(req, dict)
        assert 'id' in req or 'name' in req
        assert 'description' in req
        assert len(req['description']) > 20  # Meaningful description
        assert 'priority' in req or 'type' in req
        
    # Validate non-functional requirements if present
    if 'non_functional_requirements' in output:
        nfr = output['non_functional_requirements']
        assert isinstance(nfr, list)
        for req in nfr:
            assert len(req.get('description', '')) > 15
```

#### Code Generator Validation
```python
def validate_code_generator_output(agent_result):
    """Validate code generator produces working code."""
    output = agent_result.output
    
    # Validate code files
    assert 'code_files' in output
    code_files = output['code_files']
    assert len(code_files) >= 3  # Should have multiple files
    
    # Validate main application file
    main_files = [f for f in code_files.keys() 
                  if any(keyword in f.lower() for keyword in ['main', 'app', 'index', 'server'])]
    assert len(main_files) > 0, "Should have main application file"
    
    # Validate each code file
    for file_path, content in code_files.items():
        assert len(content.strip()) > 100  # Meaningful size
        assert 'import ' in content or 'from ' in content  # Has imports
        assert 'def ' in content or 'class ' in content  # Has functions/classes
        
        # Validate Python syntax indicators
        if file_path.endswith('.py'):
            assert 'def ' in content or 'class ' in content
            assert ':' in content  # Python syntax
            assert '(' in content and ')' in content  # Function calls/definitions
```

#### Test Generator Validation
```python
def validate_test_generator_output(agent_result):
    """Validate test generator produces meaningful tests."""
    output = agent_result.output
    
    # Validate test files
    assert 'test_files' in output
    test_files = output['test_files']
    assert len(test_files) > 0
    
    # Validate each test file
    for file_path, content in test_files.items():
        assert len(content.strip()) > 50
        assert 'test' in content.lower()  # Contains test indicators
        assert 'def test_' in content or 'class Test' in content  # Test functions/classes
        assert 'assert' in content  # Contains assertions
        
        # Validate test structure
        assert 'import ' in content or 'from ' in content  # Has imports
        assert 'pytest' in content or 'unittest' in content  # Uses testing framework
```

### 4. Integration Validation
**MANDATORY**: Tests must validate that agents work together correctly
```python
def validate_workflow_integration(result):
    """Validate that agents produce compatible outputs."""
    
    # Validate requirements -> architecture flow
    if 'requirements_analyst' in result.agent_results and 'architecture_designer' in result.agent_results:
        req_output = result.agent_results['requirements_analyst'].output
        arch_output = result.agent_results['architecture_designer'].output
        
        # Architecture should reference requirements
        arch_text = str(arch_output).lower()
        for req in req_output.get('functional_requirements', []):
            req_keywords = req.get('description', '').lower().split()[:3]
            assert any(keyword in arch_text for keyword in req_keywords), \
                f"Architecture should reference requirement: {req.get('description', '')}"
    
    # Validate architecture -> code flow
    if 'architecture_designer' in result.agent_results and 'code_generator' in result.agent_results:
        arch_output = result.agent_results['architecture_designer'].output
        code_output = result.agent_results['code_generator'].output
        
        # Code should use technologies from architecture
        if 'technology_stack' in arch_output.get('architecture_design', {}):
            tech_stack = arch_output['architecture_design']['technology_stack']
            code_text = ' '.join(code_output.get('code_files', {}).values()).lower()
            
            for tech in tech_stack:
                if isinstance(tech, dict):
                    tech_name = tech.get('name', '').lower()
                else:
                    tech_name = str(tech).lower()
                
                # Check if technology is used in code
                if tech_name in ['python', 'flask', 'django', 'fastapi', 'sqlite', 'postgresql']:
                    assert tech_name in code_text, f"Code should use {tech_name} from architecture"
```

### 5. File System Validation
**MANDATORY**: Tests must validate that files are actually saved and accessible
```python
def validate_file_system_output(result):
    """Validate that files are actually saved to filesystem."""
    
    # Check project directory exists
    project_path = Path("./generated_projects") / result.project_name
    assert project_path.exists(), f"Project directory should exist: {project_path}"
    
    # Check files are actually saved
    saved_files = list(project_path.rglob("*"))
    saved_files = [f for f in saved_files if f.is_file()]
    assert len(saved_files) > 0, "Should have saved files"
    
    # Validate file contents match expected
    for file_path in saved_files:
        assert file_path.stat().st_size > 0, f"File should not be empty: {file_path}"
        
        # Read and validate file content
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            assert len(content.strip()) > 0, f"File should have content: {file_path}"
            
            # Validate file type specific content
            if file_path.suffix == '.py':
                assert 'def ' in content or 'class ' in content, f"Python file should have functions/classes: {file_path}"
            elif file_path.suffix == '.md':
                assert '#' in content, f"Markdown file should have headers: {file_path}"
            elif file_path.suffix == '.txt':
                assert len(content.strip()) > 10, f"Text file should have meaningful content: {file_path}"
```

### 6. Error Detection Validation
**MANDATORY**: Tests must detect and fail on actual errors
```python
def validate_no_parsing_errors(result):
    """Validate that no parsing errors occurred."""
    
    # Check for fallback data usage
    for agent_name, agent_result in result.agent_results.items():
        if hasattr(agent_result, 'documentation') and agent_result.documentation:
            doc = agent_result.documentation
            if isinstance(doc, dict):
                assert not doc.get("fallback_data"), f"{agent_name} used fallback data"
                assert not doc.get("fallback_used"), f"{agent_name} used fallback data"
        
        # Check logs for parsing errors
        if hasattr(agent_result, 'logs') and agent_result.logs:
            for log_entry in agent_result.logs:
                if isinstance(log_entry, dict):
                    message = log_entry.get("message", "").lower()
                    error_indicators = [
                        "parsing failed", "output validation failed", 
                        "json parsing failed", "pydantic validation",
                        "invalid output structure", "fallback data"
                    ]
                    assert not any(indicator in message for indicator in error_indicators), \
                        f"{agent_name} has parsing error: {log_entry.get('message', '')}"
```

### 7. Performance Validation
**MANDATORY**: Tests must validate reasonable performance
```python
def validate_performance(result):
    """Validate that workflow completes in reasonable time."""
    
    # Total execution time should be reasonable
    assert result.total_execution_time < 300, f"Workflow took too long: {result.total_execution_time}s"
    assert result.total_execution_time > 5, f"Workflow completed too quickly (suspicious): {result.total_execution_time}s"
    
    # Individual agent times should be reasonable
    for agent_name, agent_result in result.agent_results.items():
        assert agent_result.execution_time < 60, f"{agent_name} took too long: {agent_result.execution_time}s"
        assert agent_result.execution_time > 1, f"{agent_name} completed too quickly: {agent_result.execution_time}s"
```

### 8. Test Structure Requirements

#### Required Test Components
```python
@pytest.mark.asyncio
async def test_complete_workflow():
    """Test complete workflow with meaningful validation."""
    
    # 1. Setup and execution
    result = await execute_workflow(project_context)
    
    # 2. Basic structure validation
    assert isinstance(result, WorkflowResult)
    assert result.status == WorkflowStatus.SUCCESS
    
    # 3. Agent output validation (MANDATORY)
    agent_validation_success = validate_agent_outputs(result)
    assert agent_validation_success, "Agent output validation failed"
    
    # 4. Integration validation (MANDATORY)
    integration_success = validate_workflow_integration(result)
    assert integration_success, "Workflow integration validation failed"
    
    # 5. File system validation (MANDATORY)
    filesystem_success = validate_file_system_output(result)
    assert filesystem_success, "File system validation failed"
    
    # 6. Error detection validation (MANDATORY)
    validate_no_parsing_errors(result)
    
    # 7. Performance validation (MANDATORY)
    validate_performance(result)
    
    # 8. Content quality validation (MANDATORY)
    validate_content_quality(result)
```

### 9. Forbidden Test Patterns

#### Superficial Assertions
```python
# FORBIDDEN: Only checking existence
assert hasattr(agent_result, 'output')
assert agent_result.output is not None

# FORBIDDEN: Only checking counts without content
assert len(result.code_files) > 0
assert len(result.test_files) > 0

# FORBIDDEN: Only checking file paths without content
assert 'main.py' in result.code_files
assert 'test_main.py' in result.test_files
```

#### False Positive Patterns
```python
# FORBIDDEN: Tests that always pass
def test_always_passes():
    assert True  # Always passes, no real validation

# FORBIDDEN: Tests that don't validate actual functionality
def test_workflow_runs():
    result = execute_workflow()
    assert result is not None  # Doesn't validate content

# FORBIDDEN: Tests that ignore errors
def test_workflow_ignores_errors():
    try:
        result = execute_workflow()
        assert result is not None
    except Exception:
        pass  # Ignores errors
```

### 10. Test Quality Metrics

#### Required Validation Coverage
- **Agent Output Validation**: 100% of agents must have specific output validation
- **Content Quality Validation**: 100% of generated files must have content validation
- **Integration Validation**: 100% of agent interactions must be validated
- **Error Detection**: 100% of error conditions must be detected
- **Performance Validation**: 100% of performance metrics must be validated

#### Test Failure Criteria
- Any agent produces invalid output structure
- Any generated file has insufficient content
- Any parsing error occurs
- Any integration between agents fails
- Performance is outside acceptable bounds
- Files are not properly saved to filesystem

## Enforcement

This rule is **ALWAYS APPLIED** and must be followed for all:
- System tests
- Integration tests
- Agent validation tests
- Workflow tests
- Performance tests
- File system tests

**Violations of this rule require immediate remediation and test rewriting.**

## Benefits

- **Real Validation**: Tests validate actual working code, not just existence
- **Error Detection**: Tests catch real errors and parsing failures
- **Quality Assurance**: Tests ensure meaningful content and functionality
- **Integration Verification**: Tests validate that agents work together
- **Performance Monitoring**: Tests ensure reasonable performance
- **No False Positives**: Tests only pass when code actually works

## Success Criteria

- **100% Agent Validation**: All agents have specific output validation
- **100% Content Validation**: All generated content is meaningfully validated
- **100% Integration Validation**: All agent interactions are validated
- **0% False Positives**: No tests pass without validating real functionality
- **100% Error Detection**: All error conditions are detected and reported
- **100% Performance Validation**: All performance metrics are validated

**Remember**: The goal is to ensure that tests validate real working productive code, not just check for superficial indicators of success.
description:
globs:
alwaysApply: true
---
