# Isolated Agent Testing and Parsing Error Resolution Rule

## Description
When an agent has parsing errors, create isolated tests with mocked inputs and systematically apply the parsing error analysis rule until the agent works correctly. Stop testing after the first parsing error to save time.

## Core Requirements

### 1. Isolated Agent Testing Process
**MANDATORY**: Create isolated tests for problematic agents
```python
# CORRECT: Isolated agent test with mocked inputs
def test_agent_name_isolated():
    """Test specific agent with mocked inputs to isolate parsing issues."""
    # Mock input state
    mock_state = {
        "project_requirements": "Create a simple calculator",
        "architecture_design": "Basic calculator with add/subtract",
        "generated_code": "def add(a, b): return a + b"
    }
    
    # Test agent in isolation
    agent = AgentNodeFactory.create_agent("agent_name")
    result = await agent.execute(mock_state)
    
    # Validate parsing success
    assert "parsing_error" not in result
    assert result.get("status") == "success"
```

### 2. Parsing Error Analysis Integration
**MANDATORY**: Apply parsing error analysis rule systematically
- Test different parser types (StrOutputParser, JsonOutputParser, PydanticOutputParser)
- Test different prompt formats (JSON, structured text, free text)
- Test different output schemas (strict vs flexible)
- Document which combinations work best

### 3. Early Termination on First Error
**MANDATORY**: Stop testing immediately after first parsing error
```python
# CORRECT: Stop on first parsing error
try:
    result = agent.execute(mock_state)
    if "parsing_error" in result or "fallback_used" in result:
        print(f"❌ Parsing error detected: {result.get('error_message')}")
        return False  # Stop testing immediately
except Exception as e:
    print(f"❌ Agent execution failed: {e}")
    return False  # Stop testing immediately
```

### 4. Systematic Parser-Prompt Testing
**MANDATORY**: Test combinations systematically
```python
# Test combinations in order of preference
parser_prompt_combinations = [
    ("StrOutputParser", "free_text_prompt"),
    ("JsonOutputParser", "json_prompt"), 
    ("PydanticOutputParser", "structured_prompt"),
    ("FlexibleOutputParser", "hybrid_prompt")
]

for parser_type, prompt_type in parser_prompt_combinations:
    success = test_agent_with_combination(agent_name, parser_type, prompt_type)
    if success:
        print(f"✅ Found working combination: {parser_type} + {prompt_type}")
        return True
    else:
        print(f"❌ Failed combination: {parser_type} + {prompt_type}")
```

## Implementation Guidelines

### 1. Test Structure
- Create isolated test files for each problematic agent
- Use mocked inputs that represent typical agent inputs
- Test one parser-prompt combination at a time
- Stop immediately on first parsing error
- Document successful combinations

### 2. Error Analysis Process
- Identify the specific parsing error type
- Analyze the LLM output vs expected schema
- Determine if issue is parser, prompt, or schema
- Test alternative combinations systematically
- Update prompts or schemas as needed

### 3. Success Criteria
- Agent executes without parsing errors
- Output matches expected schema
- No fallback data is used
- Test completes successfully
- Performance is acceptable

### 4. Documentation Requirements
- Document which parser-prompt combinations work
- Record why certain combinations failed
- Update agent configuration with working combination
- Update prompt database with optimized prompts
- Document lessons learned for future agents

## Testing Workflow

### Step 1: Identify Problematic Agent
- Run full workflow test to identify failing agents
- Note specific parsing error messages
- Create isolated test for the agent

### Step 2: Create Isolated Test
- Mock typical input state for the agent
- Test agent in isolation
- Stop on first parsing error

### Step 3: Apply Parsing Error Analysis
- Analyze the specific parsing error
- Test different parser types
- Test different prompt formats
- Test different output schemas

### Step 4: Find Optimal Combination
- Document which combinations work
- Select the best combination based on:
  - Reliability (no parsing errors)
  - Performance (fast execution)
  - Maintainability (simple implementation)
  - Flexibility (handles edge cases)

### Step 5: Update Configuration
- Update agent configuration with working combination
- Update prompt database with optimized prompts
- Update tests to use new configuration
- Validate the fix works

## Benefits

- **Efficiency**: Isolated testing is much faster than full workflow testing
- **Accuracy**: Can focus on specific parsing issues
- **Systematic**: Methodical approach to finding optimal combinations
- **Documentation**: Clear record of what works and why
- **Reusability**: Lessons learned can be applied to other agents

## Enforcement

This rule is **ALWAYS APPLIED** when:
- An agent has parsing errors in the full workflow test
- A new agent is being developed
- Parser or prompt changes are being tested
- Agent performance optimization is needed

**Violations of this rule require immediate remediation.**
description:
globs:
alwaysApply: true
---
