---
name: Isolated Agent Testing and Parsing Error Resolution Rule
description: When an agent has parsing errors, create isolated tests with mocked inputs and systematically apply the parsing error analysis rule until the agent works correctly. Stop testing after the first parsing error to save time.
globs:
alwaysApply: true
---

# Isolated Agent Testing and Parsing Error Resolution Rule

**CRITICAL**: When an agent has parsing errors, create isolated tests with mocked inputs and systematically apply the parsing error analysis rule until the agent works correctly. Stop testing after the first parsing error to save time.

## Core Requirements

### 1. Isolated Agent Testing Process
**MANDATORY**: Create isolated tests for problematic agents
```python
# CORRECT: Isolated agent test with mocked inputs
def test_agent_name_isolated():
    """Test specific agent with mocked inputs to isolate parsing issues."""
    # Mock input state
    mock_state = {
        "project_requirements": "Create a simple calculator",
        "architecture_design": "Basic calculator with add/subtract",
        "generated_code": "def add(a, b): return a + b"
    }
    
    # Test agent in isolation
    agent = AgentNodeFactory.create_agent("agent_name")
    result = await agent.execute(mock_state)
    
    # Validate parsing success
    assert "parsing_error" not in result
    assert result.get("status") == "success"
```

### 2. Parsing Error Analysis Integration
**MANDATORY**: Apply parsing error analysis rule systematically
- Test different parser types (StrOutputParser, JsonOutputParser, PydanticOutputParser)
- Test different prompt formats (JSON, structured text, free text)
- Test different output schemas (strict vs flexible)
- Document which combinations work best

### 3. Early Termination on First Error
**MANDATORY**: Stop testing immediately after first parsing error
```python
# CORRECT: Stop on first parsing error
try:
    result = agent.execute(mock_state)
    if "parsing_error" in result or "fallback_used" in result:
        print(f"❌ Parsing error detected: {result.get('error_message')}")
        return False  # Stop testing immediately
except Exception as e:
    print(f"❌ Agent execution failed: {e}")
    return False  # Stop testing immediately
```

### 4. Systematic Prompt-Parser Combination Testing
**MANDATORY**: Find the optimal prompt-parser combination for each agent task by systematically varying both prompt content and parsing logic

#### 4.1 Prompt Optimization Strategy
**CRITICAL**: Prompts must always be instructive and useful to get optimal results
```python
# Test different prompt instruction styles systematically
prompt_variations = [
    # Clear, specific instructions with examples
    {
        "style": "detailed_instructions",
        "characteristics": ["step-by-step guidance", "concrete examples", "clear constraints"]
    },
    # Task-focused with context awareness
    {
        "style": "context_aware",
        "characteristics": ["context-specific guidance", "role clarity", "output expectations"]
    },
    # Structured with format requirements
    {
        "style": "structured_format",
        "characteristics": ["explicit format requirements", "field definitions", "validation rules"]
    },
    # Flexible with fallback options
    {
        "style": "flexible_guidance",
        "characteristics": ["multiple output options", "graceful degradation", "adaptive instructions"]
    }
]

# Test different instruction clarity levels
instruction_clarity_levels = [
    "basic_instructions",      # Simple, direct instructions
    "detailed_instructions",   # Comprehensive with examples
    "expert_instructions",     # Advanced with best practices
    "adaptive_instructions"    # Context-sensitive instructions
]
```

#### 4.2 Parser Logic Variation Strategy
```python
# Test different parser types with their optimal use cases
parser_combinations = [
    # Simple text parsing for free-form outputs
    {
        "parser": "StrOutputParser",
        "best_for": ["code generation", "documentation", "narrative text"],
        "prompt_style": "detailed_instructions",
        "advantages": ["flexible", "no schema constraints", "handles complex outputs"]
    },
    # JSON parsing for structured data
    {
        "parser": "JsonOutputParser", 
        "best_for": ["requirements analysis", "architecture design", "structured reviews"],
        "prompt_style": "structured_format",
        "advantages": ["structured output", "easy validation", "consistent format"]
    },
    # Strict schema validation
    {
        "parser": "PydanticOutputParser",
        "best_for": ["data validation", "strict compliance", "type safety"],
        "prompt_style": "expert_instructions",
        "advantages": ["type safety", "validation", "clear contracts"]
    },
    # Hybrid approach for complex tasks
    {
        "parser": "FlexibleOutputParser",
        "best_for": ["multi-format outputs", "adaptive responses", "complex workflows"],
        "prompt_style": "adaptive_instructions",
        "advantages": ["multiple formats", "fallback options", "adaptive parsing"]
    }
]
```

#### 4.3 Systematic Testing Matrix
```python
def find_optimal_prompt_parser_combination(agent_name: str, task_type: str):
    """Find the best prompt-parser combination for a specific agent task."""
    
    # Define testing matrix based on task type
    if task_type == "code_generation":
        test_combinations = [
            ("StrOutputParser", "detailed_code_instructions"),
            ("StrOutputParser", "context_aware_code_instructions"),
            ("JsonOutputParser", "structured_code_format"),
            ("FlexibleOutputParser", "adaptive_code_instructions")
        ]
    elif task_type == "requirements_analysis":
        test_combinations = [
            ("JsonOutputParser", "structured_requirements_format"),
            ("PydanticOutputParser", "strict_requirements_schema"),
            ("JsonOutputParser", "detailed_requirements_instructions"),
            ("FlexibleOutputParser", "adaptive_requirements_instructions")
        ]
    # Add more task-specific combinations...
    
    best_combination = None
    best_score = 0
    
    for parser_type, prompt_style in test_combinations:
        # Test the combination
        result = test_agent_combination(agent_name, parser_type, prompt_style)
        
        # Score the combination based on multiple criteria
        score = evaluate_combination_quality(result)
        
        if score > best_score:
            best_score = score
            best_combination = (parser_type, prompt_style)
    
    return best_combination, best_score

def evaluate_combination_quality(result):
    """Evaluate the quality of a prompt-parser combination."""
    score = 0
    
    # Parsing success (40% weight)
    if result.get("parsing_success", False):
        score += 40
    
    # Output quality (30% weight)
    output_quality = assess_output_quality(result.get("output", ""))
    score += output_quality * 30
    
    # Performance (20% weight)
    performance_score = assess_performance(result.get("execution_time", 0))
    score += performance_score * 20
    
    # Maintainability (10% weight)
    maintainability_score = assess_maintainability(result.get("complexity", "high"))
    score += maintainability_score * 10
    
    return score
```

#### 4.4 Prompt Instruction Optimization
```python
def optimize_prompt_instructions(agent_type: str, task_context: str):
    """Create optimal prompt instructions for the agent task."""
    
    base_instructions = {
        "requirements_analyst": {
            "role": "Expert Requirements Analyst",
            "objective": "Analyze project requirements comprehensively",
            "output_format": "Structured JSON with detailed analysis",
            "key_instructions": [
                "Identify functional and non-functional requirements",
                "Prioritize requirements by business value",
                "Identify potential risks and constraints",
                "Provide clear, actionable recommendations"
            ]
        },
        "code_generator": {
            "role": "Expert Software Developer",
            "objective": "Generate high-quality, production-ready code",
            "output_format": "Clean, well-documented code with explanations",
            "key_instructions": [
                "Follow best practices and coding standards",
                "Include proper error handling and validation",
                "Write clear, maintainable code",
                "Provide implementation explanations"
            ]
        }
        # Add more agent types...
    }
    
    # Enhance with task-specific context
    enhanced_instructions = enhance_with_context(
        base_instructions.get(agent_type, {}),
        task_context
    )
    
    return enhanced_instructions
```

## Implementation Guidelines

### 1. Test Structure
- Create isolated test files for each problematic agent
- Use mocked inputs that represent typical agent inputs
- Test one parser-prompt combination at a time
- Stop immediately on first parsing error
- Document successful combinations

### 2. Error Analysis Process
- Identify the specific parsing error type
- Analyze the LLM output vs expected schema
- Determine if issue is parser, prompt, or schema
- Test alternative combinations systematically
- Update prompts or schemas as needed

### 2.1 Systematic Prompt-Parser Optimization Process
**MANDATORY**: Follow systematic approach to find optimal combinations
```python
def systematic_optimization_workflow(agent_name: str, task_type: str):
    """Systematic workflow to find optimal prompt-parser combination."""
    
    # Step 1: Analyze task requirements
    task_requirements = analyze_task_requirements(task_type)
    
    # Step 2: Generate prompt variations
    prompt_variations = generate_prompt_variations(agent_name, task_requirements)
    
    # Step 3: Select appropriate parser candidates
    parser_candidates = select_parser_candidates(task_type, task_requirements)
    
    # Step 4: Test combinations systematically
    results = []
    for prompt_var in prompt_variations:
        for parser_candidate in parser_candidates:
            result = test_combination(agent_name, prompt_var, parser_candidate)
            results.append({
                "prompt": prompt_var,
                "parser": parser_candidate,
                "result": result,
                "score": evaluate_result(result)
            })
    
    # Step 5: Select optimal combination
    optimal_combination = select_optimal_combination(results)
    
    return optimal_combination
```

### 2.2 Prompt Instruction Quality Criteria
**CRITICAL**: Ensure prompts are instructive and useful for optimal results
- **Clarity**: Instructions must be clear and unambiguous
- **Specificity**: Provide specific guidance for the task
- **Context**: Include relevant context and constraints
- **Examples**: Provide concrete examples when helpful
- **Constraints**: Define clear output format and validation rules
- **Adaptability**: Allow for context-specific adjustments

### 3. Success Criteria
- Agent executes without parsing errors
- Output matches expected schema
- No fallback data is used
- Test completes successfully
- Performance is acceptable

### 4. Documentation Requirements
- Document which parser-prompt combinations work
- Record why certain combinations failed
- Update agent configuration with working combination
- Update prompt database with optimized prompts
- Document lessons learned for future agents

## Testing Workflow

### Step 1: Identify Problematic Agent
- Run full workflow test to identify failing agents
- Note specific parsing error messages
- Create isolated test for the agent

### Step 2: Create Isolated Test
- Mock typical input state for the agent
- Test agent in isolation
- Stop on first parsing error

### Step 3: Apply Systematic Prompt-Parser Optimization
- Analyze task requirements and constraints
- Generate systematic prompt variations with instructive content
- Test different parser types with appropriate prompt styles
- Evaluate combinations based on multiple quality criteria
- Document successful combinations and their characteristics

### Step 4: Find Optimal Combination
- Document which combinations work
- Select the best combination based on:
  - Reliability (no parsing errors)
  - Performance (fast execution)
  - Maintainability (simple implementation)
  - Flexibility (handles edge cases)

### Step 5: Update Configuration
- Update agent configuration with working combination
- Update prompt database with optimized prompts
- Update tests to use new configuration
- Validate the fix works

## Benefits

- **Efficiency**: Isolated testing is much faster than full workflow testing
- **Accuracy**: Can focus on specific parsing issues
- **Systematic**: Methodical approach to finding optimal combinations
- **Documentation**: Clear record of what works and why
- **Reusability**: Lessons learned can be applied to other agents
- **Optimization**: Systematic approach ensures best prompt-parser combinations
- **Quality**: Focus on instructive prompts leads to better agent performance
- **Adaptability**: Task-specific optimization for different agent types

## Enforcement

This rule is **ALWAYS APPLIED** when:
- An agent has parsing errors in the full workflow test
- A new agent is being developed
- Parser or prompt changes are being tested
- Agent performance optimization is needed
- **Systematic prompt-parser optimization is required** for optimal agent performance
- **Instructive prompt quality** needs to be validated and improved
- **Task-specific optimization** is needed for different agent types

**Violations of this rule require immediate remediation.**
