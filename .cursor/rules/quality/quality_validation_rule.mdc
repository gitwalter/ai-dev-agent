---
description: "Never declare success, completion, or victory until ALL verification steps are completed and ALL tests are passing with evidence."
category: "quality-standards"
priority: "high"
alwaysApply: false
contexts: ["TESTING", "CODING", "AGILE", "DEFAULT"]
globs: ["**/*"]
tags: ["validation", "testing", "quality", "evidence", "verification"]
tier: "2"
---

# No Premature Victory Declaration Rule

**CRITICAL**: Never declare success, completion, or victory until ALL verification steps are completed and ALL tests are passing with evidence.

## Description
This rule prevents premature declarations of success, completion, or victory before proper validation and testing has been performed. All claims must be backed by concrete evidence.

## Core Requirements

### 1. Validation Before Declaration
**MANDATORY**: Never declare success without completing ALL validation steps
```bash
# FORBIDDEN: Declaring success without testing
echo "âœ… **EXCELLENT! Feature COMPLETE!**"  # NEVER do this without proof

# CORRECT: Test first, then declare
python -m pytest tests/ --tb=short
if [ $? -eq 0 ]; then
    echo "âœ… All tests passing - feature verified complete"
else
    echo "âŒ Tests still failing - work continues"
fi
```

### 2. Evidence-Based Claims
**MANDATORY**: All success claims must include concrete evidence
```bash
# REQUIRED EVIDENCE:
# - Test execution results (exit codes, pass/fail counts)
# - System status checks (health monitoring results)
# - Actual output verification (not assumed)
# - Performance metrics (response times, error rates)
# - Complete workflow execution (end-to-end validation)
```

### 3. Incremental Progress Reporting
**MANDATORY**: Report actual progress, not intended progress
```python
# CORRECT: Report actual status
def report_progress():
    test_results = run_tests()
    if test_results.failed > 0:
        return f"âŒ {test_results.failed} tests still failing - continuing work"
    else:
        return f"âœ… All {test_results.passed} tests passing - feature complete"

# FORBIDDEN: Assuming success
def report_progress():
    return "âœ… Feature complete!"  # Without testing
```

### 4. Success Criteria Definition
**MANDATORY**: Define clear, measurable success criteria before starting
```yaml
# REQUIRED: Define success criteria upfront
success_criteria:
  - "All tests pass (0 failures, 0 errors)"
  - "System health: 100% healthy agents"
  - "Performance: <3s response times"
  - "Code quality: No linting errors"
  - "Full workflow execution: End-to-end success"
```

### 5. Verification Commands
**MANDATORY**: Use these verification patterns before any success declaration
```bash
# Complete test suite verification
python -m pytest tests/ -v --tb=short --maxfail=5

# System health verification  
python scripts/health_monitor_service.py --check

# Code quality verification
python -m pylint src/ --errors-only
python -m flake8 src/ --count --max-line-length=120

# Performance verification
time python -c "import your_module; your_module.run_performance_test()"
```

## Forbidden Phrases

### ğŸš« **NEVER USE WITHOUT EVIDENCE:**
- "âœ… **EXCELLENT! [Feature] COMPLETE!**"
- "ğŸ‰ **SUCCESS!**"
- "Perfect! Everything is working!"
- "All issues are resolved!"
- "System is fully operational!"
- "100% success rate achieved!"
- "Mission accomplished!"
- "Feature ready for production!"

### âœ… **USE INSTEAD:**
- "âœ… Tests passing (127/127) - feature verified complete"
- "ğŸ¯ Progress: 95% complete, 1 test remaining"
- "âš ï¸ Partial success: main feature working, edge cases need fixes"
- "ğŸ“Š Current status: 6/7 agents healthy, investigating 1 failure"
- "ğŸ”„ Incremental progress: import errors fixed, validation errors remain"

## Implementation Guidelines

### 1. Test-Driven Validation
# Code example removed for brevity


### 2. Progress Tracking
# Code example removed for brevity


### 3. Rollback Mechanism
**MANDATORY**: If premature victory is declared, immediately correct it
# Code example removed for brevity


## Examples

### âŒ **WRONG: Premature Declaration**
# Code example removed for brevity


### âœ… **CORRECT: Evidence-Based Reporting**
# Code example removed for brevity


### 1. Code Review Requirements
- [ ] All success claims backed by test results
- [ ] No victory declarations without evidence
- [ ] Proper progress reporting with metrics
- [ ] Clear next steps if work remains

### 2. Testing Requirements
- [ ] Run full test suite before any completion claims
- [ ] Verify system health before operational claims
- [ ] Check performance metrics before performance claims
- [ ] Validate end-to-end workflows before workflow claims

### 3. Documentation Requirements
- [ ] Document exact test results
- [ ] Include error counts and pass rates
- [ ] Show system status outputs
- [ ] Record performance measurements

## Benefits

- **Improved Reliability**: No false claims about system status
- **Better Debugging**: Honest assessment reveals actual issues
- **Quality Assurance**: Forces proper validation before declaring success
- **Team Trust**: Accurate reporting builds confidence
- **Risk Reduction**: Prevents deployment of incomplete features

## Monitoring

### Track These Metrics:
- Success claims vs actual test results
- Time between declaration and discovered issues
- Frequency of rollbacks after premature claims
- Accuracy of progress reporting

### Success Indicators:
- 100% success claims backed by evidence
- Zero rollbacks due to premature declarations
- Accurate progress reporting matches actual status
- High team confidence in reported status

# No Premature Victory Declaration Rule

**CRITICAL**: Never declare success, completion, or victory until ALL verification steps are completed and ALL tests are passing with evidence.

## Description
This rule prevents premature declarations of success, completion, or victory before proper validation and testing has been performed. All claims must be backed by concrete evidence.

## Core Requirements

### 1. Validation Before Declaration
**MANDATORY**: Never declare success without completing ALL validation steps
```bash
# FORBIDDEN: Declaring success without testing
echo "âœ… **EXCELLENT! Feature COMPLETE!**"  # NEVER do this without proof

# CORRECT: Test first, then declare
python -m pytest tests/ --tb=short
if [ $? -eq 0 ]; then
    echo "âœ… All tests passing - feature verified complete"
else
    echo "âŒ Tests still failing - work continues"
fi
```

### 2. Evidence-Based Claims
**MANDATORY**: All success claims must include concrete evidence
```bash
# REQUIRED EVIDENCE:
# - Test execution results (exit codes, pass/fail counts)
# - System status checks (health monitoring results)
# - Actual output verification (not assumed)
# - Performance metrics (response times, error rates)
# - Complete workflow execution (end-to-end validation)
```

### 3. Incremental Progress Reporting
**MANDATORY**: Report actual progress, not intended progress
```python
# CORRECT: Report actual status
def report_progress():
    test_results = run_tests()
    if test_results.failed > 0:
        return f"âŒ {test_results.failed} tests still failing - continuing work"
    else:
        return f"âœ… All {test_results.passed} tests passing - feature complete"

# FORBIDDEN: Assuming success
def report_progress():
    return "âœ… Feature complete!"  # Without testing
```

### 4. Success Criteria Definition
**MANDATORY**: Define clear, measurable success criteria before starting
```yaml
# REQUIRED: Define success criteria upfront
success_criteria:
  - "All tests pass (0 failures, 0 errors)"
  - "System health: 100% healthy agents"
  - "Performance: <3s response times"
  - "Code quality: No linting errors"
  - "Full workflow execution: End-to-end success"
```

### 5. Verification Commands
**MANDATORY**: Use these verification patterns before any success declaration
```bash
# Complete test suite verification
python -m pytest tests/ -v --tb=short --maxfail=5

# System health verification  
python scripts/health_monitor_service.py --check

# Code quality verification
python -m pylint src/ --errors-only
python -m flake8 src/ --count --max-line-length=120

# Performance verification
time python -c "import your_module; your_module.run_performance_test()"
```

## Forbidden Phrases

### ğŸš« **NEVER USE WITHOUT EVIDENCE:**
- "âœ… **EXCELLENT! [Feature] COMPLETE!**"
- "ğŸ‰ **SUCCESS!**"
- "Perfect! Everything is working!"
- "All issues are resolved!"
- "System is fully operational!"
- "100% success rate achieved!"
- "Mission accomplished!"
- "Feature ready for production!"

### âœ… **USE INSTEAD:**
- "âœ… Tests passing (127/127) - feature verified complete"
- "ğŸ¯ Progress: 95% complete, 1 test remaining"
- "âš ï¸ Partial success: main feature working, edge cases need fixes"
- "ğŸ“Š Current status: 6/7 agents healthy, investigating 1 failure"
- "ğŸ”„ Incremental progress: import errors fixed, validation errors remain"

## Implementation Guidelines

### 1. Test-Driven Validation
# Code example removed for brevity


### 2. Progress Tracking
# Code example removed for brevity


### 3. Rollback Mechanism
**MANDATORY**: If premature victory is declared, immediately correct it
# Code example removed for brevity


## Examples

### âŒ **WRONG: Premature Declaration**
# Code example removed for brevity


### âœ… **CORRECT: Evidence-Based Reporting**
# Code example removed for brevity


### 1. Code Review Requirements
- [ ] All success claims backed by test results
- [ ] No victory declarations without evidence
- [ ] Proper progress reporting with metrics
- [ ] Clear next steps if work remains

### 2. Testing Requirements
- [ ] Run full test suite before any completion claims
- [ ] Verify system health before operational claims
- [ ] Check performance metrics before performance claims
- [ ] Validate end-to-end workflows before workflow claims

### 3. Documentation Requirements
- [ ] Document exact test results
- [ ] Include error counts and pass rates
- [ ] Show system status outputs
- [ ] Record performance measurements

## Benefits

- **Improved Reliability**: No false claims about system status
- **Better Debugging**: Honest assessment reveals actual issues
- **Quality Assurance**: Forces proper validation before declaring success
- **Team Trust**: Accurate reporting builds confidence
- **Risk Reduction**: Prevents deployment of incomplete features

## Monitoring

### Track These Metrics:
- Success claims vs actual test results
- Time between declaration and discovered issues
- Frequency of rollbacks after premature claims
- Accuracy of progress reporting

### Success Indicators:
- 100% success claims backed by evidence
- Zero rollbacks due to premature declarations
- Accurate progress reporting matches actual status
- High team confidence in reported status

