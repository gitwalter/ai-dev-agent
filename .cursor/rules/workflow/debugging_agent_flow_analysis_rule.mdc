---
description: "Auto-generated description for debugging_agent_flow_analysis_rule.mdc"
category: "workflow"
priority: "low"
alwaysApply: true
globs: ["**/*"]
tags: ['workflow']
tier: "2"
---

# Agent Flow Analysis Rule

---
description: "Agent workflow debugging using logs/agent.log as primary source for comprehensive flow analysis"
category: "debugging"
priority: "critical"
alwaysApply: true
globs: ["**/*.py", "logs/**/*.log", "workflow/**/*.py"]
tags: ["debugging", "agent-flow", "log-analysis", "workflow", "troubleshooting"]
---

# Agent Flow Analysis Rule

**CRITICAL**: Use agent.log as the primary source for debugging agent workflows and flow analysis. Implement comprehensive logging and analysis tools for troubleshooting agent interactions.

## Core Requirements

### 1. Agent Log Analysis
**MANDATORY**: Use agent.log as the primary debugging source for all agent workflow issues.

**Log Analysis Requirements**:
- **Primary Source**: Always check agent.log first for debugging
- **Comprehensive Logging**: Log all agent interactions and state changes
- **Structured Logging**: Use structured logging for easy parsing and analysis
- **Error Context**: Preserve full context for errors and failures
- **Flow Tracking**: Track complete agent workflow execution

**Implementation**:
```python
# ✅ CORRECT: Agent Log Analysis
import logging
import json
from datetime import datetime
from typing import Dict, List, Optional
import re
from pathlib import Path

class AgentLogAnalyzer:
    def __init__(self, log_path: str = "logs/agent.log"):
        self.log_path = Path(log_path)
        self.logger = logging.getLogger(__name__)
        self.flow_tracker = FlowTracker()
        self.error_analyzer = ErrorAnalyzer()
        self.performance_analyzer = PerformanceAnalyzer()
    
    def analyze_agent_flow(self, session_id: str = None) -> AgentFlowReport:
        """Analyze agent workflow from log file"""
        if not self.log_path.exists():
            raise FileNotFoundError(f"Agent log file not found: {self.log_path}")
        
        # Read and parse log entries
        log_entries = self.read_log_entries(session_id)
        
        # Analyze flow
        flow_analysis = self.flow_tracker.analyze_flow(log_entries)
        
        # Analyze errors
        error_analysis = self.error_analyzer.analyze_errors(log_entries)
        
        # Analyze performance
        performance_analysis = self.performance_analyzer.analyze_performance(log_entries)
        
        return AgentFlowReport(
            session_id=session_id,
            flow_analysis=flow_analysis,
            error_analysis=error_analysis,
            performance_analysis=performance_analysis,
            log_entries=log_entries
        )
    
    def read_log_entries(self, session_id: str = None) -> List[LogEntry]:
        """Read and parse log entries from agent.log"""
        entries = []
        
        try:
            with open(self.log_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        entry = self.parse_log_entry(line, line_num)
                        if entry and (not session_id or entry.session_id == session_id):
                            entries.append(entry)
                    except Exception as e:
                        self.logger.warning(f"Failed to parse log line {line_num}: {e}")
                        continue
            
            return entries
            
        except Exception as e:
            self.logger.error(f"Failed to read agent log: {e}")
            return []
    
    def parse_log_entry(self, line: str, line_num: int) -> Optional[LogEntry]:
        """Parse individual log entry"""
        # Expected format: timestamp level session_id agent_type message [json_data]
        pattern = r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) (\w+) \[(\w+)\] \[(\w+)\] (.+?)(?:\s+(\{.*\}))?$'
        
        match = re.match(pattern, line.strip())
        if not match:
            return None
        
        timestamp_str, level, session_id, agent_type, message, json_data = match.groups()
        
        try:
            timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S,%f')
            
            # Parse JSON data if present
            data = {}
            if json_data:
                data = json.loads(json_data)
            
            return LogEntry(
                timestamp=timestamp,
                level=level,
                session_id=session_id,
                agent_type=agent_type,
                message=message,
                data=data,
                line_number=line_num
            )
            
        except Exception as e:
            self.logger.warning(f"Failed to parse log entry on line {line_num}: {e}")
            return None
    
    def find_errors_in_session(self, session_id: str) -> List[LogEntry]:
        """Find all errors in a specific session"""
        log_entries = self.read_log_entries(session_id)
        return [entry for entry in log_entries if entry.level in ['ERROR', 'CRITICAL']]
    
    def get_agent_interactions(self, session_id: str) -> List[AgentInteraction]:
        """Get all agent interactions in a session"""
        log_entries = self.read_log_entries(session_id)
        interactions = []
        
        current_interaction = None
        for entry in log_entries:
            if entry.message.startswith("Agent interaction started"):
                if current_interaction:
                    interactions.append(current_interaction)
                
                current_interaction = AgentInteraction(
                    agent_type=entry.agent_type,
                    start_time=entry.timestamp,
                    session_id=entry.session_id
                )
            
            elif entry.message.startswith("Agent interaction completed"):
                if current_interaction:
                    current_interaction.end_time = entry.timestamp
                    current_interaction.success = "success" in entry.message.lower()
                    interactions.append(current_interaction)
                    current_interaction = None
        
        # Add last interaction if not completed
        if current_interaction:
            interactions.append(current_interaction)
        
        return interactions

class FlowTracker:
    def analyze_flow(self, log_entries: List[LogEntry]) -> FlowAnalysis:
        """Analyze agent workflow flow"""
        flow_analysis = FlowAnalysis()
        
        # Track agent sequence
        agent_sequence = []
        for entry in log_entries:
            if entry.message.startswith("Agent interaction started"):
                agent_sequence.append(entry.agent_type)
        
        flow_analysis.agent_sequence = agent_sequence
        
        # Analyze flow patterns
        flow_analysis.flow_patterns = self.identify_flow_patterns(agent_sequence)
        
        # Check for flow issues
        flow_analysis.flow_issues = self.detect_flow_issues(log_entries)
        
        # Calculate flow metrics
        flow_analysis.metrics = self.calculate_flow_metrics(log_entries)
        
        return flow_analysis
    
    def identify_flow_patterns(self, agent_sequence: List[str]) -> List[FlowPattern]:
        """Identify patterns in agent workflow"""
        patterns = []
        
        # Check for expected patterns
        expected_patterns = [
            ["requirements_analyst", "architecture_designer", "code_generator"],
            ["requirements_analyst", "architecture_designer", "test_generator"],
            ["code_generator", "test_generator", "code_reviewer"]
        ]
        
        for expected_pattern in expected_patterns:
            if self.sequence_contains_pattern(agent_sequence, expected_pattern):
                patterns.append(FlowPattern(
                    type="expected_pattern",
                    pattern=expected_pattern,
                    description="Expected agent workflow pattern"
                ))
        
        # Check for loops
        loops = self.detect_loops(agent_sequence)
        for loop in loops:
            patterns.append(FlowPattern(
                type="loop",
                pattern=loop,
                description="Agent workflow loop detected"
            ))
        
        return patterns
    
    def detect_flow_issues(self, log_entries: List[LogEntry]) -> List[FlowIssue]:
        """Detect issues in agent workflow"""
        issues = []
        
        # Check for incomplete interactions
        incomplete_interactions = self.find_incomplete_interactions(log_entries)
        for interaction in incomplete_interactions:
            issues.append(FlowIssue(
                type="incomplete_interaction",
                agent_type=interaction.agent_type,
                description=f"Incomplete interaction for {interaction.agent_type}",
                severity="high"
            ))
        
        # Check for unexpected agent sequences
        unexpected_sequences = self.find_unexpected_sequences(log_entries)
        for sequence in unexpected_sequences:
            issues.append(FlowIssue(
                type="unexpected_sequence",
                agent_type=sequence,
                description=f"Unexpected agent sequence: {sequence}",
                severity="medium"
            ))
        
        # Check for performance issues
        performance_issues = self.detect_performance_issues(log_entries)
        for issue in performance_issues:
            issues.append(issue)
        
        return issues
```

### 2. Error Analysis and Debugging
**MANDATORY**: Implement comprehensive error analysis for agent workflow debugging.

**Error Analysis Requirements**:
- **Error Context**: Preserve full context for all errors
- **Error Patterns**: Identify common error patterns
- **Root Cause Analysis**: Analyze errors for root causes
- **Error Impact**: Assess impact of errors on workflow
- **Recovery Analysis**: Analyze error recovery mechanisms

**Implementation**:
```python
# ✅ CORRECT: Error Analysis
class ErrorAnalyzer:
    def __init__(self):
        self.error_patterns = self.load_error_patterns()
        self.root_cause_analyzer = RootCauseAnalyzer()
        self.impact_analyzer = ImpactAnalyzer()
    
    def analyze_errors(self, log_entries: List[LogEntry]) -> ErrorAnalysis:
        """Analyze errors in agent workflow"""
        error_analysis = ErrorAnalysis()
        
        # Extract error entries
        error_entries = [entry for entry in log_entries if entry.level in ['ERROR', 'CRITICAL']]
        
        # Analyze each error
        for error_entry in error_entries:
            error_info = self.analyze_error_entry(error_entry, log_entries)
            error_analysis.add_error(error_info)
        
        # Identify error patterns
        error_analysis.patterns = self.identify_error_patterns(error_analysis.errors)
        
        # Analyze error impact
        error_analysis.impact = self.impact_analyzer.analyze_impact(error_analysis.errors)
        
        # Generate recommendations
        error_analysis.recommendations = self.generate_error_recommendations(error_analysis)
        
        return error_analysis
    
    def analyze_error_entry(self, error_entry: LogEntry, all_entries: List[LogEntry]) -> ErrorInfo:
        """Analyze individual error entry"""
        # Extract error details
        error_message = error_entry.message
        error_data = error_entry.data
        
        # Find context (entries before and after error)
        context_entries = self.find_error_context(error_entry, all_entries)
        
        # Analyze root cause
        root_cause = self.root_cause_analyzer.analyze_root_cause(error_entry, context_entries)
        
        # Determine error type
        error_type = self.classify_error(error_message, error_data)
        
        # Assess severity
        severity = self.assess_error_severity(error_entry, context_entries)
        
        return ErrorInfo(
            timestamp=error_entry.timestamp,
            agent_type=error_entry.agent_type,
            session_id=error_entry.session_id,
            error_type=error_type,
            error_message=error_message,
            error_data=error_data,
            severity=severity,
            root_cause=root_cause,
            context=context_entries
        )
    
    def find_error_context(self, error_entry: LogEntry, all_entries: List[LogEntry]) -> List[LogEntry]:
        """Find context entries around error"""
        context_entries = []
        error_index = -1
        
        # Find error entry index
        for i, entry in enumerate(all_entries):
            if entry.timestamp == error_entry.timestamp and entry.message == error_entry.message:
                error_index = i
                break
        
        if error_index == -1:
            return context_entries
        
        # Get entries before error (context window)
        start_index = max(0, error_index - 10)
        end_index = min(len(all_entries), error_index + 5)
        
        for i in range(start_index, end_index):
            if i != error_index:  # Exclude the error entry itself
                context_entries.append(all_entries[i])
        
        return context_entries
    
    def classify_error(self, error_message: str, error_data: Dict) -> str:
        """Classify error by type"""
        error_message_lower = error_message.lower()
        
        # API errors
        if any(keyword in error_message_lower for keyword in ['api', 'http', 'request', 'response']):
            return "api_error"
        
        # Parsing errors
        if any(keyword in error_message_lower for keyword in ['parse', 'json', 'xml', 'format']):
            return "parsing_error"
        
        # Database errors
        if any(keyword in error_message_lower for keyword in ['database', 'sql', 'connection', 'query']):
            return "database_error"
        
        # Authentication errors
        if any(keyword in error_message_lower for keyword in ['auth', 'authentication', 'token', 'permission']):
            return "authentication_error"
        
        # Resource errors
        if any(keyword in error_message_lower for keyword in ['memory', 'disk', 'resource', 'timeout']):
            return "resource_error"
        
        # Configuration errors
        if any(keyword in error_message_lower for keyword in ['config', 'setting', 'parameter', 'option']):
            return "configuration_error"
        
        return "general_error"
    
    def assess_error_severity(self, error_entry: LogEntry, context_entries: List[LogEntry]) -> str:
        """Assess error severity based on context"""
        # Check if error caused workflow failure
        workflow_failed = self.check_workflow_failure(error_entry, context_entries)
        
        # Check if error is recoverable
        recoverable = self.check_error_recoverability(error_entry, context_entries)
        
        # Check error frequency
        frequency = self.check_error_frequency(error_entry, context_entries)
        
        # Determine severity
        if workflow_failed:
            return "critical"
        elif not recoverable:
            return "high"
        elif frequency > 3:
            return "medium"
        else:
            return "low"

class RootCauseAnalyzer:
    def analyze_root_cause(self, error_entry: LogEntry, context_entries: List[LogEntry]) -> RootCause:
        """Analyze root cause of error"""
        error_message = error_entry.message
        error_data = error_entry.data
        
        # Check for common root causes
        if "timeout" in error_message.lower():
            return RootCause("timeout", "Operation timed out", "Check network connectivity and timeouts")
        
        if "connection" in error_message.lower():
            return RootCause("connection", "Connection failed", "Check network connectivity and service availability")
        
        if "permission" in error_message.lower():
            return RootCause("permission", "Permission denied", "Check authentication and authorization")
        
        if "not found" in error_message.lower():
            return RootCause("not_found", "Resource not found", "Check resource existence and paths")
        
        if "invalid" in error_message.lower():
            return RootCause("invalid_input", "Invalid input data", "Validate input data format and content")
        
        # Analyze context for patterns
        context_pattern = self.analyze_context_pattern(context_entries)
        if context_pattern:
            return context_pattern
        
        return RootCause("unknown", "Unknown root cause", "Manual investigation required")
    
    def analyze_context_pattern(self, context_entries: List[LogEntry]) -> Optional[RootCause]:
        """Analyze context entries for patterns"""
        # Look for repeated errors
        error_counts = {}
        for entry in context_entries:
            if entry.level in ['ERROR', 'CRITICAL']:
                error_type = self.extract_error_type(entry.message)
                error_counts[error_type] = error_counts.get(error_type, 0) + 1
        
        # Check for repeated errors
        for error_type, count in error_counts.items():
            if count > 2:
                return RootCause("repeated_error", f"Repeated {error_type} errors", "Check for systematic issues")
        
        return None
```

### 3. Performance Analysis
**MANDATORY**: Implement performance analysis for agent workflow optimization.

**Performance Analysis Requirements**:
- **Execution Time Tracking**: Track execution time for each agent
- **Performance Bottlenecks**: Identify performance bottlenecks
- **Resource Usage**: Monitor resource usage during execution
- **Performance Trends**: Analyze performance trends over time
- **Optimization Recommendations**: Generate optimization recommendations

**Implementation**:
```python
# ✅ CORRECT: Performance Analysis
class PerformanceAnalyzer:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.bottleneck_detector = BottleneckDetector()
        self.trend_analyzer = TrendAnalyzer()
        self.optimizer = PerformanceOptimizer()
    
    def analyze_performance(self, log_entries: List[LogEntry]) -> PerformanceAnalysis:
        """Analyze performance from log entries"""
        performance_analysis = PerformanceAnalysis()
        
        # Extract performance metrics
        metrics = self.metrics_collector.extract_metrics(log_entries)
        performance_analysis.metrics = metrics
        
        # Detect bottlenecks
        bottlenecks = self.bottleneck_detector.detect_bottlenecks(metrics)
        performance_analysis.bottlenecks = bottlenecks
        
        # Analyze trends
        trends = self.trend_analyzer.analyze_trends(metrics)
        performance_analysis.trends = trends
        
        # Generate optimization recommendations
        recommendations = self.optimizer.generate_recommendations(metrics, bottlenecks)
        performance_analysis.recommendations = recommendations
        
        return performance_analysis

class MetricsCollector:
    def extract_metrics(self, log_entries: List[LogEntry]) -> Dict[str, List[PerformanceMetric]]:
        """Extract performance metrics from log entries"""
        metrics = {}
        
        # Group entries by agent type
        agent_entries = {}
        for entry in log_entries:
            if entry.agent_type not in agent_entries:
                agent_entries[entry.agent_type] = []
            agent_entries[entry.agent_type].append(entry)
        
        # Extract metrics for each agent
        for agent_type, entries in agent_entries.items():
            agent_metrics = self.extract_agent_metrics(agent_type, entries)
            metrics[agent_type] = agent_metrics
        
        return metrics
    
    def extract_agent_metrics(self, agent_type: str, entries: List[LogEntry]) -> List[PerformanceMetric]:
        """Extract performance metrics for specific agent"""
        metrics = []
        
        # Find start and end times for interactions
        interaction_times = self.find_interaction_times(entries)
        
        for start_entry, end_entry in interaction_times:
            if start_entry and end_entry:
                execution_time = (end_entry.timestamp - start_entry.timestamp).total_seconds()
                
                metric = PerformanceMetric(
                    agent_type=agent_type,
                    session_id=start_entry.session_id,
                    start_time=start_entry.timestamp,
                    end_time=end_entry.timestamp,
                    execution_time=execution_time,
                    success=end_entry.message.lower().find("success") != -1
                )
                
                metrics.append(metric)
        
        return metrics
    
    def find_interaction_times(self, entries: List[LogEntry]) -> List[tuple]:
        """Find start and end times for agent interactions"""
        interaction_times = []
        current_start = None
        
        for entry in entries:
            if entry.message.startswith("Agent interaction started"):
                current_start = entry
            elif entry.message.startswith("Agent interaction completed"):
                if current_start:
                    interaction_times.append((current_start, entry))
                    current_start = None
        
        return interaction_times

class BottleneckDetector:
    def detect_bottlenecks(self, metrics: Dict[str, List[PerformanceMetric]]) -> List[Bottleneck]:
        """Detect performance bottlenecks"""
        bottlenecks = []
        
        # Calculate average execution times
        avg_times = {}
        for agent_type, agent_metrics in metrics.items():
            if agent_metrics:
                avg_time = sum(m.execution_time for m in agent_metrics) / len(agent_metrics)
                avg_times[agent_type] = avg_time
        
        # Find slowest agents
        if avg_times:
            max_time = max(avg_times.values())
            threshold = max_time * 0.8  # 80% of max time
            
            for agent_type, avg_time in avg_times.items():
                if avg_time >= threshold:
                    bottlenecks.append(Bottleneck(
                        agent_type=agent_type,
                        avg_execution_time=avg_time,
                        threshold=threshold,
                        description=f"{agent_type} is significantly slower than other agents"
                    ))
        
        # Check for outliers
        outliers = self.detect_outliers(metrics)
        bottlenecks.extend(outliers)
        
        return bottlenecks
    
    def detect_outliers(self, metrics: Dict[str, List[PerformanceMetric]]) -> List[Bottleneck]:
        """Detect performance outliers"""
        outliers = []
        
        for agent_type, agent_metrics in metrics.items():
            if len(agent_metrics) < 3:
                continue
            
            # Calculate statistics
            execution_times = [m.execution_time for m in agent_metrics]
            mean_time = sum(execution_times) / len(execution_times)
            
            # Calculate standard deviation
            variance = sum((t - mean_time) ** 2 for t in execution_times) / len(execution_times)
            std_dev = variance ** 0.5
            
            # Find outliers (beyond 2 standard deviations)
            for metric in agent_metrics:
                if abs(metric.execution_time - mean_time) > 2 * std_dev:
                    outliers.append(Bottleneck(
                        agent_type=agent_type,
                        avg_execution_time=metric.execution_time,
                        threshold=mean_time + 2 * std_dev,
                        description=f"Outlier execution time for {agent_type}",
                        session_id=metric.session_id
                    ))
        
        return outliers
```

### 4. Debugging Tools and Utilities
**MANDATORY**: Implement debugging tools and utilities for agent workflow analysis.

**Debugging Tools Requirements**:
- **Log Visualization**: Visualize agent workflow from logs
- **Error Reproduction**: Tools for reproducing errors
- **State Inspection**: Tools for inspecting agent state
- **Flow Comparison**: Compare different workflow executions
- **Debugging Reports**: Generate comprehensive debugging reports

**Implementation**:
```python
# ✅ CORRECT: Debugging Tools
class DebuggingTools:
    def __init__(self):
        self.log_analyzer = AgentLogAnalyzer()
        self.visualizer = FlowVisualizer()
        self.reporter = DebugReporter()
        self.state_inspector = StateInspector()
    
    def generate_debug_report(self, session_id: str) -> DebugReport:
        """Generate comprehensive debug report for session"""
        # Analyze agent flow
        flow_report = self.log_analyzer.analyze_agent_flow(session_id)
        
        # Generate visualization
        visualization = self.visualizer.create_flow_diagram(flow_report)
        
        # Create debug report
        debug_report = DebugReport(
            session_id=session_id,
            flow_analysis=flow_report.flow_analysis,
            error_analysis=flow_report.error_analysis,
            performance_analysis=flow_report.performance_analysis,
            visualization=visualization,
            recommendations=self.generate_recommendations(flow_report)
        )
        
        return debug_report
    
    def compare_sessions(self, session_ids: List[str]) -> ComparisonReport:
        """Compare multiple sessions for debugging"""
        session_reports = []
        
        for session_id in session_ids:
            try:
                flow_report = self.log_analyzer.analyze_agent_flow(session_id)
                session_reports.append((session_id, flow_report))
            except Exception as e:
                logging.warning(f"Failed to analyze session {session_id}: {e}")
        
        # Compare sessions
        comparison = self.compare_session_reports(session_reports)
        
        return ComparisonReport(
            session_ids=session_ids,
            comparison=comparison,
            recommendations=self.generate_comparison_recommendations(comparison)
        )
    
    def reproduce_error(self, error_info: ErrorInfo) -> ReproductionReport:
        """Reproduce error for debugging"""
        # Extract error context
        context = error_info.context
        
        # Reconstruct error scenario
        scenario = self.reconstruct_scenario(context)
        
        # Attempt reproduction
        reproduction_result = self.attempt_reproduction(scenario)
        
        return ReproductionReport(
            error_info=error_info,
            scenario=scenario,
            reproduction_result=reproduction_result,
            recommendations=self.generate_reproduction_recommendations(reproduction_result)
        )

class FlowVisualizer:
    def create_flow_diagram(self, flow_report: AgentFlowReport) -> str:
        """Create Mermaid flow diagram from agent flow"""
        mermaid_code = ["graph TD"]
        
        # Add agent nodes
        for i, agent_type in enumerate(flow_report.flow_analysis.agent_sequence):
            node_id = f"A{i}"
            mermaid_code.append(f"    {node_id}[{agent_type}]")
            
            # Add connection to next agent
            if i > 0:
                prev_node_id = f"A{i-1}"
                mermaid_code.append(f"    {prev_node_id} --> {node_id}")
        
        # Add error nodes if any
        for i, error in enumerate(flow_report.error_analysis.errors):
            error_node_id = f"E{i}"
            mermaid_code.append(f"    {error_node_id}[ERROR: {error.error_type}]")
            
            # Connect error to relevant agent
            if error.agent_type:
                agent_index = flow_report.flow_analysis.agent_sequence.index(error.agent_type)
                agent_node_id = f"A{agent_index}"
                mermaid_code.append(f"    {agent_node_id} -.-> {error_node_id}")
        
        return "\n".join(mermaid_code)
    
    def create_performance_chart(self, performance_analysis: PerformanceAnalysis) -> str:
        """Create performance chart data"""
        chart_data = {
            "labels": [],
            "datasets": []
        }
        
        # Extract agent types and execution times
        for agent_type, metrics in performance_analysis.metrics.items():
            if metrics:
                chart_data["labels"].append(agent_type)
                avg_time = sum(m.execution_time for m in metrics) / len(metrics)
                chart_data["datasets"].append({
                    "label": agent_type,
                    "data": [avg_time],
                    "backgroundColor": self.get_agent_color(agent_type)
                })
        
        return json.dumps(chart_data)
```

## Integration with Development Workflow

### Pre-Debugging Checklist
```python
# ✅ CORRECT: Pre-Debugging Checklist
class PreDebuggingChecklist:
    def __init__(self):
        self.log_analyzer = AgentLogAnalyzer()
    
    def run_pre_debugging_checks(self, session_id: str) -> PreDebuggingReport:
        """Run pre-debugging checks"""
        report = PreDebuggingReport()
        
        # Check if agent.log exists
        if not Path("logs/agent.log").exists():
            report.add_issue("missing_log", "Agent log file not found")
            return report
        
        # Check log file size
        log_size = Path("logs/agent.log").stat().st_size
        if log_size == 0:
            report.add_issue("empty_log", "Agent log file is empty")
            return report
        
        # Check for session entries
        try:
            flow_report = self.log_analyzer.analyze_agent_flow(session_id)
            if not flow_report.log_entries:
                report.add_issue("no_session_data", f"No log entries found for session {session_id}")
        except Exception as e:
            report.add_issue("log_analysis_error", f"Log analysis failed: {e}")
        
        return report
```

### Continuous Debugging Monitoring
```python
# ✅ CORRECT: Continuous Debugging
class ContinuousDebuggingMonitor:
    def __init__(self):
        self.log_analyzer = AgentLogAnalyzer()
        self.alert_system = AlertSystem()
    
    def monitor_agent_workflows(self) -> None:
        """Continuously monitor agent workflows for issues"""
        while True:
            try:
                # Get recent sessions
                recent_sessions = self.get_recent_sessions()
                
                for session_id in recent_sessions:
                    # Analyze session
                    flow_report = self.log_analyzer.analyze_agent_flow(session_id)
                    
                    # Check for critical issues
                    if flow_report.error_analysis.has_critical_errors():
                        self.alert_system.send_alert(Alert(
                            type="critical_agent_error",
                            severity="critical",
                            message=f"Critical errors detected in session {session_id}"
                        ))
                    
                    # Check for performance issues
                    if flow_report.performance_analysis.has_severe_bottlenecks():
                        self.alert_system.send_alert(Alert(
                            type="agent_performance_issue",
                            severity="high",
                            message=f"Performance issues detected in session {session_id}"
                        ))
                
                # Wait for next cycle
                time.sleep(60)  # Check every minute
                
            except Exception as e:
                logging.error(f"Continuous debugging monitoring error: {e}")
                time.sleep(30)  # Wait 30 seconds before retrying
```

## Benefits

### Comprehensive Debugging
- **Primary Source**: Use agent.log as primary debugging source
- **Complete Context**: Preserve full context for all issues
- **Systematic Analysis**: Systematic approach to debugging

### Error Resolution
- **Root Cause Analysis**: Identify root causes of issues
- **Error Patterns**: Recognize common error patterns
- **Quick Resolution**: Faster issue resolution through analysis

### Performance Optimization
- **Bottleneck Detection**: Identify performance bottlenecks
- **Trend Analysis**: Analyze performance trends
- **Optimization Recommendations**: Generate optimization recommendations

### Workflow Understanding
- **Flow Visualization**: Visualize agent workflows
- **Pattern Recognition**: Recognize workflow patterns
- **Issue Prevention**: Prevent issues through understanding

## Enforcement

This rule is **CRITICAL** and must be followed for all:
- Agent workflow debugging
- Error analysis and resolution
- Performance optimization
- Workflow troubleshooting
- Issue investigation

**Violations of this rule require immediate remediation and debugging capability restoration.**
description: "Auto-generated description"
globs: ["**/*"]
alwaysApply: true
---
